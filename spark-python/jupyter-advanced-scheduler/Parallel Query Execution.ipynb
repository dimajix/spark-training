{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Query Execution\n",
    "\n",
    "This notebook shows how to fire up concurrent queries in Spark. This may be useful to get a better overall throughput in cases that multiple outputs need to be generated.\n",
    "\n",
    "We will simply reuse the weather example and fire up two concurrent queries. Although they will generate the very same result, it is still interesting to see that even the intermediate cache will only be built once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "Running multiple queries in parallel requires some configuration of Spark. Spark can always accept multiple queries, but per default it will process those in a *FIFO* fashion. This means that one query is processed after the other. But Spark also supports a real parallel query execution using a different task scheduler.\n",
    "\n",
    "You need to configure the following values:\n",
    "\n",
    "1. Create a schduler configuration file `fairscheduler.xml` (contents see below)\n",
    "2. Set Spark config `spark.scheduler.mode` to `FAIR`\n",
    "3. Set Spark config `spark.scheduler.allocation.file` to the location of the `fairscheduler.xml` file\n",
    "\n",
    "Unfortunately these values need to configures __before the Spark session is created__.\n",
    "\n",
    "The `fairscheduler.xml` should look as follows:\n",
    "```xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<allocations>\n",
    "  <pool name=\"fair\">\n",
    "    <schedulingMode>FAIR</schedulingMode>\n",
    "    <weight>1</weight>\n",
    "    <minShare>2</minShare>\n",
    "  </pool>\n",
    "</allocations>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in all years\n",
    "\n",
    "Now we read in all years by creating a union. We also add the year as a logical partition column, this will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", lit(i)) for i in range(2003,2015)]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l,r: l.union(r), raw_weather_per_year)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Information\n",
    "\n",
    "The raw data is not exactly nice to work with, so we need to extract the relevant information by using appropriate substr operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    col(\"year\"),\n",
    "    substring(col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"),11,5).alias(\"wban\"),\n",
    "    substring(col(\"value\"),16,8).alias(\"date\"),\n",
    "    substring(col(\"value\"),24,4).alias(\"time\"),\n",
    "    substring(col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"),66,4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"),88,5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read in Station Metadata\n",
    "\n",
    "Fortunately station metadata is stored as CSV, so we can directly read that using Sparks `spark.read.csv` mechanisum. The data can be found at `storageLocation + '/isd-history'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Join and cache data\n",
    "\n",
    "Now we need to join the meta data with the measurements. We will cache the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string, USAF: string, WBAN: string, STATION NAME: string, CTRY: string, STATE: string, ICAO: string, LAT: string, LON: string, ELEV(M): string, BEGIN: string, END: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_data = weather.join(stations, (weather.usaf == stations.USAF) & (weather.wban == stations.WBAN))\n",
    "joined_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform Queries\n",
    "\n",
    "Now comes the interesting part: We will perform multiple queries in parallel. We will make use of the Python `threading` module in order to start two concurrent queries. One query will aggregate min/max of temperature, while the other query will aggragte min/max of wind speed.\n",
    "\n",
    "We will save both results to corresponding CSV files into HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define Queries\n",
    "\n",
    "First we create two Python functions which contain the two queries to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_temperature():\n",
    "    df = joined_data\n",
    "    result = df.groupBy(df.CTRY, df.year).agg(\n",
    "            min(when(df.air_temperature_qual == lit(1), df.air_temperature)).alias('min_temp'),\n",
    "            max(when(df.air_temperature_qual == lit(1), df.air_temperature)).alias('max_temp'),\n",
    "        )\n",
    "    result.write.csv(\"/user/hadoop/weather_min_max_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wind_speed():\n",
    "    df = joined_data\n",
    "    result = df.groupBy(df.CTRY, df.year).agg(\n",
    "            min(when(df.wind_speed_qual == lit(1), df.wind_speed)).alias('min_wind'),\n",
    "            max(when(df.wind_speed_qual == lit(1), df.wind_speed)).alias('max_wind')\n",
    "        )\n",
    "    result.write.csv(\"/user/hadoop/weather_min_max_wind\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Queries\n",
    "\n",
    "Now since we have the two functions, we import and use the Python `threading` module to run both queries in parallel. We also need to configure Spark to use the correct scheduler pool (in our case it is the `fair` pool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set the thread local property \"spark.scheduler.pool\" to the correct pool defined in fairscheduler.xml\n",
    "spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"fair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# First create threads\n",
    "t1 = threading.Thread(target=calc_temperature)\n",
    "t2 = threading.Thread(target=calc_wind_speed)\n",
    "\n",
    "# Then start both threads (in the background)\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# Finally wait until both threads have finished\n",
    "t1.join()\n",
    "t2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Watch Query execution\n",
    "\n",
    "Now you should open the Spark web interface and watch both queries being processed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Inspect result\n",
    "\n",
    "Finally you can inspect the results using for example Spark again (or HDFS command line tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = spark.read.text(\"/user/hadoop/weather_min_max_temp\")\n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_df = spark.read.text(\"/user/hadoop/weather_min_max_wind\")\n",
    "wind_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.4 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
