{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Data Example\n",
    "\n",
    "Window functions are commonly used together with sales data. In this notebook we will be using a data set called \"Watson Sales Product Sample Data\" which was downloaded from https://www.ibm.com/communities/analytics/watson-analytics-blog/sales-products-sample-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Watson Sales Product Sample Data\n",
    "\n",
    "First we load the data, which is provided as a single CSV file, which again is well supported by Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"s3://dimajix-training/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read\\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(basedir + \"/watson-sales-products/WA_Sales_Products_2012-14.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect schema\n",
    "\n",
    "Since we used the existing header information and also let Spark infer appropriate data types, let us inspect the schema now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preaggregate data\n",
    "\n",
    "Since we are not interested in all details, we preaggregate the data into the following columns:\n",
    "* Retailer country\n",
    "* Retailer type\n",
    "* Product line\n",
    "* Quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data = data.groupBy(\n",
    "    \"Retailer country\",\n",
    "    \"Retailer type\",\n",
    "    \"Product line\",\n",
    "    \"Quarter\"\n",
    ").agg(\n",
    "    sf.sum(\"Revenue\").alias(\"Revenue\"),\n",
    "    sf.sum(\"Quantity\").alias(\"Quantity\")\n",
    ")\n",
    "\n",
    "aggregated_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Find Difference to Average\n",
    "\n",
    "In the first example, we try to find the difference of the revenue of each quarter to the average revenue for each retailer country and retailer type over all quarters. This can be done either using a grouped aggregated followed by a join or by using window functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Self Join\n",
    "\n",
    "Just for the sake of completeness, let us start with the aggragetion and join approach. It will turn out later that this is much more complicated than using a window function, but nevertheless we implement this approach such that we can compare both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extarct year and quarter\n",
    "\n",
    "Technically the first step is not required, but in order to provide some meaningful sorting, we extract the quarter (Q1, Q2, Q3 and Q4) and the year from the incoming column `Quarter`. Otherwise sorting wouldn't work, since that column is formatted as `'Q'q YYYY` which doesn't provide a chronologically ordering if sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = aggregated_data.select(\n",
    "    # YOUR CODE HERE\n",
    ")\n",
    "\n",
    "extended_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate average revenue\n",
    "\n",
    "Now we calculate the average revenue per retailer country, retailer type and product line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data = # YOUR CODE HERE\n",
    "\n",
    "avg_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Join and calculate\n",
    "\n",
    "Now we join the average revenue with the original data set, such that we can calculate the difference of the revenue and the average revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = extended_data.join(\n",
    "        avg_data,\n",
    "        (extended_data[\"Retailer country\"] == avg_data[\"Retailer country\"]) &\n",
    "        (extended_data[\"Retailer type\"] == avg_data[\"Retailer type\"]) &\n",
    "        (extended_data[\"Product line\"] == avg_data[\"Product line\"])\n",
    "    ).select(\n",
    "        # YOUR CODE HERE\n",
    "    )\n",
    "\n",
    "sorted_result = result \\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\")\n",
    "\n",
    "sorted_result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Exeuction Plan\n",
    "\n",
    "Let us have a look at the execution plan using the `explain` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Better use Windowing\n",
    "\n",
    "Now let us perform the very same analysis, but using windowed aggregation instead of aggregation and joining. A *window* aggregates groups of records, but this grouping and aggregation will be performed (conceptionally) individually for every input record and the result will be attached to each input record. Therefore a windowed aggregation works like a normal aggregation followed by a join.\n",
    "\n",
    "In Spark we always need to specify how this aggregation window is to be constructed. It always has up to three components:\n",
    "* Partitioning - controls which records will be considered for each window\n",
    "* Sorting - sorts all records in a window\n",
    "* Range - controls how many records in the sorted list should be aggregated\n",
    "\n",
    "### Aggregazion functions\n",
    "After the window has been created, you can use any conventional aggregation function like `sum`, `avg` etc. In addition Spark also provides some special window functions which make use of the ordering (which is not available in normal aggregations). The most important window aggregation functions are:\n",
    "* `rank()`\n",
    "* `dense_rank()`\n",
    "* `row_number()`\n",
    "* `lag(column, n)` and `lead(column, n)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extarct year and quarter\n",
    "\n",
    "Technically the first step is not required, but in order to provide some meaningful sorting, we extract the quarter (Q1, Q2, Q3 and Q4) and the year from the incoming column Quarter. Otherwise sorting wouldn't work, since that column is formatted as 'Q'q YYYY which doesn't provide a chronologically ordering if sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = aggregated_data.select(\n",
    "    sf.col(\"*\"),\n",
    "    sf.substring(aggregated_data[\"Quarter\"],1,2).alias(\"q\"),\n",
    "    sf.substring(aggregated_data[\"Quarter\"],3,8).alias(\"y\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define window\n",
    "\n",
    "This time we use a windowed aggregation to calculate the average price. As the first step we need to construct a *window*. In this case it contains the following ingredients:\n",
    "* A definition of partitions (i.e. which rows should be averages together)\n",
    "* A definition of the window size in rows (i.e. which rows within each partition should take part for each average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window\n",
    "avg_window = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Perform analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform analysis\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "# Sort result for nicer output\n",
    "sorted_result = result \\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\")\n",
    "\n",
    "sorted_result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "Again let us have a look at the execution plan, which is now much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Exercise\n",
    "\n",
    "Perform a similar analysis, which compares the quantity per product line and quarter with the average quantity per product line over all quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Best Quarter\n",
    "\n",
    "Another interesting question would be, which quarter was the best one in each country for each retailer type and product line. This would be already much harder to do with a join, since the join key would probably need to contain the maximum revenue, which is a double (never join on floating point values, it might not work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Using windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Perform analysis\n",
    "\n",
    "Again we need to define a window, and within each window partition we want to sort the rows by the `Revenue` column and add the sorted position as a new column. This then allows us to trivially simply select the top most row in each window, which contains the best revenue. \n",
    "\n",
    "This time the window again needs to be partitioned and sorted by revenue, such that we can easily pick the top most revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ranking window\n",
    "rank_window = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Perform analysis\n",
    "\n",
    "By using this window, we can easily perform the analysis be calculating the position of each record within its window by using the `row_number` function and then select the top most record by filtering the row number to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform analysis using the \"row_number\" window function\n",
    "ranked_data = # YOUR CODE HERE\n",
    "\n",
    "# Pick the top entry of every window by filtering on the row number\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "# Sort result, just to improve output\n",
    "sorted_result = result \\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\", \"rank\")\n",
    "\n",
    "sorted_result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Exercise\n",
    "\n",
    "Using a similar approach now calculate the country with the largest quanitity per product line and retailer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Difference between Quarters\n",
    "\n",
    "Another common example where windowing will greatly simplify processing is accessing different rows in a single query. This cannot be done in Spark without using some trick, since Spark normally processes all rows independently. In a simple `select` you can access any number of columns, but you only have access to a single row.\n",
    "\n",
    "As an example, we'd like to calculate the difference in revenue of two consecutive quarters. Obviously we need to access the revenue of two quarters to calulcate the difference. Again we use two different approaches, the first using a `join` operation and the second using a windowed aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Self Join\n",
    "\n",
    "The first approach will join the data set to itself, such that two different quarters of the same retailer country, retailer type and product type are put together into a single row. Then a simple subtraction will provide the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Calculate previous quarter\n",
    "\n",
    "As a first step, we need to create a small helper function for calculating the previous quarter from a given quarter using the provided format `Qq YYYY`. With this function we can generate the join key required for joining the same dataset on the previous quarter.\n",
    "\n",
    "We will write a small Python UDF to perform the desired operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_quarter(quarter):\n",
    "    q = int(quarter[1:2])\n",
    "    y = int(quarter[3:8])\n",
    "    \n",
    "    prev_q = q - 1\n",
    "    if (prev_q <= 0):\n",
    "        prev_y = y - 1\n",
    "        prev_q = 4\n",
    "    else:\n",
    "        prev_y = y\n",
    "    \n",
    "    return \"Q\" + str(prev_q) + \" \" + str(prev_y)\n",
    "    \n",
    "print(prev_quarter(\"Q1 2012\"))\n",
    "print(prev_quarter(\"Q4 2012\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as st\n",
    "\n",
    "prev_quarter_udf = sf.udf(prev_quarter, st.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the `prev_quarter` UDF to the data set to create a new column containing the previous quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = aggregated_data.select(\n",
    "    sf.col(\"*\"),\n",
    "    prev_quarter_udf(aggregated_data[\"Quarter\"]).alias(\"prev_quarter\")\n",
    ")\n",
    "\n",
    "extended_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Join current and previous Quarter\n",
    "\n",
    "Now we need to join the current quarter with the last quarter using the newly created column `prev_quarter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = extended_data.alias(\"current\").join(\n",
    "        extended_data.alias(\"prev\"),\n",
    "        (sf.col(\"current.Quarter\") == sf.col(\"prev.prev_quarter\")) &\n",
    "        (sf.col(\"current.Retailer country\") == sf.col(\"prev.Retailer country\")) &\n",
    "        (sf.col(\"current.Retailer type\") == sf.col(\"prev.Retailer type\")) &\n",
    "        (sf.col(\"current.Product Line\") == sf.col(\"prev.Product Line\")),\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "joined_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most columns are present twice now, but by using the data frame aliases `current` and `prev` we still can distinguish between the two original sources. We need that capability in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate difference\n",
    "\n",
    "Now that we have the current revenue and the previous revenue joined together in a single data frame, we finally can now calculate the difference and keep only the columns from the `current` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data.select(\n",
    "        sf.col(\"current.*\"),\n",
    "        (sf.col(\"current.Revenue\") - sf.col(\"prev.Revenue\")).alias(\"revenue_delta\")\n",
    "    )\n",
    "\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Use Windows\n",
    "\n",
    "Now that we saw how to solve the problem with a join (and a UDF for calculating the previous quarter), let us get to a different approach using a windowed aggregation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = aggregated_data.select(\n",
    "    sf.col(\"*\"),\n",
    "    sf.substring(aggregated_data[\"Quarter\"],1,2).alias(\"q\"),\n",
    "    sf.substring(aggregated_data[\"Quarter\"],3,8).alias(\"y\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Window\n",
    "\n",
    "What we essentially want to do is to access values from *different rows* for calculating the difference between quarters. So what we need is something like follows:\n",
    "* Create window per retailer country, retailer type and product line\n",
    "* Sort by quarter\n",
    "* Pick previous row\n",
    "\n",
    "The last step is the interesting one. This is done by using the `lag` window aggregation function which allows you to access some preceeding record within the window. Note that the window actually has to contain exactly one record, otherwise you'll get an error by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_window = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Perform analysis\n",
    "\n",
    "Now we can use the window in the following simple select statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "\n",
    "sorted_result = result \\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\")\n",
    "\n",
    "sorted_result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Exercise\n",
    "\n",
    "Now calculate the difference in sold quantities between two consecutive quarters per retailer country, retailer type and product line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Putting it all together\n",
    "\n",
    "Of course you can also use different window aggregations with different windows in a single query as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_window = Window\\\n",
    "    .orderBy(extended_data[\"Revenue\"].desc())\\\n",
    "    .partitionBy(\n",
    "        \"Retailer country\",\n",
    "        \"Retailer type\",\n",
    "        \"Product line\"\n",
    "    )\n",
    "avg_window = Window\\\n",
    "    .orderBy(extended_data[\"Revenue\"].desc())\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing) \\\n",
    "    .partitionBy(\n",
    "        \"Retailer country\",\n",
    "        \"Retailer type\",\n",
    "        \"Product line\"\n",
    "    )\n",
    "\n",
    "prev_window = Window \\\n",
    "    .orderBy(extended_data[\"y\"].asc(),extended_data[\"q\"].asc())\\\n",
    "    .rowsBetween(-1, -1) \\\n",
    "    .partitionBy(\n",
    "        \"Retailer country\",\n",
    "        \"Retailer type\",\n",
    "        \"Product line\"\n",
    "    )\n",
    "\n",
    "result = extended_data.select(\n",
    "        sf.col(\"*\"),\n",
    "        sf.row_number().over(rank_window).alias(\"rank\"),\n",
    "        sf.avg(extended_data[\"Revenue\"]).over(avg_window).alias(\"avg_revenue\"),\n",
    "        (extended_data[\"Revenue\"] - sf.lag(extended_data[\"Revenue\"], 1).over(prev_window)).alias(\"revenue_delta\")\n",
    "    )\n",
    "\n",
    "sorted_result = result\\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\")\n",
    "\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
