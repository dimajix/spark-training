{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Functions\n",
    "\n",
    "Spark also supports window functions for aggregations. Window functions allow more complex aggregations like sliding windows or ranking, where for each row a set of 'surrounding' rows are used for calculating an additional metric.\n",
    "\n",
    "In this example, we will use the weather data and add a sliding average temparature to the existing columns. The result DataFrame shall have both metrics: The actual temperature (as stored in the original records) and an averaged value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 General Preparations\n",
    "\n",
    "First we enable Matplot inline graphics and set the base location of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Loading Data\n",
    "\n",
    "Again we load data for the single year 2003 from S3 (or whatever storage location is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "rawWeatherData = spark.read.text(storageLocation + \"/2003\")\n",
    "weatherData = rawWeatherData.select(\n",
    "    substring(col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"),11,5).alias(\"wban\"),\n",
    "    to_timestamp(substring(col(\"value\"),16,12),\"yyyyMMddHHmm\").alias(\"timestamp\"),\n",
    "    to_timestamp(substring(col(\"value\"),16,12),\"yyyyMMddHHmm\").cast(\"long\").alias(\"ts\"),\n",
    "    substring(col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"),66,4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"),88,5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the weather data available as a temporary view\n",
    "weather_all = weatherData.cache()\n",
    "weather_all.createOrReplaceTempView(\"weather_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Peek inside the data, just to make sure everything looks right\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Pick a single station\n",
    "\n",
    "For our first steps, we limit ourselves to a single weather station. We pick one with `usaf='954920'` and `wban='99999'`. This is enough to demonstrate the basic functions of window functions for a sliding average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_single = weatherData.where(\"usaf='954920' and wban='99999'\").cache()\n",
    "weather_single.createOrReplaceTempView(\"weather_single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Peek inside the data, just to make sure everything looks right\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Sliding Average\n",
    "\n",
    "Now we want to calculate the sliding average of the temperature as an additional metric. First we use SQL for that task, later we will see how to use the DataFrame API for performing the same task.\n",
    "\n",
    "In order to perform a windowed aggregation, you use the following syntax to specify a column expression:\n",
    "```\n",
    "    AGGREGATE_FUNCTION(columns) OVER(window_specification)\n",
    "```\n",
    "The term `window_specification` is constructed from the following components:\n",
    "```\n",
    "    PARTITION BY category\n",
    "    ORDER BY ordering_column [ASC|DESC]\n",
    "    RANGE BETWEEN start PRECEEDING AND end FOLLOWING\n",
    "```\n",
    "\n",
    "* `PARTITION BY` works similar to a `GROUP BY` operation. It controls which rows will be in the same partition with the given row. Also, the user might want to make sure all rows having the same value for  the category column are collected to the same machine before ordering and calculating the frame.  If no partitioning specification is given, then all data must be collected to a single machine. - it filters records which are used for creating each window\n",
    "* `ORDER BY` sorts all records of a single window accordingly. It controls the way that rows in a partition are ordered, determining the position of the given row in its partition.\n",
    "* `RANGE BETWEEN` states which rows will be included in the frame for the current input row, based on their relative position to the current row.  For example, “the three rows preceding the current row to the current row” describes a frame including the current input row and three rows appearing before the current row.\n",
    "\n",
    "### Frame Types\n",
    "\n",
    "As an alternative to `RANGE BETWEEN` there is also `ROWS BETWEEN`. While `RANGE BETWEEN` refers to the values of the sorting column, `ROWS BETWEEN` simply counts the number of rows. Both window types have their use: `RANGE BETWEEN` is perfect for sliding averages over time windows of constant duration, while `ROWS BETWEEN` is useful for ordered entries lacking a proper arithmetic scale.\n",
    "\n",
    "### Boundaries \n",
    "Both frame types (range and rows) support different boundary types:\n",
    "* `UNBOUNDED PRECEDING`\n",
    "* `UNBOUNDED FOLLOWING`\n",
    "* `CURRENT ROW`\n",
    "* `<value> PRECEDING`\n",
    "* `<value> FOLLOWING`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Sliding average calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "-- YOUR CODE HERE\n",
    "\"\"\").toPandas()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a picture\n",
    "\n",
    "In order to verify our approach, let's draw a picture with Matplotlib, which shows the current temperature and the sliding average in a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Window Aggregation Functions\n",
    "\n",
    "We already used simple standard aggregation functions, which are also available without windows. But there are also some special aggregation functions, which were specifically designed to be used with windowed aggregation and cannot be used without a window definition.\n",
    "\n",
    "These are\n",
    "\n",
    "Function class | SQL | DataFrame Function | Description\n",
    "---------------|-----|--------------------|-------------\n",
    "Ranking functions|rank|ranke|Get rank in window\n",
    "|dense_rank|denseRank|\n",
    "|percent_rank|percentRank|\n",
    "|ntile|ntile|\n",
    "|row_number|rowNumber|Get row number in window\n",
    "Analytic functions|cume_dist|cumeDist|\n",
    "|first_value|first|Pick first value in window\n",
    "|last_value|last|Pick last value in window\n",
    "|lag|lag|Pick preceeding value\n",
    "|lead|lead|Pick following value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Exercise: Comparing to pervious day\n",
    "\n",
    "Another use case for window functions is to compare todays temperature to yesterday at the same time. This can be achived by using the function `FIRST_VALUE` together with an appropriate window with a range from 86400 (number of seconds of one day) preceeding and the current row.\n",
    "\n",
    "**Exercise**: Create a DataFrame with the columns `timestamp`, `temp` (current temperature) and `prev_temp` (previous temperature) and plot the first 300 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a picture\n",
    "\n",
    "Again, draw a picture of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 DataFrame Window API\n",
    "\n",
    "In addition to the SQL interface, there is also a direct Python interface for creating windowed aggregations. Let us reformulate the initial sliding window average aggregation using the Spark DataFrame API instead of SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Sliding average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = # YOUR CODE HERE\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a picture\n",
    "\n",
    "Using Matplotlib, let's make a picture containing the current temperature and the average temperature in a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.plot(x='timestamp', y=['temp','avg_temp'], figsize=[16,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Exercise: Compare temperature to previous day\n",
    "\n",
    "Now perform the same task as the previous exercise: Make a plot of the current temperature and the one 24h ago using the `first` function. But this time, use the DataFrame API instead of SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = # YOUR CODE HERE\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a picture\n",
    "\n",
    "In order to verify our approach, let's draw a picture with Matplotlib, which shows the current temperature and the previous temperature in a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Partitioned Windows\n",
    "\n",
    "So far we only used windows covering a specific time range. This was good enough, since we were only looking at a single station. But in most cases, you want to perform analyses covering multiple different entitites (different weather stations in this example). In these cases you also need to *partition* the aggregation window, such that only records from the same entity are processed.\n",
    "\n",
    "Let us calculate the difference of the current temperature to the average of the last day, but this time for all stations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = # YOUR CODE HERE\n",
    "    \n",
    "# Common column expression for valid temperature value or NULL otherwise    \n",
    "valid_temp = # YOUR CODE HERE\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "result.limit(300).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a Picture\n",
    "\n",
    "In order to check the result, we again pick a single station. But this time, we pick it from the final result and not from the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf = result.where(\"usaf='954920' and wban='99999'\").limit(300).toPandas()\n",
    "pdf.plot(x='timestamp', y=['temp','temp_avg_diff'], figsize=[16,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Exercise: Min/Max Change Analysis\n",
    "\n",
    "Now we want to calculate for every weather station:\n",
    "* The maximum upward difference of temperature within 5 days\n",
    "* The maximum downward difference of temperature within 5 days\n",
    "\n",
    "Logically, we want to perform the following steps for every weather station:\n",
    "1. For every measurement, look back five days\n",
    "2. Within these five days, find the minimum and maximum temperature\n",
    "3. Calculate the difference of the current temepature and the minimum and maximum. Store these in `temp_rise` and `temp_fall`\n",
    "4. Calculate the overall maximum of `temp_rise` and `temp_fall` per station for the whole year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the number of seconds for five days\n",
    "one_day = 24*60*60\n",
    "five_days = 5*one_day\n",
    "five_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a window, which creates a new partition per weather station and looks back 5 days\n",
    "window_spec = # YOUR CODE HERE\n",
    "    \n",
    "# Create a column representing a valid temperature or NULL otherwise    \n",
    "valid_temp = # YOUR CODE HERE\n",
    "\n",
    "# Calculate the difference for each day from the maximum and minimum temperature of the last five days using the window\n",
    "# The resulting DataFrame should have the following columns:\n",
    "#   timestamp\n",
    "#   usaf\n",
    "#   wban\n",
    "#   temp_rise = valid_temp - min(valid_temp).over(window_spec)\n",
    "#   temp_fall = max(valid_temp).over(window_spec) - valid_temp\n",
    "weather_rise_fall = # YOUR CODE HERE\n",
    "\n",
    "# Calculate the maximum raise and fall for each station for the whole year. This should be done by a simple grouped aggregation.\n",
    "# The groups are determined by the weather station id, which is given by usaf and wban\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "# Finally show the whole result by converting it to a Pandas DataFrame\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
