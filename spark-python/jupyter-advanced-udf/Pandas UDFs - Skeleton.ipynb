{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas UDFs\n",
    "\n",
    "\"Normal\" Python UDFs are pretty expensive (in terms of execution time), since for every record the following steps need to be performed:\n",
    "* record is serialized inside JVM\n",
    "* record is sent to an external Python process\n",
    "* record is deserialized inside Python\n",
    "* record is Processed in Python\n",
    "* result is serialized in Python\n",
    "* result is sent back to JVM\n",
    "* result is deserialized and stored inside result DataFrame\n",
    "\n",
    "This does not only sound like a lot of work, it actually is. Therefore Python UDFs are a magnitude slower than native UDFs written in Scala or Java, which run directly inside the JVM.\n",
    "\n",
    "But since Spark 2.3 an alternative approach is available for defining Python UDFs with so called *Pandas UDFs*. Pandas is a commonly used Python framework which also offers DataFrames (but Pandas DataFrames, not Spark DataFrames). Spark 2.3 now can convert inside the JVM a Spark DataFrame into a shareable memory buffer by using a library called *Arrow*. Python then can also treat this memory buffer as a Pandas DataFrame and can directly work on this shared memory.\n",
    "\n",
    "This approach has two major advantages:\n",
    "* No need for serialization and deserialization, since data is shared directly in memory between the JVM and Python\n",
    "* Pandas has lots of very efficient implementations in C for many functions\n",
    "\n",
    "Due to these two facts, Pandas UDFs are much faster and should be preferred over traditional Python UDFs whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Data Example\n",
    "\n",
    "In this notebook we will be using a data set called \"Watson Sales Product Sample Data\" which was downloaded from https://www.ibm.com/communities/analytics/watson-analytics-blog/sales-products-sample-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Retailer country: string (nullable = true)\n",
      " |-- Order method type: string (nullable = true)\n",
      " |-- Retailer type: string (nullable = true)\n",
      " |-- Product line: string (nullable = true)\n",
      " |-- Product type: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Quarter: string (nullable = true)\n",
      " |-- Revenue: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Gross margin: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read\\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"/dimajix/data/watson-sales-products/WA_Sales_Products_2012-14.csv\")\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classic UDF Approach\n",
    "\n",
    "As an example, let's create a function which simply increments a numeric column by one. First let us have a look using a traditional Python UDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 2011\n",
      "Q3 2012\n"
     ]
    }
   ],
   "source": [
    "def prev_quarter(quarter):\n",
    "    q = int(quarter[1:2])\n",
    "    y = int(quarter[3:8])\n",
    "    \n",
    "    prev_q = q - 1\n",
    "    if (prev_q <= 0):\n",
    "        prev_y = y - 1\n",
    "        prev_q = 4\n",
    "    else:\n",
    "        prev_y = y\n",
    "    \n",
    "    return \"Q\" + str(prev_q) + \" \" + str(prev_y)\n",
    "    \n",
    "print(prev_quarter(\"Q1 2012\"))\n",
    "print(prev_quarter(\"Q4 2012\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Use udf to define a row-at-a-time udf\n",
    "@udf('string')\n",
    "# Input/output are both a single double value\n",
    "def prev_quarter_udf(quarter):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scalar Pandas UDF\n",
    "\n",
    "Increment a value using a Pandas UDF. The Pandas UDF receives a `pandas.Series` object and also has to return a `pandas.Series` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "result = data.withColumn('prev_quarter', prev_quarter_pudf(data[\"Quarter\"]))\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Limtations of Scalar UDFs\n",
    "\n",
    "Scalar Pandas UDFs are used for vectorizing scalar operations. They can be used with functions such as select and withColumn. The Python function should take `pandas.Series` as inputs and return a `pandas.Series` of the same length. Internally, Spark will execute a Pandas UDF by splitting columns into batches and calling the function for each batch as a subset of the data, then concatenating the results together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Grouped Pandas Aggregate UDFs\n",
    "\n",
    "Since version 2.4.0, Spark also supports Pandas aggregation functions. This is the only way to implement custom aggregation functions in Python. Note that this type of UDF does not support partial aggregation and all data for a group or window will be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "result = data.groupBy(\"Quarter\").agg(mean_udf(data[\"Revenue\"]).alias(\"mean_revenue\"))\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Limitation of Aggregate UDFs\n",
    "\n",
    "A Grouped Aggregate UDF defines an aggregation from one or more `pandas.Series` to a single scalar value, where each `pandas.Series` represents a column within the group or window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Grouped Pandas Map UDFs\n",
    "While the example above transforms all records independently, but only one column at a time, Spark also offers a so called *grouped Pandas UDF* which operates on complete groups of records (as created by a `groupBy` method). This could be used to replace windowing functions with some Pandas implementation.\n",
    "\n",
    "For example let's subtract the mean of a group from all entries of a group. In Spark this could be achieved directly by using windowed aggregations. But let's first have a look at a Python implementation which does not use Pandas Grouped UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@udf(ArrayType(DoubleType()))\n",
    "def subtract_mean(values):\n",
    "    series = pd.Series(values)\n",
    "    center = series - series.mean()\n",
    "    return [x for x in center]\n",
    "\n",
    "groups = data.groupBy('Quarter').agg(sf.collect_list(data[\"Revenue\"]).alias('values'))\n",
    "result = groups.withColumn('center', sf.explode(subtract_mean(groups.values))).drop('values')\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is even incomplete, as all other columns are now missing... we don't want to complete this example, since Pandas Grouped Map UDFs provide a much better approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Using Pandas Grouped Map UDFs\n",
    "\n",
    "Now let's try to implement the same function using a Pandas grouped UDF. Grouped map Pandas UDFs are used with `groupBy().apply()` which implements the “split-apply-combine” pattern. Split-apply-combine consists of three steps:\n",
    "1. Split the data into groups by using DataFrame.groupBy.\n",
    "2. Apply a function on each group. The input and output of the function are both pandas.DataFrame. The input data contains all the rows and columns for each group.\n",
    "3. Combine the results into a new DataFrame.\n",
    "\n",
    "To use groupBy().apply(), the user needs to define the following:\n",
    "* A Python function that defines the computation for each group.\n",
    "* A StructType object or a string that defines the schema of the output DataFrame.\n",
    "\n",
    "The column labels of the returned `pandas.DataFrame` must either match the field names in the defined output schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define result schema\n",
    "result_schema = StructType(data.schema.fields + [StructField(\"revenue_diff\", DoubleType())])\n",
    "\n",
    "def subtract_mean(pdf):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Limitations of Grouped Map UDFs\n",
    "\n",
    "Grouped Map UDFs are the most flexible Spark Pandas UDFs in regards with the return type. A Grouped Map UDF always returns a `pandas.DataFrame`, but with an arbitrary amount of rows and columns (although the columns need to be defined in the schema in the Python decorator `@pandas_udf`). This means specifically that the number of rows is not fixed as opposed to scalar UDFs (where the number of output rows must match the number of input rows) and grouped map UDFs (which can only produce a single scalar value per incoming group)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
