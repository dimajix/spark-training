{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Load sales data from S3 / HDFS. We use the built-in \"csv\" method, which can use the first line has column names and which also supports infering the schema automatically. We use both and save some code for specifying the schema explictly.\n",
    "\n",
    "We also peek inside the data by retrieving the first five records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = spark.read\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .csv(\"s3a://dimajix-training/data/kc-house-data\")\n",
    "\n",
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Schema\n",
    "\n",
    "Now that we have loaded the data and that the schema was inferred automatically, let's inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Investigations\n",
    "\n",
    "As a first step to get an idea of our data, we create some simple visualizations. We use the Python matplot lib package for creating simple two-dimensional plots, where the x axis will be one of the provided attributes and the y axis will be the house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import relevant Python packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Price in Relation to sqft_living\n",
    "\n",
    "Probably one of the most important attributes is the size of the house. This is provided in the data in the column \"sqft_living\". We extract the price column and the sqft_living column and create a simple scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract price and attribute\n",
    "price = data.select(\"price\").toPandas()\n",
    "sqft_living = data.select(\"sqft_living\").toPandas()\n",
    "\n",
    "# Create simple scatter plot\n",
    "plt.plot(sqft_living, price, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price in Relation to sqft_lot\n",
    "\n",
    "Another interesting attribute for predicting the house price might be the size of the whole lot, which is provided in the column \"sqft_lot\". So let's create another plot, now with \"price\" and \"sqft_lot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Linear Regression\n",
    "\n",
    "Let's try to fit a line into the picture by performing a linear regression. This is done in two steps:\n",
    "1. Extract so called features from the raw data. The features have to be stored in a new column of type \"Vector\"\n",
    "2. Train a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "\n",
    "# Extract features using VectorAssembler\n",
    "vector_assembler = VectorAssembler(inputCols=['sqft_living'], outputCol='features')\n",
    "features = vector_assembler.transform(data)\n",
    "\n",
    "# Traing linear regression model\n",
    "regression = LinearRegression(featuresCol='features',labelCol='price')\n",
    "model = regression.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Model\n",
    "\n",
    "Let's inspect the generated linear model. It has two fields, \"intercept\" and \"coefficients\" which completely describe the model.\n",
    "\n",
    "The basic formular of the model is\n",
    "\n",
    "    y = SUM(coeff[i]*x[i]) + intercept\n",
    "\n",
    "where y is the prediction variable, and x[i] are the input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Intercept: \" + str(model.intercept))\n",
    "print(\"Coefficients: \" + str(model.coefficients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data and ModelÂ¶\n",
    "\n",
    "Now let's overlay the original scatter plot with the trained model. The model encodes a line, which can be overlayed by an additional invocation of \"plt.plot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For plotting the model, we need to generate input and output values. Input values are stored in \"model_x\"\n",
    "model_x = np.linspace(0,14000,100)\n",
    "# model_y contains the model applied to model_x. The model has only one feature and an intercept\n",
    "model_y = model_x * model.coefficients[0] + model.intercept\n",
    "\n",
    "plt.plot(sqft_living, price, \".\")\n",
    "plt.plot(model_x, model_y, \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Fit\n",
    "\n",
    "Now the important question of course is, how well does the model approximate the real data. We can find our by transforming our input data using the model. This is done by using the function\n",
    "\n",
    "    model.transform\n",
    "    \n",
    "which accepts one parameter and adds a new column \"prediction\" to input data, which contains the evaluated model for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = ... # YOUR CODE HERE\n",
    "\n",
    "# Take the first five records of the result \"prediction\" and display it as a Pandas dataframe\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Calculate RMSE\n",
    "\n",
    "Using SQL we compute the root mean squared error (RMSE). Formally it is calculated as\n",
    "\n",
    "    SQRT(SUM((price - prediction)**2) / n)\n",
    "    \n",
    "where n is the number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "prediction.selectExpr(...).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Built in Functionality to Measure the Fit\n",
    "Of course Spark ML already contains evaluators for the most relevant metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Generalization of Model\n",
    "\n",
    "Now we have an idea how well the model approximates the given data. But for machine learning it is more important to understand how well a model generalizes from the training data to new data. New data could contain different outliers.\n",
    "\n",
    "In order to measure the generalization of the model, we need to change our high level approach. Our new approach needs to provide distinct sets of training data and test data. We can create such data using the Spark method \"randomSplit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = features.randomSplit([0.8,0.2], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a linear regression model\n",
    "regression = # YOUR CODE HERE\n",
    "model = # YOUR CODE HERE\n",
    "\n",
    "# Now create predictions, but this time for the \"test_data\" and NOT for the training data itself\n",
    "prediction = # YOUR CODE HERE\n",
    "\n",
    "# Evaluate model using RegressionEvaluator again, but this time using the \"prediction\" data frame\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Prediction\n",
    "\n",
    "Now that we have a metric and a valid approachm, the next question is: How can we improve the model? So far we only used the column \"sqft_living\" for building the model, but we have much more information about the houses. A very simple way is to include more attributes into the feature vector.\n",
    "\n",
    "Remember that the schema looked as follows:\n",
    "\n",
    "    root\n",
    "     |-- id: long (nullable = true)\n",
    "     |-- date: string (nullable = true)\n",
    "     |-- price: decimal(7,0) (nullable = true)\n",
    "     |-- bedrooms: integer (nullable = true)\n",
    "     |-- bathrooms: double (nullable = true)\n",
    "     |-- sqft_living: integer (nullable = true)\n",
    "     |-- sqft_lot: integer (nullable = true)\n",
    "     |-- floors: double (nullable = true)\n",
    "     |-- waterfront: integer (nullable = true)\n",
    "     |-- view: integer (nullable = true)\n",
    "     |-- condition: integer (nullable = true)\n",
    "     |-- grade: integer (nullable = true)\n",
    "     |-- sqft_above: integer (nullable = true)\n",
    "     |-- sqft_basement: integer (nullable = true)\n",
    "     |-- yr_built: integer (nullable = true)\n",
    "     |-- yr_renovated: integer (nullable = true)\n",
    "     |-- zipcode: integer (nullable = true)\n",
    "     |-- lat: double (nullable = true)\n",
    "     |-- long: double (nullable = true)\n",
    "     |-- sqft_living15: integer (nullable = true)\n",
    "     |-- sqft_lot15: integer (nullable = true)\n",
    "     \n",
    "We simply use all real numeric columns. Some columns like \"condition\", \"grade\", \"zipcode\" are categorical variables, which we don't want to use now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract features using VectorAssembler\n",
    "vector_assembler = VectorAssembler(inputCols=[\n",
    "            'bedrooms',\n",
    "            'bathrooms',\n",
    "            'sqft_living',\n",
    "            'sqft_lot',\n",
    "            'floors',\n",
    "            'sqft_above',\n",
    "            'sqft_basement',\n",
    "            'yr_built',\n",
    "            'yr_renovated',\n",
    "            'sqft_living15',\n",
    "            'sqft_lot15'], \n",
    "        outputCol='features')\n",
    "features = vector_assembler.transform(data)\n",
    "\n",
    "# Again split into training and test data\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Traing linear regression model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Evaluate model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.1 (Python 3.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
