{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Refine Data\n",
    "\n",
    "The second step for analyzing the data is to perform some additional preparations and enrichments. While the first step of storing the data into the structured zone should be mainly a technical conversion without losing any information, this next step will integrate some data and also preaggregate weather data to simplify working with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Prepare Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"64G\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read Taxi Data\n",
    "\n",
    "Now we can read in the taxi data from the structured zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Trip Data\n",
    "\n",
    "Let us load the NYC Taxi trip data from the Hive table `taxi.trip` and let us display the first 10 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data = # YOUR CODE HERE\n",
    "trip_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be sure, let us inspect the schema. It should match exactly the specified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Fare information\n",
    "\n",
    "Now we read in the second table `taxi.fare` containing the trips fare information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_data = spark.read.table(\"taxi.fare\")\n",
    "fare_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Join datasets\n",
    "\n",
    "We can now join both the trip information and the fare information together in order to get a complete picture. Since the trip records do not contain a technical unique key, we use the following columns as the composite primary key of each trip:\n",
    "* medallion\n",
    "* hack_license\n",
    "* vendor_id\n",
    "* pickup_datetime\n",
    "\n",
    "Finally the result is stored into the refined zone into the Hive table `refined.taxi_trip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hive database 'refined'\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join trip_data with fare_data using the columns \"medallion\", \"hack_license\", \"vendor_id\", \"pickup_datetime\"\n",
    "taxi_trips = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save taxi_trips into the Hive table \"refined.taxi_trip\"\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from Refined Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips = spark.read.table(\"refined.taxi_trip\")\n",
    "taxi_trips.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the schema of the refined table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us count the number of records in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weather Data\n",
    "\n",
    "The weather data also requires some additional preprocessing, especially when we want to join against weather data. The primary problem of all measurements is, that they might happen at different time intervals and not all measurements contain all metrics. Therefore we preaggregate the weather data to hourly and daily measurements, which can directly be used for joining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Weather Data\n",
    "\n",
    "We already have weather data, but only individual measurements. We do not know how many measurements there are per hour and per day, so the raw table is not very useable for joining. Instead we'd like to have an hourly and a daily weather table containing average temperature, wind speed and precipitation. Since we are only interested in the year 2013, we also only load that specific year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = spark.read.table(\"isd.weather\").where(f.col(\"year\") == 2013)\n",
    "weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Calculate derived metrics and preaggregate data\n",
    "\n",
    "In order to simplify joining against weather data, we now preaggregate weather measurements to a single record per weather station and hour or per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly Preaggregation\n",
    "\n",
    "For the hourly aggregation, we want to get the following columns\n",
    "* `date` - day of the measurements. The day can be extracted from the timestamp column `ts` by using the Spark function `to_date` (available in the imported module `f`)\n",
    "* `hour` - hour of the measurements. The hour can be extracted using the Spark function `hour`\n",
    "* Grouping should be performed on the weather station IDs `usaf` and `wban` together with both extracted time columns `date` and `hour`\n",
    "* For the following metrics, we are interested in the averages: `wind_speed`, `air_temperature` and `precipitation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather = # YOUR CODE HERE\n",
    "\n",
    "hourly_weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Preaggregation\n",
    "\n",
    "In addition to the hourly metrics, we also preaggregate the data to daily records. This can easily be performed based on the hourly aggregations with a grouping on `usaf`, `wban` and `date`. Again we want to have the metrics `temperature`, `wind_speed` and `precipitation`. For the first two metrics, we are interested in the average (as this seems to make sense), while for precipitation we are interested in the sum (total amount of rainfall per day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather = # YOUR CODE HERE\n",
    "\n",
    "daily_weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Preaggregated Weather\n",
    "\n",
    "Finally we save both tables (hourly and daily weather), so we can directly reuse the data in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather.write.format(\"parquet\").saveAsTable(\"refined.weather_hourly\")\n",
    "daily_weather.write.format(\"parquet\").saveAsTable(\"refined.weather_daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Reload Data and draw Pictures\n",
    "\n",
    "Now let us reload the data (just to make sure everything worked out nicely) and let's draw some pictures. We use a single station (which, by pure incident, is a weather station in NYC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather = spark.read.table(\"refined.weather_daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_station_usaf = \"725053\"\n",
    "nyc_station_wban = \"94728\"\n",
    "\n",
    "# Filter data only of that weather station, order it by date and convert it to a Pandas DataFrame\n",
    "pdf = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind Speed\n",
    "\n",
    "The first picture will simply contain the wind speed for every day in 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Plot\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(pdf[\"date\"],pdf[\"wind_speed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Air Temperature\n",
    "\n",
    "The next picture contains the average air temperature for every day in 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Plot\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(pdf[\"date\"],pdf[\"temperature\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation\n",
    "\n",
    "The last picture contains the precipitation for every day in 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Plot\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(pdf[\"date\"],pdf[\"precipitation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.4 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
