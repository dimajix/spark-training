{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Machine Learning\n",
    "\n",
    "Finally we want to apply some machine learning to the taxi data. We will try to build a model for answering the following question:\n",
    "\n",
    "    As a Taxi driver, what time and location would be best on a specific day to make most money.\n",
    "    \n",
    "This is a relevant question in order to help maximizing a Taxi drivers income. We will try to build a model which predicts the total fare amount for each date and hour and each (grid) location. This way the taxi driver can predict the expected total amount of money being made at a specific point in time at a specific location, so he can better plan his shifts.\n",
    "\n",
    "Of course given the data that we currently have, we cannot precisely give an answer, because even locations with a low total fare amount may be attractive, as long as there are only very few taxi cabs. But we do not know how many taxi cabs were simply waiting or were driving around without passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwh_basedir = \"/user/hadoop/nyc-dwh\"\n",
    "integrated_basedir = dwh_basedir + \"/integrated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Setup Environment\n",
    "\n",
    "Before we begin, we create a local Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"64G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read Taxi Data\n",
    "\n",
    "Now we can read in the taxi data from the structured zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_taxi_trips = spark.read.parquet(integrated_basedir + \"/taxi-trips-hourly\")\n",
    "hourly_taxi_trips.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_taxi_trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Split Training and Validation set\n",
    "\n",
    "As a first step, we split up the whole data set into a training and a validation data set. Typical data sets are split randomly, but for time series data sets a non-random split is preferrable in order to avoid an undesired information creep from future observations. Therefore we create a split filtering by date, such that about 80% of records are used for training and the remaining 20% of all records will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 1 - training_fraction\n",
    "split_date = datetime.date(2013, 1, 1) + datetime.timedelta(days=training_fraction*(365))\n",
    "print(\"split_date=\\\"\" + str(split_date) + \"\\\"\")\n",
    "\n",
    "# Pickup all records with a date lower than the split date\n",
    "training_data = # YOUR CODE HERE\n",
    "# Pickup all records with a date grater or equal than the split date\n",
    "validation_data = # YOUR CODE HERE\n",
    "\n",
    "# Count the number of records for both the training and the validation data set\n",
    "training_data_count = # YOUR CODE HERE\n",
    "validation_data_count = # YOUR CODE HERE\n",
    "\n",
    "print(\"training_data count = \" + str(training_data_count))\n",
    "print(\"validation_data count = \" + str(validation_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Features\n",
    "\n",
    "As a first step we need to create so called *features* from the training data set. Most PySpark ML algorithms expect two specific input columns: A so called *label* column containing the true value and a so called *features* column containing a vector of all variables used for prediction. \n",
    "\n",
    "The label column has to be a simple numeric value, in our case it will be the *total amount*.  The features column needs to contain the special data type *vector*, which is constructed from various attributes of our observations. Some of these attributes can be taken directly from our training data set, while other columns also need to be derived from the original values.\n",
    "\n",
    "### Feature Engineering Building Blocks\n",
    "PySpark provides lots of different feature engineering algorithms as building blocks. These building blocks are simple (or complex) transformations which typically will add new derived columns to a data frame. We will see how we can chain multiple of these components together into a so called *pipeline* later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Transformer\n",
    "PySpark provides a very generic building block for transformation which simply executes some SQL. For example for creating a combined geo location inside a single column, we can use the following SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_location_transformer = # YOUR CODE HERE\n",
    "\n",
    "training_data_1 = geo_location_transformer.transform(training_data)\n",
    "training_data_1.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "One important case where the original values cannot be used directly is categorial data. For example the geo location cannot be used as a numerical value. Therefore we need a transformation which creates numerical values from this categorial feature. PySpark provides the pair of a *string indexer* followed by *one hot encoding* to create a separate multidimensional vector for each categorial variable. The pattern is always the same:\n",
    "\n",
    "```\n",
    "categorial data => StringIndexer => OneHotEncoder => vector\n",
    "```\n",
    "\n",
    "Specifically the code for the geo location looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create an index into all geo locations\n",
    "geo_indexer = # YOUR CODE HERE\n",
    "geo_index_model = # YOUR CODE HERE\n",
    "training_data_2 = # YOUR CODE HERE\n",
    "\n",
    "# Display some records\n",
    "training_data_2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now one-hot encode the generated index value\n",
    "geo_encoder = # YOUR CODE HERE\n",
    "geo_encoder_model = # YOUR CODE HERE\n",
    "training_data_3 = # YOUR CODE HERE\n",
    "\n",
    "# Display some records\n",
    "training_data_2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Assembler\n",
    "\n",
    "As already noted at the beginning of this section, PySpark requires all features to be available in a single column. This can be achieved by using a *vector assembler*, which will glue together all specified numerical and vector columns into a single vector column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = # YOUR CODE HERE\n",
    "\n",
    "training_data_4 = # YOUR CODE HERE\n",
    "\n",
    "training_data_4.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Now we have met all relevant building blocks. Instead of manually chaining these transformations together, you always should use a *pipeline*, where you can simply specify all transformations to apply. The pipeline also takes care of performing any `fit` phase of some transformers (like `StringIndexer` or `OneHotEncoderEstimator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "feature_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        SQLTransformer(\n",
    "            statement=\"\"\"\n",
    "                SELECT\n",
    "                    total_amount,\n",
    "                    date,\n",
    "                    hour,\n",
    "                    daily_temperature,\n",
    "                    hourly_temperature,\n",
    "                    daily_precipitation,\n",
    "                    hourly_precipitation,\n",
    "                    daily_wind_speed,\n",
    "                    hourly_wind_speed,\n",
    "                    month(`date`) - 1 AS `month_idx`,\n",
    "                    dayofweek(`date`) - 1 AS `weekday_idx`,\n",
    "                    CASE\n",
    "                        WHEN lat_idx IS NULL OR lat_idx < 0 THEN NULL\n",
    "                        WHEN long_idx IS NULL  OR long_idx < 0 THEN NULL\n",
    "                        ELSE concat(lat_idx, \"/\", long_idx) \n",
    "                    END AS geo_location,\n",
    "                    CASE WHEN\n",
    "                        bank_holiday = true THEN 1\n",
    "                        ELSE 0\n",
    "                    END AS bank_holiday\n",
    "                FROM __THIS__\n",
    "            \"\"\"\n",
    "        ),\n",
    "        # Create one hot encoded geo location via StringIndexer and OneHotEncoderEstimator\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Create one hot encoded hour via OneHotEncoderEstimator\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Create one hot encoded weekday via OneHotEncoderEstimator\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Assembler the following columns\n",
    "        #  - one hot encoded weekday\n",
    "        #  - one hot encoded hour\n",
    "        #  - bank holiday\n",
    "        #  - one hot encoded geo location\n",
    "        #  - daily temperature\n",
    "        #  - hourly temperature\n",
    "        #  - daily precipitation\n",
    "        #  - hourly precipitation\n",
    "        #  - daily wind speed\n",
    "        #  - hourly wind speed\n",
    "        VectorAssembler(\n",
    "            handleInvalid=\"skip\",\n",
    "            inputCols=[\n",
    "                # YOUR CODE HERE\n",
    "            ],\n",
    "            outputCol='features'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature model now contains all stages and can be used as a transformer. Let's transform and display the training data as a quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training_data = # YOUR CODE HERE\n",
    "features_training_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we specified `skip` as the strategy to handle invalid records in the vector assembler, let us check how many records where dropped from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data_count = feature_model.transform(training_data).count()\n",
    "\n",
    "print(\"feature_data_count = \" + str(features_data_count))\n",
    "print(\"training_data_count = \" + str(training_data_count))\n",
    "print(\"skipped records = \" + str(training_data_count - features_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model\n",
    "\n",
    "So far we have only prepared the data by adding a feature column containg some information which should be used as independant variables for prediction. Now we finally want to train a model which makes use of these features and predict the total amount for every date, hour and geo location.\n",
    "\n",
    "We create another pipeline, which contains the feature pipeline as its first entry and a simple linear regression as its second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pipeline = Pipeline(stages=[\n",
    "    feature_pipeline,\n",
    "    # YOUR CODE HERE\n",
    "])\n",
    "\n",
    "pred_model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Prediction\n",
    "\n",
    "The `pred_model` now contains all feature transformation steps of the feature pipeline and a linear model. We can directly use this model for performing predictions of the total amount.\n",
    "\n",
    "Now we use the validation data for prediction, which we set aside at the beginning and which could not influence the training phase in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to display some columns of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data\\\n",
    "    .orderBy(\"date\", \"hour\") \\\n",
    "    .select(\n",
    "        \"date\", \"hour\",\n",
    "        \"geo_location\",\n",
    "        \"total_amount\",\n",
    "        \"pred_total_amount\"\n",
    "    ) \\\n",
    "    .limit(10)\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Validation\n",
    "\n",
    "When looking at the predictions and comparing them with the true total amounts, we already suspect that our model does not perform very well. Now we want to quantify the goodness of fit using an appropriate evaluator. Note that evaluation should always be performed with the validation data, since we are not interested very much into the question how well a model describes the training data, but instead we want to understand how well the model performs with new data, which was not used during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "evaluator = # YOUR CODE HERE\n",
    "\n",
    "rmse = # YOUR CODE HERE\n",
    "\n",
    "print(\"rmse = \" + str(rmse))\n",
    "print(\"real_avg = \" + str(validation_data.select(f.avg(\"total_amount\")).first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently our model is not very good, since its RMSE is about twice as large as the average value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Baseline Model\n",
    "\n",
    "We already saw that the model is not very good. But what can we expect? It is always helpful to come up with a very simple base line model, and every true model should better beat the base line model. In our case, we simply use the average total amount as a constant base line model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_total_amount = # YOUR CODE HERE\n",
    "baseline_validation_data = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the base line model, we can now calculate the RMSE of this model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(baseline_validation_data)\n",
    "\n",
    "print(\"rmse = \" + str(rmse))\n",
    "print(\"real_avg = \" + str(validation_data.select(f.avg(\"total_amount\")).first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Improve Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Integrate information from the past\n",
    "\n",
    "Depending on the scenario, it can be completely legal to use data from the past as an additional feature. In this example, we assume that we can use the total amount from at least three days ago for the same location as an additional features. This implies that we assume that these numbers ares available at the time when new predictions are made.\n",
    "\n",
    "In other scenarios, the minimum amount of time to go back into the past, may be much larger or smaller. The importat question here is always what data is available when a new prediciton is performed.\n",
    "\n",
    "One important aspect of this approach is that no data may be available for some locations. But since our algorithms always require that all observations contain valid numbers, we fill the overall average of the metric over all locations for a specific date and hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Window Functions\n",
    "\n",
    "By using so called window functions, we can access data from different rows for the same geo location. In this example, we calculate the average of the total amount of the same hour, but seven days ago. We will add more data from the past afterwards.\n",
    "\n",
    "A Spark window function can be used with any aggregation by specifying the range in a `over` clause\n",
    "\n",
    "```\n",
    "f.avg(\"total_amount\") \\\n",
    "    .over(\n",
    "        Window.partitionBy(\"lat_idx\", \"long_idx\") \\\n",
    "            .orderBy(\"ts\") \\\n",
    "            .rangeBetween(-7*24*60*60,-(7*24-1)*60*60)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "hourly_taxi_trips_ext = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat_idx</th>\n",
       "      <th>long_idx</th>\n",
       "      <th>trip_count</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>holiday_description</th>\n",
       "      <th>bank_holiday</th>\n",
       "      <th>hourly_wind_speed</th>\n",
       "      <th>hourly_temperature</th>\n",
       "      <th>hourly_precipitation</th>\n",
       "      <th>daily_temperature</th>\n",
       "      <th>daily_wind_speed</th>\n",
       "      <th>daily_precipitation</th>\n",
       "      <th>ts</th>\n",
       "      <th>total_amount_P7D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3084</td>\n",
       "      <td>5691</td>\n",
       "      <td>35249.0</td>\n",
       "      <td>4251.12</td>\n",
       "      <td>42693.57</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359763200</td>\n",
       "      <td>34775.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3072</td>\n",
       "      <td>5814</td>\n",
       "      <td>36131.5</td>\n",
       "      <td>4534.85</td>\n",
       "      <td>43823.40</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359766800</td>\n",
       "      <td>36133.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2930</td>\n",
       "      <td>5406</td>\n",
       "      <td>34157.5</td>\n",
       "      <td>4162.83</td>\n",
       "      <td>41329.23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359770400</td>\n",
       "      <td>31693.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3709</td>\n",
       "      <td>25022.5</td>\n",
       "      <td>2787.28</td>\n",
       "      <td>29859.68</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6.2</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359774000</td>\n",
       "      <td>25013.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1004</td>\n",
       "      <td>1795</td>\n",
       "      <td>13438.0</td>\n",
       "      <td>1268.93</td>\n",
       "      <td>15796.78</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.1</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359777600</td>\n",
       "      <td>14193.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>615</td>\n",
       "      <td>5087.5</td>\n",
       "      <td>448.56</td>\n",
       "      <td>5932.96</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5.7</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359781200</td>\n",
       "      <td>5673.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>382</td>\n",
       "      <td>3529.0</td>\n",
       "      <td>325.21</td>\n",
       "      <td>4027.36</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.6</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359784800</td>\n",
       "      <td>3744.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "      <td>510</td>\n",
       "      <td>4486.0</td>\n",
       "      <td>556.77</td>\n",
       "      <td>5278.87</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359788400</td>\n",
       "      <td>5101.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>646</td>\n",
       "      <td>1068</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>734.98</td>\n",
       "      <td>7871.93</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359792000</td>\n",
       "      <td>7261.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>984</td>\n",
       "      <td>1634</td>\n",
       "      <td>9785.5</td>\n",
       "      <td>1127.77</td>\n",
       "      <td>11481.32</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.370833</td>\n",
       "      <td>3.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1359795600</td>\n",
       "      <td>10919.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  hour  lat_idx  long_idx  trip_count  passenger_count  \\\n",
       "0  2013-02-02     0        1         0        3084             5691   \n",
       "1  2013-02-02     1        1         0        3072             5814   \n",
       "2  2013-02-02     2        1         0        2930             5406   \n",
       "3  2013-02-02     3        1         0        2013             3709   \n",
       "4  2013-02-02     4        1         0        1004             1795   \n",
       "5  2013-02-02     5        1         0         348              615   \n",
       "6  2013-02-02     6        1         0         248              382   \n",
       "7  2013-02-02     7        1         0         323              510   \n",
       "8  2013-02-02     8        1         0         646             1068   \n",
       "9  2013-02-02     9        1         0         984             1634   \n",
       "\n",
       "   fare_amount  tip_amount  total_amount holiday_description bank_holiday  \\\n",
       "0      35249.0     4251.12      42693.57                None         None   \n",
       "1      36131.5     4534.85      43823.40                None         None   \n",
       "2      34157.5     4162.83      41329.23                None         None   \n",
       "3      25022.5     2787.28      29859.68                None         None   \n",
       "4      13438.0     1268.93      15796.78                None         None   \n",
       "5       5087.5      448.56       5932.96                None         None   \n",
       "6       3529.0      325.21       4027.36                None         None   \n",
       "7       4486.0      556.77       5278.87                None         None   \n",
       "8       6738.0      734.98       7871.93                None         None   \n",
       "9       9785.5     1127.77      11481.32                None         None   \n",
       "\n",
       "   hourly_wind_speed  hourly_temperature  hourly_precipitation  \\\n",
       "0                NaN                -3.3                   0.0   \n",
       "1                2.1                -3.3                   0.0   \n",
       "2                3.6                -3.9                   0.0   \n",
       "3                6.2                -4.4                   0.0   \n",
       "4                4.1                -4.4                   0.0   \n",
       "5                5.7                -4.4                   0.0   \n",
       "6                4.6                -5.0                   0.0   \n",
       "7                3.1                -5.6                   0.0   \n",
       "8                1.5                -6.1                   0.0   \n",
       "9                3.6                -6.7                   0.0   \n",
       "\n",
       "   daily_temperature  daily_wind_speed  daily_precipitation          ts  \\\n",
       "0          -4.370833          3.261905                  0.0  1359763200   \n",
       "1          -4.370833          3.261905                  0.0  1359766800   \n",
       "2          -4.370833          3.261905                  0.0  1359770400   \n",
       "3          -4.370833          3.261905                  0.0  1359774000   \n",
       "4          -4.370833          3.261905                  0.0  1359777600   \n",
       "5          -4.370833          3.261905                  0.0  1359781200   \n",
       "6          -4.370833          3.261905                  0.0  1359784800   \n",
       "7          -4.370833          3.261905                  0.0  1359788400   \n",
       "8          -4.370833          3.261905                  0.0  1359792000   \n",
       "9          -4.370833          3.261905                  0.0  1359795600   \n",
       "\n",
       "   total_amount_P7D  \n",
       "0          34775.37  \n",
       "1          36133.87  \n",
       "2          31693.87  \n",
       "3          25013.01  \n",
       "4          14193.87  \n",
       "5           5673.73  \n",
       "6           3744.35  \n",
       "7           5101.26  \n",
       "8           7261.41  \n",
       "9          10919.05  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_taxi_trips_ext.filter(f.col(\"date\") > \"2013-02-01\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more history\n",
    "\n",
    "Now we add more columns with more hours from the past by using multiple window expressions. We will denote the columns with `total_amount_<ISO Duration>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# Helper function for creating a history column name\n",
    "def column_name(prefix, days, hours):\n",
    "    if hours > 0:\n",
    "        timedelta = \"P\" + str(days) + \"DT\" + str(hours) + \"H\"\n",
    "    else:\n",
    "        timedelta = \"P\" + str(days) + \"D\"\n",
    "        \n",
    "    return prefix + \"_\" + timedelta\n",
    "\n",
    "\n",
    "# Function which adds history information for \"total_amount\" going back the specified number of days and hours\n",
    "def add_history(df, days, hours):\n",
    "    column = column_name(\"total_amount\", days, hours)\n",
    "    window_start = (days*24+hours)*60*60\n",
    "    window_end = window_start - 60*60\n",
    "    result = df.withColumn(column,\n",
    "                f.avg(\"total_amount\").over(Window.partitionBy(\"lat_idx\", \"long_idx\").orderBy(\"ts\").rangeBetween(-window_start, -window_end))\n",
    "            ) \n",
    "    return result\n",
    "\n",
    "# Add timestamp column, which is required for the Window function\n",
    "hourly_taxi_trips_ext = hourly_taxi_trips \\\n",
    "    .withColumn(\"ts\", f.unix_timestamp(f.concat(f.col(\"date\"),f.lit(\" \"),f.col(\"hour\")), \"yyyy-MM-dd H\"))\n",
    "\n",
    "# Add history for each hour for seven days\n",
    "df = hourly_taxi_trips_ext\n",
    "for day in range(10,2,-1):\n",
    "    for hour in range(23,-1,-1):\n",
    "        df = add_history(df, day, hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat_idx</th>\n",
       "      <th>long_idx</th>\n",
       "      <th>trip_count</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>holiday_description</th>\n",
       "      <th>...</th>\n",
       "      <th>total_amount_P3DT9H</th>\n",
       "      <th>total_amount_P3DT8H</th>\n",
       "      <th>total_amount_P3DT7H</th>\n",
       "      <th>total_amount_P3DT6H</th>\n",
       "      <th>total_amount_P3DT5H</th>\n",
       "      <th>total_amount_P3DT4H</th>\n",
       "      <th>total_amount_P3DT3H</th>\n",
       "      <th>total_amount_P3DT2H</th>\n",
       "      <th>total_amount_P3DT1H</th>\n",
       "      <th>total_amount_P3D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3084</td>\n",
       "      <td>5691</td>\n",
       "      <td>35249.0</td>\n",
       "      <td>4251.12</td>\n",
       "      <td>42693.57</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>15601.280</td>\n",
       "      <td>16972.870</td>\n",
       "      <td>24001.425</td>\n",
       "      <td>29154.720</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3072</td>\n",
       "      <td>5814</td>\n",
       "      <td>36131.5</td>\n",
       "      <td>4534.85</td>\n",
       "      <td>43823.40</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>16972.870</td>\n",
       "      <td>24001.425</td>\n",
       "      <td>29154.720</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2930</td>\n",
       "      <td>5406</td>\n",
       "      <td>34157.5</td>\n",
       "      <td>4162.83</td>\n",
       "      <td>41329.23</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>24001.425</td>\n",
       "      <td>29154.720</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3709</td>\n",
       "      <td>25022.5</td>\n",
       "      <td>2787.28</td>\n",
       "      <td>29859.68</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>29154.720</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1004</td>\n",
       "      <td>1795</td>\n",
       "      <td>13438.0</td>\n",
       "      <td>1268.93</td>\n",
       "      <td>15796.78</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>615</td>\n",
       "      <td>5087.5</td>\n",
       "      <td>448.56</td>\n",
       "      <td>5932.96</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>382</td>\n",
       "      <td>3529.0</td>\n",
       "      <td>325.21</td>\n",
       "      <td>4027.36</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "      <td>13335.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "      <td>510</td>\n",
       "      <td>4486.0</td>\n",
       "      <td>556.77</td>\n",
       "      <td>5278.87</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "      <td>13335.995</td>\n",
       "      <td>20798.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>646</td>\n",
       "      <td>1068</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>734.98</td>\n",
       "      <td>7871.93</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "      <td>13335.995</td>\n",
       "      <td>20798.740</td>\n",
       "      <td>23241.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>984</td>\n",
       "      <td>1634</td>\n",
       "      <td>9785.5</td>\n",
       "      <td>1127.77</td>\n",
       "      <td>11481.32</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "      <td>13335.995</td>\n",
       "      <td>20798.740</td>\n",
       "      <td>23241.515</td>\n",
       "      <td>20071.160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  hour  lat_idx  long_idx  trip_count  passenger_count  \\\n",
       "0  2013-02-02     0        1         0        3084             5691   \n",
       "1  2013-02-02     1        1         0        3072             5814   \n",
       "2  2013-02-02     2        1         0        2930             5406   \n",
       "3  2013-02-02     3        1         0        2013             3709   \n",
       "4  2013-02-02     4        1         0        1004             1795   \n",
       "5  2013-02-02     5        1         0         348              615   \n",
       "6  2013-02-02     6        1         0         248              382   \n",
       "7  2013-02-02     7        1         0         323              510   \n",
       "8  2013-02-02     8        1         0         646             1068   \n",
       "9  2013-02-02     9        1         0         984             1634   \n",
       "\n",
       "   fare_amount  tip_amount  total_amount holiday_description  ...  \\\n",
       "0      35249.0     4251.12      42693.57                None  ...   \n",
       "1      36131.5     4534.85      43823.40                None  ...   \n",
       "2      34157.5     4162.83      41329.23                None  ...   \n",
       "3      25022.5     2787.28      29859.68                None  ...   \n",
       "4      13438.0     1268.93      15796.78                None  ...   \n",
       "5       5087.5      448.56       5932.96                None  ...   \n",
       "6       3529.0      325.21       4027.36                None  ...   \n",
       "7       4486.0      556.77       5278.87                None  ...   \n",
       "8       6738.0      734.98       7871.93                None  ...   \n",
       "9       9785.5     1127.77      11481.32                None  ...   \n",
       "\n",
       "  total_amount_P3DT9H  total_amount_P3DT8H  total_amount_P3DT7H  \\\n",
       "0           15601.280            16972.870            24001.425   \n",
       "1           16972.870            24001.425            29154.720   \n",
       "2           24001.425            29154.720            30482.215   \n",
       "3           29154.720            30482.215            32745.990   \n",
       "4           30482.215            32745.990            34820.995   \n",
       "5           32745.990            34820.995            30702.955   \n",
       "6           34820.995            30702.955            21801.965   \n",
       "7           30702.955            21801.965            14469.490   \n",
       "8           21801.965            14469.490             9521.730   \n",
       "9           14469.490             9521.730             6108.380   \n",
       "\n",
       "   total_amount_P3DT6H  total_amount_P3DT5H  total_amount_P3DT4H  \\\n",
       "0            29154.720            30482.215            32745.990   \n",
       "1            30482.215            32745.990            34820.995   \n",
       "2            32745.990            34820.995            30702.955   \n",
       "3            34820.995            30702.955            21801.965   \n",
       "4            30702.955            21801.965            14469.490   \n",
       "5            21801.965            14469.490             9521.730   \n",
       "6            14469.490             9521.730             6108.380   \n",
       "7             9521.730             6108.380             4940.735   \n",
       "8             6108.380             4940.735             4026.995   \n",
       "9             4940.735             4026.995             6018.900   \n",
       "\n",
       "   total_amount_P3DT3H  total_amount_P3DT2H  total_amount_P3DT1H  \\\n",
       "0            34820.995            30702.955            21801.965   \n",
       "1            30702.955            21801.965            14469.490   \n",
       "2            21801.965            14469.490             9521.730   \n",
       "3            14469.490             9521.730             6108.380   \n",
       "4             9521.730             6108.380             4940.735   \n",
       "5             6108.380             4940.735             4026.995   \n",
       "6             4940.735             4026.995             6018.900   \n",
       "7             4026.995             6018.900            13335.995   \n",
       "8             6018.900            13335.995            20798.740   \n",
       "9            13335.995            20798.740            23241.515   \n",
       "\n",
       "   total_amount_P3D  \n",
       "0         14469.490  \n",
       "1          9521.730  \n",
       "2          6108.380  \n",
       "3          4940.735  \n",
       "4          4026.995  \n",
       "5          6018.900  \n",
       "6         13335.995  \n",
       "7         20798.740  \n",
       "8         23241.515  \n",
       "9         20071.160  \n",
       "\n",
       "[10 rows x 210 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(f.col(\"date\") > \"2013-02-01\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values\n",
    "\n",
    "We now have the problem that we do not neccessarily have a complete history for every geo location. In this case, the data set will contain NULL values which will cause errors in the algorithms later on. Therefore we need to impute some values. Spark provides some functionality for doing that, but that is very coarse (you can only replace all NULL values with a single common value). Instead we want to replace each NULL value by an average of every specific geo location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate average per geo location\n",
    "taxi_trips_avg = # YOUR CODE HERE\n",
    "\n",
    "# Helper function for imputing missing values for a specific hour on a specific date\n",
    "def impute_history(df, days, hours):\n",
    "    column = column_name(\"total_amount\", days, hours)\n",
    "    result = # YOUR CODE HERE \n",
    "    return result\n",
    "\n",
    "df = df.join(taxi_trips_avg, [\"lat_idx\",\"long_idx\"], how=\"leftOuter\")\n",
    "for day in range(10,2,-1):\n",
    "    for hour in range(23,-1,-1):\n",
    "        df = impute_history(df, day, hour)\n",
    "        \n",
    "# Clean up DataFrame        \n",
    "df = df.drop(\"avg_fare_amount\",\"avg_tip_amount\",\"avg_total_amount\")\n",
    "\n",
    "# Finally cache the result\n",
    "hourly_taxi_trips_ext = df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display some records again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat_idx</th>\n",
       "      <th>long_idx</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>trip_count</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>holiday_description</th>\n",
       "      <th>...</th>\n",
       "      <th>total_amount_P3DT9H</th>\n",
       "      <th>total_amount_P3DT8H</th>\n",
       "      <th>total_amount_P3DT7H</th>\n",
       "      <th>total_amount_P3DT6H</th>\n",
       "      <th>total_amount_P3DT5H</th>\n",
       "      <th>total_amount_P3DT4H</th>\n",
       "      <th>total_amount_P3DT3H</th>\n",
       "      <th>total_amount_P3DT2H</th>\n",
       "      <th>total_amount_P3DT1H</th>\n",
       "      <th>total_amount_P3D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>0</td>\n",
       "      <td>3084</td>\n",
       "      <td>5691</td>\n",
       "      <td>35249.0</td>\n",
       "      <td>4251.12</td>\n",
       "      <td>42693.57</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>15601.280</td>\n",
       "      <td>16972.870</td>\n",
       "      <td>24001.425</td>\n",
       "      <td>29154.720</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>1</td>\n",
       "      <td>3072</td>\n",
       "      <td>5814</td>\n",
       "      <td>36131.5</td>\n",
       "      <td>4534.85</td>\n",
       "      <td>43823.40</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>16972.870</td>\n",
       "      <td>24001.425</td>\n",
       "      <td>29154.720</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>2</td>\n",
       "      <td>2930</td>\n",
       "      <td>5406</td>\n",
       "      <td>34157.5</td>\n",
       "      <td>4162.83</td>\n",
       "      <td>41329.23</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>24001.425</td>\n",
       "      <td>29154.720</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>3709</td>\n",
       "      <td>25022.5</td>\n",
       "      <td>2787.28</td>\n",
       "      <td>29859.68</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>29154.720</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>4</td>\n",
       "      <td>1004</td>\n",
       "      <td>1795</td>\n",
       "      <td>13438.0</td>\n",
       "      <td>1268.93</td>\n",
       "      <td>15796.78</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>30482.215</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>5</td>\n",
       "      <td>348</td>\n",
       "      <td>615</td>\n",
       "      <td>5087.5</td>\n",
       "      <td>448.56</td>\n",
       "      <td>5932.96</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>32745.990</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>6</td>\n",
       "      <td>248</td>\n",
       "      <td>382</td>\n",
       "      <td>3529.0</td>\n",
       "      <td>325.21</td>\n",
       "      <td>4027.36</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>34820.995</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "      <td>13335.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>7</td>\n",
       "      <td>323</td>\n",
       "      <td>510</td>\n",
       "      <td>4486.0</td>\n",
       "      <td>556.77</td>\n",
       "      <td>5278.87</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>30702.955</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "      <td>13335.995</td>\n",
       "      <td>20798.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>8</td>\n",
       "      <td>646</td>\n",
       "      <td>1068</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>734.98</td>\n",
       "      <td>7871.93</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>21801.965</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "      <td>13335.995</td>\n",
       "      <td>20798.740</td>\n",
       "      <td>23241.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>9</td>\n",
       "      <td>984</td>\n",
       "      <td>1634</td>\n",
       "      <td>9785.5</td>\n",
       "      <td>1127.77</td>\n",
       "      <td>11481.32</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>14469.490</td>\n",
       "      <td>9521.730</td>\n",
       "      <td>6108.380</td>\n",
       "      <td>4940.735</td>\n",
       "      <td>4026.995</td>\n",
       "      <td>6018.900</td>\n",
       "      <td>13335.995</td>\n",
       "      <td>20798.740</td>\n",
       "      <td>23241.515</td>\n",
       "      <td>20071.160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lat_idx  long_idx        date  hour  trip_count  passenger_count  \\\n",
       "0        1         0  2013-02-02     0        3084             5691   \n",
       "1        1         0  2013-02-02     1        3072             5814   \n",
       "2        1         0  2013-02-02     2        2930             5406   \n",
       "3        1         0  2013-02-02     3        2013             3709   \n",
       "4        1         0  2013-02-02     4        1004             1795   \n",
       "5        1         0  2013-02-02     5         348              615   \n",
       "6        1         0  2013-02-02     6         248              382   \n",
       "7        1         0  2013-02-02     7         323              510   \n",
       "8        1         0  2013-02-02     8         646             1068   \n",
       "9        1         0  2013-02-02     9         984             1634   \n",
       "\n",
       "   fare_amount  tip_amount  total_amount holiday_description  ...  \\\n",
       "0      35249.0     4251.12      42693.57                None  ...   \n",
       "1      36131.5     4534.85      43823.40                None  ...   \n",
       "2      34157.5     4162.83      41329.23                None  ...   \n",
       "3      25022.5     2787.28      29859.68                None  ...   \n",
       "4      13438.0     1268.93      15796.78                None  ...   \n",
       "5       5087.5      448.56       5932.96                None  ...   \n",
       "6       3529.0      325.21       4027.36                None  ...   \n",
       "7       4486.0      556.77       5278.87                None  ...   \n",
       "8       6738.0      734.98       7871.93                None  ...   \n",
       "9       9785.5     1127.77      11481.32                None  ...   \n",
       "\n",
       "  total_amount_P3DT9H  total_amount_P3DT8H  total_amount_P3DT7H  \\\n",
       "0           15601.280            16972.870            24001.425   \n",
       "1           16972.870            24001.425            29154.720   \n",
       "2           24001.425            29154.720            30482.215   \n",
       "3           29154.720            30482.215            32745.990   \n",
       "4           30482.215            32745.990            34820.995   \n",
       "5           32745.990            34820.995            30702.955   \n",
       "6           34820.995            30702.955            21801.965   \n",
       "7           30702.955            21801.965            14469.490   \n",
       "8           21801.965            14469.490             9521.730   \n",
       "9           14469.490             9521.730             6108.380   \n",
       "\n",
       "   total_amount_P3DT6H  total_amount_P3DT5H  total_amount_P3DT4H  \\\n",
       "0            29154.720            30482.215            32745.990   \n",
       "1            30482.215            32745.990            34820.995   \n",
       "2            32745.990            34820.995            30702.955   \n",
       "3            34820.995            30702.955            21801.965   \n",
       "4            30702.955            21801.965            14469.490   \n",
       "5            21801.965            14469.490             9521.730   \n",
       "6            14469.490             9521.730             6108.380   \n",
       "7             9521.730             6108.380             4940.735   \n",
       "8             6108.380             4940.735             4026.995   \n",
       "9             4940.735             4026.995             6018.900   \n",
       "\n",
       "   total_amount_P3DT3H  total_amount_P3DT2H  total_amount_P3DT1H  \\\n",
       "0            34820.995            30702.955            21801.965   \n",
       "1            30702.955            21801.965            14469.490   \n",
       "2            21801.965            14469.490             9521.730   \n",
       "3            14469.490             9521.730             6108.380   \n",
       "4             9521.730             6108.380             4940.735   \n",
       "5             6108.380             4940.735             4026.995   \n",
       "6             4940.735             4026.995             6018.900   \n",
       "7             4026.995             6018.900            13335.995   \n",
       "8             6018.900            13335.995            20798.740   \n",
       "9            13335.995            20798.740            23241.515   \n",
       "\n",
       "   total_amount_P3D  \n",
       "0         14469.490  \n",
       "1          9521.730  \n",
       "2          6108.380  \n",
       "3          4940.735  \n",
       "4          4026.995  \n",
       "5          6018.900  \n",
       "6         13335.995  \n",
       "7         20798.740  \n",
       "8         23241.515  \n",
       "9         20071.160  \n",
       "\n",
       "[10 rows x 210 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_taxi_trips_ext.filter(f.col(\"date\") > \"2013-02-01\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Split Training and Validation set\n",
    "\n",
    "As a first step, we split up the whole data set into a training and a validation data set. Typical data sets are split randomly, but for time series data sets a non-random split is preferrable in order to avoid an undesired information creep from future observations. Therefore we create a split filtering by date, such that about 80% of records are used for training and the remaining 20% of all records will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 1 - training_fraction\n",
    "split_date = datetime.date(2013, 1, 7) + datetime.timedelta(days=training_fraction*(365-7))\n",
    "print(\"split_date=\\\"\" + str(split_date) + \"\\\"\")\n",
    "\n",
    "training_data = hourly_taxi_trips_ext.filter(f.col(\"date\") < split_date)\n",
    "validation_data = hourly_taxi_trips_ext.filter(f.col(\"date\") >= split_date)\n",
    "\n",
    "training_data_count = training_data.count()\n",
    "validation_data_count = validation_data.count()\n",
    "\n",
    "print(\"training_data count = \" + str(training_data_count))\n",
    "print(\"validation_data count = \" + str(validation_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create Features and Train Model\n",
    "\n",
    "Using building blocks of the PySpark ML package, we create a machine learning pipeline with all feature engineering steps and the regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing\n",
    "\n",
    "For some numerical features (like temperature and wind speed), it may be more appropriate to model them as categorical features. This can be done by *bucketing* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Buckets\n",
    "bucketizer = # YOUR CODE HERE\n",
    "training_data_1 = # YOUR CODE HERE\n",
    "\n",
    "training_data_1.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encode buckets\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[\"daily_temperature_bucket\"],\n",
    "    outputCols=[\"daily_temperature_onehot\"]\n",
    ")\n",
    "encoder_model = encoder.fit(training_data_1)\n",
    "training_data_2 = encoder_model.transform(training_data_1)\n",
    "\n",
    "training_data_2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Now we can create a more extensive pipeline, which makes use of more features and which also performs bucketing of the weather data.\n",
    "\n",
    "In particulat the pipeline performs the following steps:\n",
    "* one hot encode geo location\n",
    "* one hot encode hour\n",
    "* one hot encode day of week\n",
    "* bucketize all weather measurements\n",
    "* perform regression\n",
    "* truncate predictions to zero from below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_amount_P10DT23H',\n",
       " 'total_amount_P10DT22H',\n",
       " 'total_amount_P10DT21H',\n",
       " 'total_amount_P10DT20H',\n",
       " 'total_amount_P10DT19H']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_amount_history = [column_name(\"total_amount\", day, hour) for day in range(10,2,-1) for hour in range(23,-1,-1)]\n",
    "total_amount_history[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        SQLTransformer(\n",
    "            statement=\"\"\"\n",
    "                SELECT\n",
    "                    *,\n",
    "                    CASE WHEN\n",
    "                        bank_holiday = true THEN 0\n",
    "                        ELSE dayofweek(`date`)\n",
    "                    END AS weekday_idx,\n",
    "                    CASE\n",
    "                        WHEN lat_idx IS NULL OR lat_idx < 0 THEN NULL\n",
    "                        WHEN long_idx IS NULL  OR long_idx < 0 THEN NULL\n",
    "                        ELSE concat(lat_idx, \"/\", long_idx) \n",
    "                    END AS geo_location\n",
    "                FROM __THIS__\n",
    "            \"\"\"\n",
    "        ),\n",
    "        StringIndexer(\n",
    "            inputCol=\"geo_location\",\n",
    "            outputCol=\"geo_location_idx\",\n",
    "            handleInvalid=\"keep\"\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"geo_location_idx\"],\n",
    "            outputCols=[\"geo_location_onehot\"]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"hour\"],\n",
    "            outputCols=[\"hour_onehot\"]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"weekday_idx\"],\n",
    "            outputCols=[\"weekday_onehot\"]\n",
    "        ),\n",
    "        \n",
    "        Bucketizer(\n",
    "            inputCol=\"daily_temperature\",\n",
    "            outputCol=\"daily_temperature_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),-10,0,10,15,20,25,30,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"daily_temperature_bucket\"],\n",
    "            outputCols=[\"daily_temperature_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"hourly_temperature\",\n",
    "            outputCol=\"hourly_temperature_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),-10,0,10,15,20,25,30,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"hourly_temperature_bucket\"],\n",
    "            outputCols=[\"hourly_temperature_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"daily_precipitation\",\n",
    "            outputCol=\"daily_precipitation_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),0,100,200,300,400,500,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"daily_precipitation_bucket\"],\n",
    "            outputCols=[\"daily_precipitation_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"hourly_precipitation\",\n",
    "            outputCol=\"hourly_precipitation_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),0,50,100,150,200,250,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"hourly_precipitation_bucket\"],\n",
    "            outputCols=[\"hourly_precipitation_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"daily_wind_speed\",\n",
    "            outputCol=\"daily_wind_speed_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),0,1,2,3,4,5,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"daily_wind_speed_bucket\"],\n",
    "            outputCols=[\"daily_wind_speed_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"hourly_wind_speed\",\n",
    "            outputCol=\"hourly_wind_speed_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),0,1,2,3,4,5,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"hourly_wind_speed_bucket\"],\n",
    "            outputCols=[\"hourly_wind_speed_onehot\"]\n",
    "        ),\n",
    "        \n",
    "        # Linear Prediction\n",
    "        VectorAssembler(\n",
    "            handleInvalid=\"skip\",\n",
    "            inputCols=total_amount_history + [\n",
    "                'weekday_onehot',\n",
    "                'hour_onehot',\n",
    "                'geo_location_onehot',\n",
    "                'daily_temperature_onehot',\n",
    "                'hourly_temperature_onehot',\n",
    "                'daily_precipitation_onehot',\n",
    "                'hourly_precipitation_onehot',\n",
    "                'daily_wind_speed_onehot',\n",
    "                'hourly_wind_speed_onehot'\n",
    "            ],\n",
    "            outputCol='features'\n",
    "        ),\n",
    "        LinearRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"total_amount\",\n",
    "            predictionCol=\"pred_total_amount\"\n",
    "        ),\n",
    "        SQLTransformer(\n",
    "            statement=\"\"\"\n",
    "                SELECT\n",
    "                    date,\n",
    "                    hour,\n",
    "                    geo_location,\n",
    "                    total_amount,\n",
    "                    CASE \n",
    "                        WHEN pred_total_amount > 0 THEN pred_total_amount\n",
    "                        ELSE 0\n",
    "                    END AS pred_total_amount\n",
    "                FROM __THIS__\n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pred_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let us check how many training records have been dropped during the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data_count = pred_model.transform(training_data).count()\n",
    "\n",
    "print(\"training_data_count = \" + str(training_data_count))\n",
    "print(\"feature_data_count = \" + str(features_data_count))\n",
    "print(\"skipped records = \" + str(training_data_count - features_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data = pred_model.transform(validation_data)\n",
    "pred_validation_data\\\n",
    "    .orderBy(\"date\", \"hour\") \\\n",
    "    .select(\n",
    "        \"date\", \"hour\",\n",
    "        \"geo_location\",\n",
    "        \"total_amount\",\n",
    "        \"pred_total_amount\"\n",
    "    ) \\\n",
    "    .limit(10)\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol = \"total_amount\",\n",
    "    predictionCol = \"pred_total_amount\",\n",
    "    metricName = \"rmse\"\n",
    ")\n",
    "\n",
    "rmse = # YOUR CODE HERE\n",
    "\n",
    "print(\"rmse = \" + str(rmse))\n",
    "print(\"real_avg = \" + str(validation_data.select(f.avg(\"total_amount\")).first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Baseline Model\n",
    "\n",
    "In order to make sense of the number, we use a simple base line model as comparison again. This time we simply predict the total amount by using the previous total amount from seven days ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_validation_data = # YOUR CODE HERE\n",
    "\n",
    "rmse = # YOUR CODE HERE\n",
    "\n",
    "print(\"rmse = \" + str(rmse))\n",
    "print(\"real_avg = \" + str(validation_data.select(f.avg(\"total_amount\")).first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Best Time and Location\n",
    "\n",
    "Although the predicted values are not really satisfying so far, they are good enough for deciding when to make most money. In order to underline this claim, let us compare the top ten hours and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data.filter(\"date='2013-11-01'\") \\\n",
    "    .select(\"date\", \"hour\", \"geo_location\", \"total_amount\", \"pred_total_amount\") \\\n",
    "    .orderBy(f.desc(\"total_amount\")) \\\n",
    "    .limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data.filter(\"date='2013-11-01'\") \\\n",
    "    .select(\"date\", \"hour\", \"geo_location\", \"total_amount\", \"pred_total_amount\") \\\n",
    "    .orderBy(f.desc(\"pred_total_amount\")) \\\n",
    "    .limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Top 10 Recommendations\n",
    "\n",
    "Let us find out how good the recommendatations of our algorithm would be. We pick the ten best hour-locations for each day from the real data, the predicted data and the baseline model. For each selection, we also compute the *real total revenue* for all these location-hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real 10 best location-hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "real_best_locations = # YOUR CODE HERE\n",
    "\n",
    "real_best_locations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_top10_totals = # YOUR CODE HERE\n",
    "\n",
    "print(\"real_top10_totals = \" + str(real_top10_totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted 10 best location-hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_best_locations = pred_validation_data \\\n",
    "    .select(\n",
    "        f.col(\"date\"),\n",
    "        f.col(\"hour\"),\n",
    "        f.col(\"total_amount\"),\n",
    "        f.col(\"geo_location\"),\n",
    "        f.row_number().over(Window.partitionBy(\"date\").orderBy(f.col(\"pred_total_amount\").desc())).alias(\"row_number\")\n",
    "    ) \\\n",
    "    .filter(f.col(\"row_number\") < 10)\n",
    "\n",
    "pred_best_locations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_top10_totals = pred_best_locations.select(\n",
    "    f.sum(f.col(\"total_amount\"))\n",
    ").first()[0] \n",
    "\n",
    "print(\"pred_top10_totals = \" + str(pred_top10_totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 10 best location-hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_best_locations = validation_data \\\n",
    "    .select(\n",
    "        f.col(\"date\"),\n",
    "        f.col(\"hour\"),\n",
    "        f.col(\"total_amount\"),\n",
    "        f.concat(f.col(\"lat_idx\"), f.lit(\"/\"), f.col(\"long_idx\")).alias(\"geo_location\"),\n",
    "        f.row_number().over(Window.partitionBy(\"date\").orderBy(f.col(\"total_amount_P7D\").desc())).alias(\"row_number\")\n",
    "    ) \\\n",
    "    .filter(f.col(\"row_number\") < 10)\n",
    "\n",
    "baseline_best_locations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_top10_totals = baseline_best_locations.select(\n",
    "    f.sum(f.col(\"total_amount\"))\n",
    ").first()[0] \n",
    "\n",
    "print(\"baseline_top10_totals = \" + str(baseline_top10_totals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.4 (Python 3.7)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
