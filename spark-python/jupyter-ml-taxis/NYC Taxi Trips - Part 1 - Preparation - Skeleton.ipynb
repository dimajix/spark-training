{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Trips Example\n",
    "\n",
    "This data is freely available. You can find some interesting background information at https://chriswhong.com/open-data/foil_nyc_taxi/ . We will use this data to perform some analytical tasks. The whole wotkshop is split up into multiple sections, which represents the typical data processing flow in a data centric project. We will follow the (simplified) steps when using a data lake.\n",
    "\n",
    "1. Build \"Structured Zone\" containing all sources\n",
    "2. Build \"Refined Zone\" that contains pre-processed data\n",
    "3. Analyze the data before working on the next steps to find an appropriate approach\n",
    "4. Build \"Integrated Zone\" that contains integrated data\n",
    "5. Use Machine Learning for business questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "The workshop will require the following Python packages:\n",
    "\n",
    "* PySpark (tested with Spark 2.4)\n",
    "* Matplotlib\n",
    "* Pandas\n",
    "* GeoPandas\n",
    "* Cartopy\n",
    "* Contextily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Build Structured Zone\n",
    "\n",
    "The first part is about building the structured zone. It will contain a copy of the raw data stored in Hive tables and thereby easily accessible for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_basedir = \"s3://dimajix-training/data/nyc-taxi-trips/\"\n",
    "weather_basedir = \"s3://dimajix-training/data/weather/\"\n",
    "holidays_basedir = \"s3://dimajix-training/data/bank-holidays/\"\n",
    "dwh_basedir = \"/user/hadoop/nyc-dwh\"\n",
    "structured_basedir = dwh_basedir + \"/structured\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Create Spark Session\n",
    "\n",
    "Before we begin, we create a Spark session if none was provided in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"64G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Taxi Data\n",
    "\n",
    "This data is freely available. You can find some interesting background information at https://chriswhong.com/open-data/foil_nyc_taxi/ . In the first step we read in the raw data. The data is split into two different entities: Basic trip information and payment information. We will store the data in a more efficient representation (Parquet) to form the structured zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Trip Information\n",
    "\n",
    "We start with reading in the trip information. It contains the following columns\n",
    "* **medallion** - This is some sort of a license for a taxi company. A single medallion is attached to a single cab and may be used by multiple drivers.\n",
    "* **hack_license** - This is the drivers license\n",
    "* **vendor_id**\n",
    "* **rate_code** The final rate code in effect at the end of the trip. \n",
    "  * 1=Standard rate\n",
    "  * 2=JFK\n",
    "  * 3=Newark\n",
    "  * 4=Nassau or Westchester\n",
    "  * 5=Negotiated fare\n",
    "  * 6=Group ride\n",
    "* **store_and_fwd_flag** This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server\n",
    "* **pickup_datetime** This is the time when a passenger was picked up\n",
    "* **dropoff_datetime** This is the time when the passenger was dropped off again\n",
    "* **passenger_count** Number of passengers of this trip\n",
    "* **trip_time_in_secs**\n",
    "* **trip_distance**\n",
    "* **pickup_longitude**\n",
    "* **pickup_latitude**\n",
    "* **dropoff_longitude**\n",
    "* **dropoff_latitude**\n",
    "\n",
    "The primary key uniquely identifying each trip is given by the columns `medallion`, `hack_license`, `vendor_id` and `pickip_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "trip_schema = StructType([\n",
    "    StructField('medallion', StringType()),\n",
    "    StructField('hack_license', StringType()),\n",
    "    StructField('vendor_id', StringType()),\n",
    "    StructField('rate_code', StringType()),\n",
    "    StructField('store_and_fwd_flag', StringType()),\n",
    "    StructField('pickup_datetime', TimestampType()),\n",
    "    StructField('dropoff_datetime', TimestampType()),\n",
    "    StructField('passenger_count', IntegerType()),\n",
    "    StructField('trip_time_in_secs', IntegerType()),\n",
    "    StructField('trip_distance', DoubleType()),\n",
    "    StructField('pickup_longitude', DoubleType()),\n",
    "    StructField('pickup_latitude', DoubleType()),\n",
    "    StructField('dropoff_longitude', DoubleType()),\n",
    "    StructField('dropoff_latitude', DoubleType()),\n",
    "    ])\n",
    "\n",
    "trip_data = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first 10 rows by converting them to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Schema\n",
    "\n",
    "Just to be sure, let us inspect the schema. It should match exactly the specified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into Structured Zone\n",
    "\n",
    "Now we store data as parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Fare information\n",
    "\n",
    "Now we read in the second table containing the trips fare information.\n",
    "\n",
    "* **medallion** - This is some sort of a license for a taxi company\n",
    "* **hack_license** - This is the drivers license\n",
    "* **vendor_id**\n",
    "* **pickup_datetime** This is the time when a passenger was picked up\n",
    "* **payment_type** A numeric code signifying how the passenger paid for the trip. \n",
    "  * CRD = Credit card\n",
    "  * CDH = Cash\n",
    "  * ??? = No charge\n",
    "  * ??? = Dispute\n",
    "  * ??? = Unknown\n",
    "  * ??? = Voided trip\n",
    "* **fare_amount** The time-and-distance fare calculated by the meter\n",
    "* **surcharge**\n",
    "* **mta_tax** $0.50 MTA tax that is automatically triggered based on the metered rate in use\n",
    "* **tip_amount** Tip amount –This field is automatically populated for credit card tips. Cash tips are not included\n",
    "* **tolls_amount** Total amount of all tolls paid in trip\n",
    "* **total_amount** The total amount charged to passengers. Does not include cash tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_schema = StructType([\n",
    "    StructField('medallion', StringType()),\n",
    "    StructField('hack_license', StringType()),\n",
    "    StructField('vendor_id', StringType()),\n",
    "    StructField('pickup_datetime', TimestampType()),\n",
    "    StructField('payment_type', StringType()),\n",
    "    StructField('fare_amount', DoubleType()),\n",
    "    StructField('surcharge', DoubleType()),\n",
    "    StructField('mta_tax', DoubleType()),\n",
    "    StructField('tip_amount', DoubleType()),\n",
    "    StructField('tolls_amount', DoubleType()),\n",
    "    StructField('total_amount', DoubleType())\n",
    "    ])\n",
    "\n",
    "trip_fare = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\", True) \\\n",
    "    .schema(fare_schema) \\\n",
    "    .csv(taxi_basedir + \"/fare/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_fare.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Schema\n",
    "\n",
    "Let us inspect the schema of the data, which should match exactly the schema that we originally specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_fare.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store into Structured Zone\n",
    "\n",
    "Finally store the data into the structured zone as Parquet files into the sub directory `taxi-fare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_fare.write.parquet(structured_basedir + \"/taxi-fare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weather Data\n",
    "\n",
    "In order to improve our analysis, we will relate the taxi trips with weather information. We use the NOAA ISD weather data (https://www.ncdc.noaa.gov/isd), which contains measurements from many stations around the world, some of them dating back to 1901. You can download all data from ftp://ftp.ncdc.noaa.gov/pub/data/noaa . We will only use a small subset of the data which is good enough for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Station Master Data\n",
    "\n",
    "The weather data is split up into two different data sets: the measurements themselves and meta data about the stations. The later contains valuable information like the geo location of the weather station. This will be useful when trying to find the weather station nearest to all taxi trips.\n",
    "\n",
    "Among other data the columns provide specifically the following informations\n",
    "* **USAF** & **WBAN** - weather station id\n",
    "* **CTRY** - the country of the weather station\n",
    "* **STATE** - the state of the weather station\n",
    "* **LAT** & **LONG** - latitude and longitude of the weather station (geo coordinates)\n",
    "* **BEGIN** & **END** - date range when this weather station was active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stations = # YOUR CODE HERE\n",
    "\n",
    "weather_stations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data into Structured Zone\n",
    "\n",
    "In the next step we want to store the data as Parquet files (which are much more efficient and very well supported by most batch frameworks in the Hadoop and Spark universe). In order to do so, we first need to rename some columns, which contain unsupported characters:\n",
    "* \"STATION NAME\" => \"STATION_NAME\"\n",
    "* \"ELEV(M)\" => \"ELEVATION\"\n",
    "\n",
    "After the columns have been renamed, the data frame is written into the structured zone into the sub directory `weather-stations` using the `DataFrame.write.parquet` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stations \\\n",
    "    .withColumnRenamed(\"STATION NAME\", \"STATION_NAME\") \\\n",
    "    .withColumnRenamed(\"ELEV(M)\", \"ELEVATION\") \\\n",
    "    .write.parquet(structured_basedir + \"/weather-stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data agin\n",
    "\n",
    "Using the `spark.read.parquet` function we read in the data back into Spark and display some records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stations = # YOUR CODE HERE\n",
    "weather_stations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Weather Measurements\n",
    "\n",
    "Now we will work with the second and more interesting part of the NOAA weather data set: The measurements. These are stored in different subdirectories per year. For us, the year 2013 is good enough, since the taxi trips are all from 2013.\n",
    "\n",
    "The data format is a proprietary ASCII encoding, so we use the `spark.read.text` method to read each line as one record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = # YOUR CODE HERE\n",
    "raw_weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract precipitation\n",
    "\n",
    "Now we extract the precipitation from the measurements. This is not trivial, since that information is stored in a variable part. We assume that the record contains precipitation data when it contains the substring `AA1` at position 109. This denotes the type of the subsection in the data record followed by the number of hours of this measurement and the precipitation depth.\n",
    "\n",
    "We use some PySpark string functions to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather.select(\n",
    "        f.substring(raw_weather[\"value\"],106,999),\n",
    "        f.instr(raw_weather[\"value\"],\"AA1\").alias(\"s\"),\n",
    "        f.when(f.instr(raw_weather[\"value\"],\"AA1\") == 109,f.substring(raw_weather[\"value\"], 109+3, 8)).alias(\"AAD\")\n",
    "    )\\\n",
    "    .withColumn(\"precipitation_hours\", f.substring(f.col(\"AAD\"), 1, 2).cast(\"INT\")) \\\n",
    "    .withColumn(\"precipitation_depth\", f.substring(f.col(\"AAD\"), 3, 4).cast(\"FLOAT\")) \\\n",
    "    .filter(f.col(\"precipitation_depth\") > 0) \\\n",
    "    .limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all relevant measurements\n",
    "\n",
    "The precipitation was the hardest part. Other measurements like wind speed and air temperature are stored at fixed positions together with some quality flags denoting if a measurement is valid. In the following statement, we extract all relevant measurements. Specifically we extract the following information\n",
    "* **USAF** & **WBAN** - weather station identifier\n",
    "* **ts** - timestamp of measurement\n",
    "* **wind_direction** - wind direction (in degrees)\n",
    "* **wind_direction_qual** - quality flag of the wind direction\n",
    "* **wind_speed** - wind speed\n",
    "* **wind_speed_qual** - quality flag indicating the validity of the wind speed\n",
    "* **air_temperature** - air temperature in degree Celsius\n",
    "* **air_temperature_qual** - quality flag for air temperature\n",
    "* **precipitation_hours**\n",
    "* **precipitation_depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "        f.substring(raw_weather[\"value\"],5,6).alias(\"usaf\"),\n",
    "        f.substring(raw_weather[\"value\"],11,5).alias(\"wban\"),\n",
    "        f.to_timestamp(f.substring(raw_weather[\"value\"],16,12), \"yyyyMMddHHmm\").alias(\"ts\"),\n",
    "        f.substring(raw_weather[\"value\"],42,5).alias(\"report_type\"),\n",
    "        f.substring(raw_weather[\"value\"],61,3).alias(\"wind_direction\"),\n",
    "        f.substring(raw_weather[\"value\"],64,1).alias(\"wind_direction_qual\"),\n",
    "        f.substring(raw_weather[\"value\"],65,1).alias(\"wind_observation\"),\n",
    "        (f.substring(raw_weather[\"value\"],66,4).cast(\"float\") / 10.0).alias(\"wind_speed\"),\n",
    "        f.substring(raw_weather[\"value\"],70,1).alias(\"wind_speed_qual\"),\n",
    "        (f.substring(raw_weather[\"value\"],88,5).cast(\"float\") / 10.0).alias(\"air_temperature\"),\n",
    "        f.substring(raw_weather[\"value\"],93,1).alias(\"air_temperature_qual\"),\n",
    "        f.when(f.instr(raw_weather[\"value\"],\"AA1\") == 109,f.substring(raw_weather[\"value\"], 109+3, 8)).alias(\"AAD\")\n",
    "    ) \\\n",
    "    .withColumn(\"precipitation_hours\", f.substring(f.col(\"AAD\"), 1, 2).cast(\"INT\")) \\\n",
    "    .withColumn(\"precipitation_depth\", f.substring(f.col(\"AAD\"), 3, 4).cast(\"FLOAT\")) \\\n",
    "    .withColumn(\"date\", f.to_date(f.col(\"ts\"))) \\\n",
    "    .drop(\"AAD\")\n",
    "    \n",
    "weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store into Structured Zone\n",
    "\n",
    "After successful extraction, we write the result again into the structured zone into the subdirectory `weather/2013`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.write.parquet(structured_basedir + \"/weather/2013\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in from Structured Zone\n",
    "\n",
    "Again we read back the data from the Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = spark.read.parquet(structured_basedir + \"/weather/2013\")\n",
    "weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Holidays\n",
    "\n",
    "Another important data source is additional date information, specifically if a certain date is a bank holiday. While other information like week days can be directly computed from a date, for bank holidays an additional source is required.\n",
    "\n",
    "We follow again the same approach of reading in the raw data and storing it into the structured zone as Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_schema = StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('date', DateType()),\n",
    "    StructField('description', StringType()),\n",
    "    StructField('bank_holiday', BooleanType())\n",
    "    ])\n",
    "\n",
    "holidays = # YOUR CODE HERE\n",
    "\n",
    "holidays.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store into Structured Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.write.parquet(structured_basedir + \"/holidays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in from Structured Zone\n",
    "\n",
    "Again let us check if writing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = spark.read.parquet(structured_basedir + \"/holidays\")\n",
    "holidays.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.4 (Python 3.7)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
