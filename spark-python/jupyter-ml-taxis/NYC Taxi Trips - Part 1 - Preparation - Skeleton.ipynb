{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Trips Example\n",
    "\n",
    "This data is freely available. You can find some interesting background information at https://chriswhong.com/open-data/foil_nyc_taxi/ . We will use this data to perform some analytical tasks. The whole wotkshop is split up into multiple sections, which represents the typical data processing flow in a data centric project. We will follow the (simplified) steps when using a data lake.\n",
    "\n",
    "1. Build \"Structured Zone\" containing all sources\n",
    "2. Build \"Refined Zone\" that contains pre-processed data\n",
    "3. Analyze the data before working on the next steps to find an appropriate approach\n",
    "4. Build \"Integrated Zone\" that contains integrated data\n",
    "5. Use Machine Learning for business questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "The workshop will require the following Python packages:\n",
    "\n",
    "* PySpark (tested with Spark 2.4)\n",
    "* Matplotlib\n",
    "* Pandas\n",
    "* GeoPandas\n",
    "* Cartopy\n",
    "* Contextily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Build Structured Zone\n",
    "\n",
    "The first part is about building the structured zone. It will contain a copy of the raw data stored in Hive tables and thereby easily accessible for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_basedir = \"s3://dimajix-training/data/nyc-taxi-trips/\"\n",
    "weather_basedir = \"s3://dimajix-training/data/weather/\"\n",
    "holidays_basedir = \"s3://dimajix-training/data/bank-holidays/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Create Spark Session\n",
    "\n",
    "Before we begin, we create a Spark session if none was provided in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"64G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Taxi Data\n",
    "\n",
    "This data is freely available. You can find some interesting background information at https://chriswhong.com/open-data/foil_nyc_taxi/ . In the first step we read in the raw data. The data is split into two different entities: Basic trip information and payment information. We will store the data in a more efficient representation (Parquet) to form the structured zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Trip Information\n",
    "\n",
    "We start with reading in the trip information. It contains the following columns\n",
    "* **medallion** - This is some sort of a license for a taxi company. A single medallion is attached to a single cab and may be used by multiple drivers.\n",
    "* **hack_license** - This is the drivers license\n",
    "* **vendor_id**\n",
    "* **rate_code** The final rate code in effect at the end of the trip. \n",
    "  * 1=Standard rate\n",
    "  * 2=JFK\n",
    "  * 3=Newark\n",
    "  * 4=Nassau or Westchester\n",
    "  * 5=Negotiated fare\n",
    "  * 6=Group ride\n",
    "* **store_and_fwd_flag** This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server\n",
    "* **pickup_datetime** This is the time when a passenger was picked up\n",
    "* **dropoff_datetime** This is the time when the passenger was dropped off again\n",
    "* **passenger_count** Number of passengers of this trip\n",
    "* **trip_time_in_secs**\n",
    "* **trip_distance**\n",
    "* **pickup_longitude**\n",
    "* **pickup_latitude**\n",
    "* **dropoff_longitude**\n",
    "* **dropoff_latitude**\n",
    "\n",
    "The primary key uniquely identifying each trip is given by the columns `medallion`, `hack_license`, `vendor_id` and `pickip_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "trip_schema = StructType([\n",
    "    StructField('medallion', StringType()),\n",
    "    StructField('hack_license', StringType()),\n",
    "    StructField('vendor_id', StringType()),\n",
    "    StructField('rate_code', StringType()),\n",
    "    StructField('store_and_fwd_flag', StringType()),\n",
    "    StructField('pickup_datetime', TimestampType()),\n",
    "    StructField('dropoff_datetime', TimestampType()),\n",
    "    StructField('passenger_count', IntegerType()),\n",
    "    StructField('trip_time_in_secs', IntegerType()),\n",
    "    StructField('trip_distance', DoubleType()),\n",
    "    StructField('pickup_longitude', DoubleType()),\n",
    "    StructField('pickup_latitude', DoubleType()),\n",
    "    StructField('dropoff_longitude', DoubleType()),\n",
    "    StructField('dropoff_latitude', DoubleType()),\n",
    "    ])\n",
    "\n",
    "# Read in the data into a PySpark DataFrame using the schema above\n",
    "#  location: taxi_basedir + \"/data/\"\n",
    "#  schema: trip_schema\n",
    "#  format: csv\n",
    "#  header: True\n",
    "trip_data = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first 10 rows by converting them to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Schema\n",
    "\n",
    "Just to be sure, let us inspect the schema. It should match exactly the specified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into Structured Zone\n",
    "\n",
    "Now we store data into Hive tables as parquet files. In order to do that, we first create an empty Hive database \"taxi\", in order to reflect the source of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame trip_data into Hive by using the method `saveAsTable`\n",
    "#   format: parquet\n",
    "#   Hive table: taxi.trip\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Fare information\n",
    "\n",
    "Now we read in the second table containing the trips fare information.\n",
    "\n",
    "* **medallion** - This is some sort of a license for a taxi company\n",
    "* **hack_license** - This is the drivers license\n",
    "* **vendor_id**\n",
    "* **pickup_datetime** This is the time when a passenger was picked up\n",
    "* **payment_type** A numeric code signifying how the passenger paid for the trip. \n",
    "  * CRD = Credit card\n",
    "  * CDH = Cash\n",
    "  * ??? = No charge\n",
    "  * ??? = Dispute\n",
    "  * ??? = Unknown\n",
    "  * ??? = Voided trip\n",
    "* **fare_amount** The time-and-distance fare calculated by the meter\n",
    "* **surcharge**\n",
    "* **mta_tax** $0.50 MTA tax that is automatically triggered based on the metered rate in use\n",
    "* **tip_amount** Tip amount –This field is automatically populated for credit card tips. Cash tips are not included\n",
    "* **tolls_amount** Total amount of all tolls paid in trip\n",
    "* **total_amount** The total amount charged to passengers. Does not include cash tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_schema = StructType([\n",
    "    StructField('medallion', StringType()),\n",
    "    StructField('hack_license', StringType()),\n",
    "    StructField('vendor_id', StringType()),\n",
    "    StructField('pickup_datetime', TimestampType()),\n",
    "    StructField('payment_type', StringType()),\n",
    "    StructField('fare_amount', DoubleType()),\n",
    "    StructField('surcharge', DoubleType()),\n",
    "    StructField('mta_tax', DoubleType()),\n",
    "    StructField('tip_amount', DoubleType()),\n",
    "    StructField('tolls_amount', DoubleType()),\n",
    "    StructField('total_amount', DoubleType())\n",
    "    ])\n",
    "\n",
    "# Read in the Taxi fare information into a PySpark DataFrame trip_fare\n",
    "#  location: taxi_basedir + \"/fare/\"\n",
    "#  schema: fare_schema\n",
    "#  format: csv\n",
    "#  option 'header': True\n",
    "#  option 'ignoreLeadingWhiteSpace': True\n",
    "trip_fare = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_fare.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Schema\n",
    "\n",
    "Let us inspect the schema of the data, which should match exactly the schema that we originally specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_fare.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store into Structured Zone\n",
    "\n",
    "Finally store the data into the structured zone as Parquet files into the a different Hive table `taxi.fare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_fare.write.format(\"parquet\").saveAsTable(\"taxi.fare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weather Data\n",
    "\n",
    "In order to improve our analysis, we will relate the taxi trips with weather information. We use the NOAA ISD weather data (https://www.ncdc.noaa.gov/isd), which contains measurements from many stations around the world, some of them dating back to 1901. You can download all data from ftp://ftp.ncdc.noaa.gov/pub/data/noaa . We will only use a small subset of the data which is good enough for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Station Master Data\n",
    "\n",
    "The weather data is split up into two different data sets: the measurements themselves and meta data about the stations. The later contains valuable information like the geo location of the weather station. This will be useful when trying to find the weather station nearest to all taxi trips.\n",
    "\n",
    "Among other data the columns provide specifically the following informations\n",
    "* **USAF** & **WBAN** - weather station id\n",
    "* **CTRY** - the country of the weather station\n",
    "* **STATE** - the state of the weather station\n",
    "* **LAT** & **LONG** - latitude and longitude of the weather station (geo coordinates)\n",
    "* **BEGIN** & **END** - date range when this weather station was active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in weather station master data into a PySpark DataFrame weather_stations\n",
    "#  location: weather_basedir + \"/isd-history/\"\n",
    "#  format: csv\n",
    "#  header: True\n",
    "weather_stations = # YOUR CODE HERE\n",
    "\n",
    "weather_stations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data into Structured Zone\n",
    "\n",
    "In the next step we want to store the data as Parquet files (which are much more efficient and very well supported by most batch frameworks in the Hadoop and Spark universe). In order to do so, we first need to rename some columns, which contain unsupported characters:\n",
    "* \"STATION NAME\" => \"STATION_NAME\"\n",
    "* \"ELEV(M)\" => \"ELEVATION\"\n",
    "\n",
    "After the columns have been renamed, the data frame is written into the structured zone into the Hive table `isd.stations` using the `DataFrame.write.saveAsTable` function. But we also need to take care of creating the Hive database `isd` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hive database \"isd\" using spark.sql(...)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write stations into Hive table \"isd.stations\"\n",
    "weather_stations \\\n",
    "    .withColumnRenamed(\"STATION NAME\", \"STATION_NAME\") \\\n",
    "    .withColumnRenamed(\"ELEV(M)\", \"ELEVATION\") \\\n",
    "    .write.format(\"parquet\").saveAsTable(\"isd.stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data agin\n",
    "\n",
    "Using the `spark.read.table` function we read in the data back into Spark and display some records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stations = # YOUR CODE HERE\n",
    "weather_stations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Weather Measurements\n",
    "\n",
    "Now we will work with the second and more interesting part of the NOAA weather data set: The measurements. These are stored in different subdirectories per year. For us, the year 2013 is good enough, since the taxi trips are all from 2013.\n",
    "\n",
    "The data format is a proprietary ASCII encoding, so we use the `spark.read.text` method to read each line as one record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw measurements into PySpark DataFrame raw_weather\n",
    "#  location: weather_basedir + \"/2013\"\n",
    "#  format: text\n",
    "raw_weather = # YOUR CODE HERE\n",
    "raw_weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract precipitation\n",
    "\n",
    "Now we extract the precipitation from the measurements. This is not trivial, since that information is stored in a variable part. We assume that the record contains precipitation data when it contains the substring `AA1` at position 109. This denotes the type of the subsection in the data record followed by the number of hours of this measurement and the precipitation depth.\n",
    "\n",
    "We use some PySpark string functions to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather.select(\n",
    "        f.substring(raw_weather[\"value\"],106,999),\n",
    "        f.instr(raw_weather[\"value\"],\"AA1\").alias(\"s\"),\n",
    "        f.when(f.instr(raw_weather[\"value\"],\"AA1\") == 109,f.substring(raw_weather[\"value\"], 109+3, 8)).alias(\"AAD\")\n",
    "    )\\\n",
    "    .withColumn(\"precipitation_hours\", f.substring(f.col(\"AAD\"), 1, 2).cast(\"INT\")) \\\n",
    "    .withColumn(\"precipitation_depth\", f.substring(f.col(\"AAD\"), 3, 4).cast(\"FLOAT\")) \\\n",
    "    .filter(f.col(\"precipitation_depth\") > 0) \\\n",
    "    .limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all relevant measurements\n",
    "\n",
    "The precipitation was the hardest part. Other measurements like wind speed and air temperature are stored at fixed positions together with some quality flags denoting if a measurement is valid. In the following statement, we extract all relevant measurements. Specifically we extract the following information\n",
    "* **USAF** & **WBAN** - weather station identifier\n",
    "* **ts** - timestamp of measurement\n",
    "* **wind_direction** - wind direction (in degrees)\n",
    "* **wind_direction_qual** - quality flag of the wind direction\n",
    "* **wind_speed** - wind speed\n",
    "* **wind_speed_qual** - quality flag indicating the validity of the wind speed\n",
    "* **air_temperature** - air temperature in degree Celsius\n",
    "* **air_temperature_qual** - quality flag for air temperature\n",
    "* **precipitation_hours**\n",
    "* **precipitation_depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "        f.substring(raw_weather[\"value\"],5,6).alias(\"usaf\"),\n",
    "        f.substring(raw_weather[\"value\"],11,5).alias(\"wban\"),\n",
    "        f.to_timestamp(f.substring(raw_weather[\"value\"],16,12), \"yyyyMMddHHmm\").alias(\"ts\"),\n",
    "        f.substring(raw_weather[\"value\"],42,5).alias(\"report_type\"),\n",
    "        f.substring(raw_weather[\"value\"],61,3).alias(\"wind_direction\"),\n",
    "        f.substring(raw_weather[\"value\"],64,1).alias(\"wind_direction_qual\"),\n",
    "        f.substring(raw_weather[\"value\"],65,1).alias(\"wind_observation\"),\n",
    "        (f.substring(raw_weather[\"value\"],66,4).cast(\"float\") / 10.0).alias(\"wind_speed\"),\n",
    "        f.substring(raw_weather[\"value\"],70,1).alias(\"wind_speed_qual\"),\n",
    "        (f.substring(raw_weather[\"value\"],88,5).cast(\"float\") / 10.0).alias(\"air_temperature\"),\n",
    "        f.substring(raw_weather[\"value\"],93,1).alias(\"air_temperature_qual\"),\n",
    "        f.when(f.instr(raw_weather[\"value\"],\"AA1\") == 109,f.substring(raw_weather[\"value\"], 109+3, 8)).alias(\"AAD\")\n",
    "    ) \\\n",
    "    .withColumn(\"precipitation_hours\", f.substring(f.col(\"AAD\"), 1, 2).cast(\"INT\")) \\\n",
    "    .withColumn(\"precipitation_depth\", f.substring(f.col(\"AAD\"), 3, 4).cast(\"FLOAT\")) \\\n",
    "    .withColumn(\"date\", f.to_date(f.col(\"ts\"))) \\\n",
    "    .drop(\"AAD\")\n",
    "    \n",
    "weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store into Structured Zone\n",
    "\n",
    "After successful extraction, we write the result again into the structured zone into Hive table `isd.weather`. Since we originally have weather measurements for different years, we create a partitioned Hive table - although we are only interested in the weather of 2013. Unfortunately the support for writing into partitioned Hive tables is currently rather limited within the PySpark API. But we can perform everything using Spark SQL instead of the PySpark API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create partitioned table\n",
    "\n",
    "As mentioned above, we cannot easily create a partitioned table using the PySpark API. But we can create one using Spark SQL. We create the required SQL statement dynamically from the given schema (but with a fixed partition column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f.name + \" \" + f.dataType.simpleString() for f in weather.schema.fields]\n",
    "sql = \"CREATE TABLE IF NOT EXISTS isd.weather(\" + \",\".join(columns) + \") PARTITIONED BY(year INT)\"\n",
    "print(sql)\n",
    "\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store data into partition\n",
    "\n",
    "In order to write into a single partition, we again need to use Spark SQL. Therefore we register the weather data of 2013 as a namedtemporary view for SQL access and then insert all records into one specific partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.createOrReplaceTempView(\"weather_2013\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT OVERWRITE TABLE isd.weather PARTITION(year=2013)\n",
    "    SELECT * FROM weather_2013\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in from Structured Zone\n",
    "\n",
    "Again we read back the data from the Hive table `isd.weather` and display 10 first records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Holidays\n",
    "\n",
    "Another important data source is additional date information, specifically if a certain date is a bank holiday. While other information like week days can be directly computed from a date, for bank holidays an additional source is required.\n",
    "\n",
    "We follow again the same approach of reading in the raw data and storing it into the structured zone as Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_schema = StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('date', DateType()),\n",
    "    StructField('description', StringType()),\n",
    "    StructField('bank_holiday', BooleanType())\n",
    "    ])\n",
    "\n",
    "# Read in holidays file into PySpark DataFrame holidays\n",
    "#  location: holidays_basedir\n",
    "#  schema: holidays_schema\n",
    "#  format: csv\n",
    "#  header: False\n",
    "holidays = # YOUR CODE HERE\n",
    "\n",
    "# Peek inside the data\n",
    "holidays.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let us inspect the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store into Structured Zone\n",
    "\n",
    "Same game. Let us create a new database `ref` for simple reference table and let us store the holidays into a table `ref.holidays`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hive database ref\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store holidays into Hive table \"ref.holidays\"\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in from Structured Zone\n",
    "\n",
    "Again let us check if writing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = spark.read.table(\"ref.holidays\")\n",
    "holidays.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
