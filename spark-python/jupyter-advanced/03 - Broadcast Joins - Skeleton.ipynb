{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast Joins\n",
    "\n",
    "Joining two (or more) data sources is an important and elementary operation in an relation algebra, like Spark. But actually the implementation is not trivial, especially for distributed systems like Spark. The main challenge is to physically bring together all records that need to be joined from both data sources onto a single machine, otherwise they cannot be merged. This means that data needs to be exchanged over the network, which is complex and slower than local access.\n",
    "\n",
    "Depending on the size of the DataFrames to be joined, different strategies can be used. Spark supports two different join implementations:\n",
    "* Shuffle join - will shuffle both DataFrames over the network to ensure that matching records end up on the same machine\n",
    "* Broadcast join - will provide a copy of one DataFrames to all machines of the network\n",
    "\n",
    "While shuffle joins can work with arbitrary large data sets, a broadcast join always requires that the broadcast DataFrame completely fits into memory on all machines. But it can be much faster when the DataFrame is small enoguh.\n",
    "\n",
    "### Weather Example\n",
    "\n",
    "Again we will investigate into the different join types with our weather example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "First we load the weather data, which consists of the measurement data and some station metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements\n",
    "\n",
    "Measurements are stored in multiple directories (one per year). But we will limit ourselves to a single year in the analysis to improve readability of execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", lit(i)) for i in range(2003,2015)]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l,r: l.union(r), raw_weather_per_year)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single year to keep execution plans small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", lit(2003))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Measurements\n",
    "\n",
    "Measurements were stored in a proprietary text based format, with some values at fixed positions. We need to extract these values with a simple SELECT statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    col(\"year\"),\n",
    "    substring(col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"),11,5).alias(\"wban\"),\n",
    "    substring(col(\"value\"),16,8).alias(\"date\"),\n",
    "    substring(col(\"value\"),24,4).alias(\"time\"),\n",
    "    substring(col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"),66,4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"),88,5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata\n",
    "\n",
    "We also need to load the weather station meta data containing information about the geo location, country etc of individual weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Standard Joins\n",
    "\n",
    "Per defaulkt Spark will automatically decide which join implementation to use (broadcast or hash exchange). In order to see the differences, we disable this automatic optimization and later we will explicitly instruct Spark how to perform a join.\n",
    "\n",
    "With the automatic optimization disabled, all joins will be performed as hash exchange joins if not told otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Original Execution Plan\n",
    "\n",
    "Let us have a look at the execution plan of the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As said before, the join is a `SortMergeJoin` requiring a hash exchange shuffle operation. The join has the following steps:\n",
    "1. Filter away `NULL` values (this is an inner join)\n",
    "2. Repartition both DataFrames according to the join columns (`Exchange hashpartitioning`) with the same number of partitions each\n",
    "3. Sort each partition of both DataFrames independently\n",
    "4. Perform SortMergeJoin of both DataFrames by merging two according partitions from both DataFrames\n",
    "\n",
    "This is a rather expensive operation, since it requires a repartitioning over network of both DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Explicit Broadcast Joins\n",
    "\n",
    "Now let us perform the logically same join operation, but this time using a *broadcast join* (also called *mapside join*). We can instruct Spark to broadcast a DataFrame to all worker nodes by using the `broadcast` function. This actually serves as a hint and returns a new DataFrame which is marked to be broadcasted in `JOIN` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Now the execution plan looks significantly differnt. The stations metadata DataFrame is now broadcast to all worker nodes (still a network operation), but the measurement DataFrame does not require any repartitioning or shuffling any more. The broadcast join operation now is executed in three steps:\n",
    "* Filter `NULL` values again\n",
    "* Broadcast station metadata to all Spark executors\n",
    "* Perform `BroadcastHashJoin`\n",
    "\n",
    "A broadcast operation often makes sense in similar cases where you have large fact tables (measurements, purchase orders etc) and smaller lookup tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Automatic Broadcast Joins\n",
    "\n",
    "Per default Spark automatically determines which join strategy to use depending on the size of the DataFrames. This mechanism works fine when reading data from disk, but will not work after non-trivial transformations like `JOIN`s or grouped aggregations. In these cases Spark has no idea how large the results will be, but the execution plan has to be fixed before the first transformation is executed. In these cases (if by domain knowledge) you know that certain DataFrames will be small, an explicit `broadcast()` will still help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reenable automatic broadcast\n",
    "\n",
    "In order to re-enable Sparks default mechanism for selecting the `JOIN` strategy, we simply need to unset the configuration variable `spark.sql.autoBroadcastJoinThreshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect automatic execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Since the stations metadata table is relatively small, Spark automatically decides to use a broadcast join again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.3 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
