{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing DataFrames\n",
    "\n",
    "Sometimes execution plans can get pretty long and Spark might run into trouble. Common scenarios are iterative algorithms like ML or graph algorithms, which contain a big outer loop and iteratively transform a DataFrame over and over again. This would result in a really huge execution plan.\n",
    "\n",
    "In these cases you could use `cache()` or `persist()` in order to improve performance (otherwise all steps of the loop would be executed again from the very beginning leading to a runtime of O(n^2)). But this will not cut off the lineage.\n",
    "\n",
    "Checkpointing is the right solution for these cases. It will persist the data of a DataFrame in a reliable distributed storage (most commonly HDFS) and cut off the lineage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "We will load the weather data again for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", lit(i)) for i in range(2003,2015)]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l,r: l.union(r), raw_weather_per_year)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single year to keep execution plans small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", lit(2003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    col(\"year\"),\n",
    "    substring(col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"),11,5).alias(\"wban\"),\n",
    "    substring(col(\"value\"),16,8).alias(\"date\"),\n",
    "    substring(col(\"value\"),24,4).alias(\"time\"),\n",
    "    substring(col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"),66,4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"),88,5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")\n",
    "    \n",
    "weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")\n",
    "\n",
    "# Display first 10 records    \n",
    "stations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Join Data\n",
    "\n",
    "Now we perform the join between the station master data and the measurements, as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_weather = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Truncating Execution Plans\n",
    "\n",
    "Now we want to understand the effect of checkpointing. First we will use the traditional aggregation and print the execution plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Traditional Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = joined_weather.groupBy(joined_weather[\"ctry\"], joined_weather[\"year\"]).agg(\n",
    "        min(when(joined_weather.air_temperature_qual == lit(1), joined_weather.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined_weather.air_temperature_qual == lit(1), joined_weather.air_temperature)).alias('max_temp')\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Checkpointing\n",
    "\n",
    "Now we first checkpoint the joined weather data set and then perform the aggregation on the checkpointed DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Checkpoint directory\n",
    "\n",
    "First we need to specify a checkpoint directory on a reliable shared file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create checkpoint\n",
    "\n",
    "Now we can create a checkpoint for the joined weather. Note that this takes some time, as checkpointing is not a lazy operation, it will be executed immediately. This is also conceptionally neccessary, because one aspect of checkpointing is that the whole lineage gets cut off. So there is no way around executing the computation for materializing the DataFrame inside the checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect execution plan\n",
    "\n",
    "Let us have a look at the execution plan of the checkpointed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the lineage got lost and is replaced by a `Scan ExistingRDD` which refers to the data in the checkpoint directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform aggregation\n",
    "\n",
    "Now we can perform the aggregation with the checkpointed variant of the joined weather DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cp_weather.groupBy(cp_weather[\"ctry\"], cp_weather[\"year\"]).agg(\n",
    "        min(when(cp_weather.air_temperature_qual == lit(1), cp_weather.air_temperature)).alias('min_temp'),\n",
    "        max(when(cp_weather.air_temperature_qual == lit(1), cp_weather.air_temperature)).alias('max_temp')\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the execution plan now essentially only contains the aggregation in three steps (partial aggregation, shuffle, final aggregation). The lineage of the join is not present any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.3 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
