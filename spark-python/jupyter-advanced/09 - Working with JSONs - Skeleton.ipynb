{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with JSON data\n",
    "\n",
    "In addition to simple tabular data, Spark also supportes nested data containing strcutures, arrays and maps. This is particular interesting if working with non-relational, semi-structured data like JSONs.\n",
    "\n",
    "### Example Data\n",
    "\n",
    "This time we will not work with weather data, since that data set does not contain the features we want to discuss. Instead we use Twitter data, which is provided as JSON data (one record for one tweet). As we will see, even simple things like Tweets end up in fairly complex data structures with lots of information. Welcome to the new world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Inspect Data\n",
    "\n",
    "So as a simple first step, let's try to load the data and inspect it like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/twitter-sample/00.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect\n",
    "Load data as JSON and convert it to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = spark.read.json(storageLocation)\n",
    "twitter.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Schema\n",
    "\n",
    "After we already saw that some columns seem to contain nested data (for example the `entities` column), let's inspect the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "That pretty large and complex schema gives you an impression of what you have to expect from social networking platforms. Similar complex structures also appear with event sourcing architectures.\n",
    "\n",
    "But the big question now is, how can we work with this data. There are multiple challenges:\n",
    "* Nested data\n",
    "* Arrays of sub-entities\n",
    "\n",
    "Theoretically Spark also supports maps, but JSON cannot distinguish between maps and structs. A good schema design would always use struct instead of maps, because this gives a static schema and therefore a reliable contract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Accessing Elements\n",
    "\n",
    "So let's start with the first simple exercise: We try to access some nested element by its top-level name. We chose the `geo` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Accessing nested entries\n",
    "\n",
    "You can also access nested entries by using the JSON path, which simply consists of the element names concatenated by a dot (.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Accessing Array Entries\n",
    "\n",
    "The next challenge after accessing nested elements is to access entris inside an array. This can be achieved by subscripting a column with a numerical index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = twitter \\\n",
    "    .filter(twitter[\"geo.coordinates\"].isNotNull()) \\\n",
    "    .select(\n",
    "        # YOUR CODE HERE\n",
    "    )\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Exploding Entries\n",
    "\n",
    "Accessing individual elements in an array via its index works fine as long as the number of entries is known. But in different scenarios, an array can contain an arbitrary number of elements. The Twitter data for example contains an array of used hashtags. Spark 2.3 does not provide much support, but it is possible to convert an array of entries into multiple records using the `explode` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Exploding sub-entities\n",
    "\n",
    "In the example above, it might be useful to access sub-entries of an array. If no subscription is used, this will result again in an array, which can be exploded afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding\n",
    "\n",
    "The `explode` function allows to create multiple records for each entry in an array while retaining other non-array columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = twitter \\\n",
    "    .select(\n",
    "        # YOUR CODE HERE\n",
    "    )\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark\n",
    "\n",
    "Note that `explode` will actually create no record for empty lists of hashtags. If you still require all records which do not have any hashtags, you can use the function `explode_outer` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = twitter \\\n",
    "    .select(\n",
    "        # YOUR CODE HERE\n",
    "    )\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Working with UDFs\n",
    "\n",
    "Of course another approach to work with nested data (specifically with arrays) is to use UDFs. For example let us try to extract the longest hashtag for every tweet. This would be rather difficult with the current functionality of Spark, since we cannot create subselects inside a single record.\n",
    "\n",
    "But a small Python UDF will just do the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Define Python Function\n",
    "\n",
    "First we define and test a small Python function, which should perform the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import builtin Python functions, like max\n",
    "import builtins\n",
    "\n",
    "def select_longest(tags):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Python function\n",
    "\n",
    "We should test the function with some common cases\n",
    "* non-empty list\n",
    "* empty list\n",
    "* `NULL` value (i.e. `None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(select_longest([\"x\", \"12345\", \"abc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(select_longest([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(select_longest(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Convert Python function to UDF\n",
    "\n",
    "Now we have to encapsulate the Python function into a Spark UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "select_longest_udf = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use UDF\n",
    "\n",
    "Now we can use the Python UDF in a simple `select` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = twitter \\\n",
    "    .select(\n",
    "        # YOUR CODE HERE\n",
    "    )\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Use Pandas UDF\n",
    "\n",
    "Of course a Pandas UDF might improve performance significantly. Let's try that instead of the classic Python UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('string', PandasUDFType.SCALAR)\n",
    "def select_longest(series):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pandas UDF\n",
    "\n",
    "We can use the Pandas UDF in the same way as we did with the original Python UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = twitter \\\n",
    "    .select(\n",
    "        twitter[\"id\"],\n",
    "        twitter[\"created_at\"],\n",
    "        select_longest_udf(twitter[\"entities.hashtags.text\"]).alias(\"longest_hashtag\")\n",
    "    )\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.3 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
