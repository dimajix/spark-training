{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions\n",
    "\n",
    "From time to time you hit a wall where you need a simple transformation, but Spark does not offer an appropriate function in the `pyspark.sql.functions` module. Fortunately you can simply define new functions, so called *user defined functions* or short *UDFs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df = spark.createDataFrame([('Alice & Bob',12),('Thelma & Louise',17)],['name','age'])\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "html.escape(\"Thelma & Louise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "html_encode = # YOUR CODE HERE\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, you can also use a Python decorator for declaring a UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "result = df.select(html_encode('name').alias('html_name'))\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex return types\n",
    "\n",
    "PySpark also supports complex return types, for example structs (or also arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(StructType([\n",
    "    StructField(\"org_name\", StringType()), \n",
    "    StructField(\"html_name\", StringType())\n",
    "]))\n",
    "def html_encode(s):\n",
    "    return (s,html.escape(s))\n",
    "\n",
    "result = df.select(html_encode('name').alias('both_names'))\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Support\n",
    "\n",
    "If you wanto to use the Python UDF inside a SQL query, you also need to register it, so PySpark knows its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_encode = # YOUR CODE HERE\n",
    "\n",
    "df.createOrReplaceTempView(\"famous_pairs\")\n",
    "result = # YOUR CODE HERE\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas UDFs\n",
    "\n",
    "\"Normal\" Python UDFs are pretty expensive (in terms of execution time), since for every record the following steps need to be performed:\n",
    "* record is serialized inside JVM\n",
    "* record is sent to an external Python process\n",
    "* record is deserialized inside Python\n",
    "* record is Processed in Python\n",
    "* result is serialized in Python\n",
    "* result is sent back to JVM\n",
    "* result is deserialized and stored inside result DataFrame\n",
    "\n",
    "This does not only sound like a lot of work, it actually is. Therefore Python UDFs are a magnitude slower than native UDFs written in Scala or Java, which run directly inside the JVM.\n",
    "\n",
    "But since Spark 2.3 an alternative approach is available for defining Python UDFs with so called *Pandas UDFs*. Pandas is a commonly used Python framework which also offers DataFrames (but Pandas DataFrames, not Spark DataFrames). Spark 2.3 now can convert inside the JVM a Spark DataFrame into a shareable memory buffer by using a library called *Arrow*. Python then can also treat this memory buffer as a Pandas DataFrame and can directly work on this shared memory.\n",
    "\n",
    "This approach has two major advantages:\n",
    "* No need for serialization and deserialization, since data is shared directly in memory between the JVM and Python\n",
    "* Pandas has lots of very efficient implementations in C for many functions\n",
    "\n",
    "Due to these two facts, Pandas UDFs are much faster and should be preferred over traditional Python UDFs whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = spark.range(0,100)\n",
    "df = r.withColumn('v', r.id.cast(\"double\")).withColumn(\"group\", r.id % 5)\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic UDF Approach\n",
    "\n",
    "As an example, let's create a function which simply increments a numeric column by one. First let us have a look using a traditional Python UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Use udf to define a row-at-a-time udf\n",
    "@udf('double')\n",
    "# Input/output are both a single double value\n",
    "def plus_one(v):\n",
    "      return v + 1\n",
    "\n",
    "result = df.withColumn('v2', plus_one(df.v))\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDF\n",
    "\n",
    "Increment a value using a Pandas UDF. The Pandas UDF receives a `pandas.Series` object and also has to return a `pandas.Series` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Pandas UDFs\n",
    "While the example above transforms all records independently, but only one column at a time, Spark also offers a so called *grouped Pandas UDF* which operates on complete groups of records (as created by a `groupBy` method). This is a great mean to replace the (in PySpark missing) *User Defined Aggregation Functions* (UDAFs).\n",
    "\n",
    "For example let's subtract the mean of a group from all entries of a group. In Spark this could be achieved directly by using windowed aggregations. But let's first have a look at a Python implementation which does not use Pandas Grouped UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@udf(ArrayType(DoubleType()))\n",
    "def subtract_mean(values):\n",
    "    series = pd.Series(values)\n",
    "    center = series - series.mean()\n",
    "    return [x for x in center]\n",
    "\n",
    "groups = df.groupBy('group').agg(collect_list(df.v).alias('values'))\n",
    "result = groups.withColumn('center', explode(subtract_mean(groups.values))).drop('values')\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is even incomplete, as the `id` column is now missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas Grouped UDFs\n",
    "\n",
    "Now let's try to implement the same function using a Pandas grouped UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def subtract_mean(pdf):\n",
    "    return pdf.assign(v=pdf.v - pdf.v.mean())\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of grouped regressions\n",
    "\n",
    "In this section, we want to demanstrate a slightly advanced example for using Pandas grouped transformation for performing many ordinary least square model fits in parallel. We reuse the weather data and try to predict the temperature of all stations with a very simple model per station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "First we load data of a single year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "rawWeatherData = spark.read.text(storageLocation + \"/2003\")\n",
    "weather_all = rawWeatherData.select(\n",
    "    substring(col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"),11,5).alias(\"wban\"),\n",
    "    to_timestamp(substring(col(\"value\"),16,12),\"yyyyMMddHHmm\").alias(\"timestamp\"),\n",
    "    to_timestamp(substring(col(\"value\"),16,12),\"yyyyMMddHHmm\").cast(\"long\").alias(\"ts\"),\n",
    "    substring(col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"),66,4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"),88,5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of one station\n",
    "\n",
    "First we only analyse a single station, just to check our approach and the expressiveness of our model. It won't be a very good fit, but it will be good enough for our needs to demonstrate the concept.\n",
    "\n",
    "So first we pick a single station, and we also only keep those records with a valid temeprature measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_single = weather_all.where(\"usaf='954920' and wban='99999'\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = # YOUR CODE HERE\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Space\n",
    "\n",
    "Our model will simply predict the temperature depending on the time and day of year. We use sin and cos of with a day-wide period and a year-wide period as features for fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "seconds_per_day = 24*60*60\n",
    "seconds_per_year = 365*seconds_per_day\n",
    "\n",
    "# Add sin and cos as features for fitting\n",
    "pdf['daily_sin'] = np.sin(pdf['ts']/seconds_per_day*2.0*math.pi)\n",
    "pdf['daily_cos'] = np.cos(pdf['ts']/seconds_per_day*2.0*math.pi)\n",
    "pdf['yearly_sin'] = np.sin(pdf['ts']/seconds_per_year*2.0*math.pi)\n",
    "pdf['yearly_cos'] = np.cos(pdf['ts']/seconds_per_year*2.0*math.pi)\n",
    "\n",
    "# Make a plot, just to check how it looks like\n",
    "pdf[0:200].plot(x='timestamp', y=['daily_sin','daily_cos','air_temperature'], figsize=[16,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model\n",
    "\n",
    "Now that we have the temperature and some features, we fit a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# define target variable y\n",
    "y = pdf['air_temperature']\n",
    "# define feature variables X\n",
    "X = pdf[['ts', 'daily_sin', 'daily_cos', 'yearly_sin', 'yearly_cos']]\n",
    "X = sm.add_constant(X)\n",
    "# fit model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# perform prediction\n",
    "pdf['pred'] = model.predict(X)\n",
    "\n",
    "# Make a plot of real temperature vs predicted temperature\n",
    "pdf[0:200].plot(x='timestamp', y=['pred','air_temperature'], figsize=[16,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Model\n",
    "\n",
    "Now let us inspect the model, in order to find a way to store it in a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame from the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = X.columns\n",
    "pd.DataFrame([[model.params[i] for i in  x_columns]], columns=x_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform OLS for all stations\n",
    "\n",
    "Now we want to create a model for all stations. First we filter the data again, such that we only have valid temperature measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_weather = weather_all.filter(weather_all.air_temperature_qual == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "Now we generate the same features, but this time we use Spark instead of Pandas operations. This simplifies later model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "seconds_per_day = 24*60*60\n",
    "seconds_per_year = 365*seconds_per_day\n",
    "\n",
    "features = valid_weather.select(\n",
    "    valid_weather.usaf,\n",
    "    valid_weather.wban,\n",
    "    valid_weather.air_temperature,\n",
    "    valid_weather.ts,\n",
    "    lit(1.0).alias('const'),\n",
    "    sin(valid_weather.ts * 2.0 * math.pi / seconds_per_day).alias('daily_sin'),\n",
    "    cos(valid_weather.ts * 2.0 * math.pi / seconds_per_day).alias('daily_cos'),\n",
    "    sin(valid_weather.ts * 2.0 * math.pi / seconds_per_year).alias('yearly_sin'),\n",
    "    cos(valid_weather.ts * 2.0 * math.pi / seconds_per_year).alias('yearly_cos')\n",
    ")\n",
    "\n",
    "features.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Models\n",
    "\n",
    "Now we use a Spark Pandas grouped UDF in order to fit models for all weather stations in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_columns = ['usaf', 'wban']\n",
    "y_column = 'air_temperature'\n",
    "x_columns = ['ts', 'const', 'daily_sin', 'daily_cos', 'yearly_sin', 'yearly_cos']\n",
    "schema = features.select(*group_columns, *x_columns).schema\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "models = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and compare results\n",
    "\n",
    "Now let's pick the same station again, and compare the model to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.where(\"usaf='954920' and wban='99999'\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.3 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
