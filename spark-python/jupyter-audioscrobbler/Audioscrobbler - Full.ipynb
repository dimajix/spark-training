{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Audioscrobbler Data\n",
    "\n",
    "In this notebook we want to create two very simple statistics on artists from data provided by Audioscrobbler. We are working with three related data sets:\n",
    "\n",
    "1. A list of users containing their number of plays per artist\n",
    "2. A list which maps a generic artist-id to its real (band) name\n",
    "3. A list which maps bad artist ids to good ones (fixing typing errors)\n",
    "\n",
    "Then we will ask two simple questions:\n",
    "\n",
    "1. Which artists have the most listeners (in terms of number of unique users)\n",
    "2. Which artists are played most often (in terms of total play counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "First of all we have to load the data from S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Read User-Artist Data\n",
    "\n",
    "First we read in the most important data set, containing the information how often a user played songs of a specific artist. This information is stored in the file at `s3://dimajix-training/data/audioscrobbler/user_artist_data/`. The file has the following characteristics:\n",
    "\n",
    "* Format: CSV (kind of)\n",
    "* Separator: Space (” “)\n",
    "* Header: No\n",
    "* Fields: user_id (INT), artist_id(INT), play_count(INT)\n",
    "\n",
    "So we need to read in this file and store it in a local variable user_artist_data. Since we do not have any header contained in the file itself, we need to specify the schema explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"user_id\", IntegerType()),\n",
    "        StructField(\"artist_id\", IntegerType()),\n",
    "        StructField(\"play_count\", IntegerType())\n",
    "    ])\n",
    "    \n",
    "user_artist_data = spark.read \\\n",
    "    .option(\"sep\", \" \") \\\n",
    "    .option(\"header\", False) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"s3://dimajix-training/data/audioscrobbler/user_artist_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek inside\n",
    "Let us have a look at the first 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_artist_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Read in Artist aliases\n",
    "\n",
    "Now we read in a file containing mapping of bad artist IDs to good IDs. This fixes typos in the artists names and thereby enables us to merge information with different artist IDs belonging to the same band. This information is stored in the file at `s3://dimajix-training/data/audioscrobbler/artist_alias/`. The file has the following characteristics:\n",
    "\n",
    "* Format: CSV (kind of)\n",
    "* Separator: Tab (“\\t”)\n",
    "* Header: No\n",
    "* Fields: bad_id (INT), good_id(INT)\n",
    "\n",
    "So we need to read in this file and store it in a local variable `artist_alias`. Since we do not have any header contained in the file itself, we need to specify the schema explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"bad_id\", IntegerType()),\n",
    "        StructField(\"good_id\", IntegerType())\n",
    "    ])\n",
    "    \n",
    "artist_alias = spark.read \\\n",
    "    .option(\"sep\",\"\\t\") \\\n",
    "    .option(\"header\", False) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"s3://dimajix-training/data/audioscrobbler/artist_alias/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artist_alias.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Read in Artist names\n",
    "\n",
    "The third file contains the artists name for his/her id. We also use this file in order to be able to display results with the artists names instead of their IDs. This information is stored in the file at `s3://dimajix-training/data/audioscrobbler/artist_data/`. The file has the following characteristics:\n",
    "\n",
    "* Format: CSV (kind of)\n",
    "* Separator: Tab (“\\t”)\n",
    "* Header: No\n",
    "* Fields: artist_id (INT), artist_name(STRING)\n",
    "\n",
    "So we need to read in this file and store it in a local variable `artist_data`. Since we do not have any header contained in the file itself, we need to specify the schema explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"artist_id\", IntegerType),\n",
    "        StructField(\"artist_name\", StringType)\n",
    "    ])\n",
    "\n",
    "artist_data = spark.read \\\n",
    "    .option(\"sep\",\"\\t\") \\\n",
    "    .option(\"header\",False) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"s3://dimajix-training/data/audioscrobbler/artist_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artist_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Clean Data\n",
    "\n",
    "Before continuing with the analysis, we first create a cleaned version of the `artist_user_data` DataFrame with the `artist_alias` mapping applied. This means that we need to lookup each artist-id in the original data set in the alias data set and see if we find have a matching `bad_id` entry with a replacement `good_id`. The result should be stored in a variable `cleaned_user_artist_data`. This DataFrame should contain the columns Finally select only the columns `user_id`, `artist_id` (the corrected one) and `play_count`.\n",
    "\n",
    "Hints:\n",
    "\n",
    "1. Join the user artist data DataFrame with the artist alias DataFrame containing fixes for some artists. Which join type is appropriate?    \n",
    "2. Replace the artist-id in the user artists data with the `good_id` from the artist alias DataFrame - if a match is found on `bad_id`    \n",
    "3. Finally select only the columns `user_id`, `artist_id` (the corrected one) and `play_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "cleaned_user_artist_data = user_artist_data.join(artist_alias, user_artist_data[\"artist_id\"] == artist_alias[\"bad_id\"], \"left\") \\\n",
    "    .select(\n",
    "        user_artist_data[\"user_id\"], \n",
    "        coalesce(artist_alias[\"good_id\"], user_artist_data[\"artist_id\"]).alias(\"artist_id\"), \n",
    "        user_artist_data[\"play_count\"]\n",
    "    )\n",
    "    \n",
    "cleaned_user_artist_data.limit(5).toPandas()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Question 1: Artist with most unique listeners\n",
    "\n",
    "Who are the artist with the most unique listeners? For this question, it is irrelevant how often an individul user has played songs of a specific artist. It is only important how many different users have played each artists work. Of course we do not want to see the artist-id, but their real names as provided in the DataFrame `artist_data`!\n",
    "\n",
    "Hints:\n",
    "1. Group cleaned data by `artist_id`\n",
    "2. Perform aggregation by counting unique user ids\n",
    "3. Join `artist_data`\n",
    "4. Lookup the artists name\n",
    "5. Sort result by descending user counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = cleaned_user_artist_data \\\n",
    "    .groupBy(\"artist_id\") \\\n",
    "    .agg(countDistinct(col(\"user_id\")).alias(\"user_count\")) \\\n",
    "    .join(artist_data, cleaned_user_artist_data[\"artist_id\"] == artist_data[\"artist_id\"]) \\\n",
    "    .select(artist_data[\"artist_name\"], col(\"user_count\")) \\\n",
    "    .orderBy(col(\"user_count\").desc())\n",
    "\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Question 2: Artist with most Plays\n",
    "\n",
    "Now we also take into account how often each user played the work of individual artists. That is, we also include the `play_coun` column into our calculations. So which artists have the most plays in total? Of course we do not want to see the artist-id, but their real names as provided in the DataFrame `artist_data`!\n",
    "\n",
    "Hints:\n",
    "1. Group cleaned data by `artist_id`\n",
    "2. Perform aggregation by summing up play count\n",
    "3. Join `artist_data`\n",
    "4. Lookup the artists name\n",
    "5. Sort result by descending play counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = cleaned_user_artist_data \\\n",
    "    .groupBy(\"artist_id\") \\\n",
    "    .agg(sum(col(\"play_count\")).alias(\"play_count\")) \\\n",
    "    .join(artist_data, cleaned_user_artist_data[\"artist_id\"] == artist_data[\"artist_id\"])\n",
    "    .select(artist_data[\"artist_name\"], col(\"play_count\"))\n",
    "    .orderBy(col(\"play_count\").desc())\n",
    "\n",
    "result.limit(5).toPandas()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.1 (Python 3.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
