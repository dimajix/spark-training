{"paragraphs":[{"text":"%md\n# Weather Data Analytics\nThis notebook performs some basic weather data analytics using the Spark RDD interface.","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508718_-861255915","id":"20160617-194148_200534060","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Weather Data Analytics</h1>\n<p>This notebook performs some basic weather data analytics using the Spark RDD interface.</p>\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2946"},{"text":"%md\n# Loading Data\nAs a very first step, we need to read our data. We also define some global variables where the data is actually stored.","dateUpdated":"Jun 17, 2016 7:51:31 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508718_-861255915","id":"20160617-194148_974104277","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Loading Data</h1>\n<p>As a very first step, we need to read our data. We also define some global variables where the data is actually stored.</p>\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2947","dateFinished":"Jun 17, 2016 7:51:29 PM","dateStarted":"Jun 17, 2016 7:51:29 PM","focus":true},{"text":"val storageLocation = \"s3://is24-data-dev-spark-training/data/weather\"","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508718_-861255915","id":"20160617-194148_1502742458","result":{"code":"SUCCESS","type":"TEXT","msg":"storageLocation: String = s3://is24-data-dev-spark-training/data/weather\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2948"},{"text":"val weather_2011 = sc.textFile(storageLocation + \"/2011\")","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508718_-861255915","id":"20160617-194148_37319239","result":{"code":"SUCCESS","type":"TEXT","msg":"weather_2011: org.apache.spark.rdd.RDD[String] = s3://is24-data-dev-spark-training/data/weather/2011 MapPartitionsRDD[387] at textFile at <console>:103\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2949"},{"text":"val isd_history = sc.textFile(storageLocation + \"/isd-history\")","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508719_-861640664","id":"20160617-194148_159224949","result":{"code":"SUCCESS","type":"TEXT","msg":"isd_history: org.apache.spark.rdd.RDD[String] = s3://is24-data-dev-spark-training/data/weather/isd-history MapPartitionsRDD[391] at textFile at <console>:103\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2950"},{"text":"%md\n# Custom Classes\nFirst we need to define some custom classes which will hold the weather data and station data. Also included are some helper methods for extracting the data from the raw data.","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508719_-861640664","id":"20160617-194148_329093681","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Custom Classes</h1>\n<p>First we need to define some custom classes which will hold the weather data and station data. Also included are some helper methods for extracting the data from the raw data.</p>\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2951"},{"text":"def getFloat(str:String) : Float = {\n  if (str.isEmpty)\n    return Float.NaN\n  else if (str(0) == '+')\n    return str.substring(1).toFloat\n  else\n    return str.toFloat\n}\n\n// This class contains the actual measurement of a weather station at a specific point in time\ncase class WeatherData(\n    date:String,\n    time:String,\n    usaf:String,\n    wban:String,\n    validTemperature:Boolean,\n    temperature:Float,\n    validWindSpeed:Boolean,\n    windSpeed:Float\n)\n\n// This function can be used for extracting a WeatherData object from a single line of the weather data\ndef extractWeatherData(row:String) = {\n  val date = row.substring(15,23)\n  val time = row.substring(23,27)\n  val usaf = row.substring(4,10)\n  val wban = row.substring(10,15)\n  val airTemperatureQuality = row.charAt(92)\n  val airTemperature = row.substring(87,92)\n  val windSpeedQuality = row.charAt(69)\n  val windSpeed = row.substring(65,69)\n\n  WeatherData(date,time,usaf,wban,airTemperatureQuality == '1',airTemperature.toFloat/10,windSpeedQuality == '1',windSpeed.toFloat/10)\n}\n","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508719_-861640664","id":"20160617-194148_1307105542","result":{"code":"SUCCESS","type":"TEXT","msg":"getFloat: (str: String)Float\ndefined class WeatherData\nextractWeatherData: (row: String)WeatherData\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2952"},{"text":"// This class contains the weather station meta data\ncase class StationData(\n    usaf:String,\n    wban:String,\n    name:String,\n    country:String,\n    state:String,\n    icao:String,\n    latitude:Float,\n    longitude:Float,\n    elevation:Float,\n    date_begin:String,\n    date_end:String\n)\n\n// This method can be used for extracting a StationData object from a single line of the weather data\ndef extractStationData(row:String) = {\n  val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n  val latitude = getFloat(columns(6))\n  val longitude = getFloat(columns(7))\n  val elevation = getFloat(columns(8))\n  StationData(columns(0),columns(1),columns(2),columns(3),columns(4),columns(5),latitude,longitude,elevation,columns(9),columns(10))\n}\n","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508719_-861640664","id":"20160617-194148_1912619231","result":{"code":"SUCCESS","type":"TEXT","msg":"defined class StationData\nextractStationData: (row: String)StationData\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2953"},{"text":"%md\n# Load Weather Data\nNow we can load both Weather data and the station data using the helper functions defined above. The raw data can be found at storageLocation + \"/<<year>>\"\n\n1. Load data from raw text files for years 2003 to 2014 (you might start with a single year for perfomance reasons)\n2. Put all RDDs from all years into a single RDD using SparkContext.union method\n3. Transform the raw data into a meaningful RDD with WeatherData as object type. Use the extractWeatherData for this Transformation\n4. Have a look at the first couple of entries (say the first 10 entries)","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508719_-861640664","id":"20160617-194148_1856187121","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Weather Data</h1>\n<p>Now we can load both Weather data and the station data using the helper functions defined above. The raw data can be found at storageLocation + &ldquo;/&laquo;year&raquo;&rdquo;</p>\n<ol>\n<li>Load data from raw text files for years 2003 to 2014 (you might start with a single year for perfomance reasons)</li>\n<li>Put all RDDs from all years into a single RDD using SparkContext.union method</li>\n<li>Transform the raw data into a meaningful RDD with WeatherData as object type. Use the extractWeatherData for this Transformation</li>\n<li>Have a look at the first couple of entries (say the first 10 entries)</li>\n</ol>\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2954"},{"text":"// Get RDDs for all years\n// YOUR CODE HERE\nval raw_weather_years = ...\n\n// Put all RDDs into a single one using the \"union\" method of the SparkContext\n// YOUR CODE HERE\nval raw_weather = ...\n\n// Now extract the WeatherData objects from the raw data using the extractWeatherData method defined above\n// YOUR CODE HERE\nval weather = ...\n\n// Let's have a look at the first couple of entries\nweather.take(10).foreach(println)","dateUpdated":"Jun 17, 2016 7:55:17 PM","config":{"enabled":true,"graph":{"mode":"table","height":253,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508719_-861640664","id":"20160617-194148_442002678","dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2955","focus":true},{"text":"%md\n# Load Station Meta Data\nWe also need the weather station meta data, which can be found at \"storageLocation/isd-history\". Again we perform the following steps to load the data:\n\n1. Load data from raw text file at storageLocation/isd-history\n2. Transform the raw data into a meaningful RDD with StationData as object type. Use the extractStationData for this Transformation\n3. Create appropriate keys (usaf + wban) for joining later\n4. Convert RDD ti local map using collectAsMap()\n5. Create a broadcast variable from this local map","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508720_-851252444","id":"20160617-194148_593074173","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Station Meta Data</h1>\n<p>We also need the weather station meta data, which can be found at &ldquo;storageLocation/isd-history&rdquo;. Again we perform the following steps to load the data:</p>\n<ol>\n<li>Load data from raw text file at storageLocation/isd-history</li>\n<li>Transform the raw data into a meaningful RDD with StationData as object type. Use the extractStationData for this Transformation</li>\n<li>Create appropriate keys (usaf + wban) for joining later</li>\n<li>Convert RDD ti local map using collectAsMap()</li>\n<li>Create a broadcast variable from this local map</li>\n</ol>\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2956"},{"text":"// Load raw station meta data\nval isd_raw = sc.textFile(storageLocation + \"/isd-history\")\n\n\n// Fetch all rows from the raw data. And extract the StationData using the extractStationData function\nval isd = ...\n\n// Create a PairRDD with usaf+wban as key, and fetch it to the local machine via collectAsMap\nval isd_index = ...\n\n// Create Broadcast Variable\nval isd_index_bc = sc.broadcast(isd_index)\n","dateUpdated":"Jun 17, 2016 7:55:13 PM","config":{"enabled":true,"graph":{"mode":"table","height":116,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508720_-851252444","id":"20160617-194148_1326019884","dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2957","focus":true},{"text":"%md\n# Join Data\nWe wanto to perform some simple analytics on the weather data, but we need some station details (i.e. the country) for the anylsis. This time we will join the weather station details using the index created above inside a simple map transformation. This saves us one explicit shuffle join.\n\n1. Create a helper function extractCountryYearWeather which looks up all information for a given measurement\n2. Join the meta data to the weather measurements using a map transformation and the function defined above\n3. Print the first couple of entries of the joined RDD","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508720_-851252444","id":"20160617-194148_1978385469","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Join Data</h1>\n<p>We wanto to perform some simple analytics on the weather data, but we need some station details (i.e. the country) for the anylsis. This time we will join the weather station details using the index created above inside a simple map transformation. This saves us one explicit shuffle join.</p>\n<ol>\n<li>Create a helper function extractCountryYearWeather which looks up all information for a given measurement</li>\n<li>Join the meta data to the weather measurements using a map transformation and the function defined above</li>\n<li>Print the first couple of entries of the joined RDD</li>\n</ol>\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2958"},{"text":"// Helper method extracting\ndef extractCountryYearWeather(data:WeatherData) = {\n    // Create station ID from measurement for lookup\n    val stationId = ...\n    // Lookup station metadata in broadcast variable\n    val station = ...\n    // Get country\n    val country = ...\n    // Get year\n    val year = ...\n    // Return a new Pair\n    ((country,year),data)\n}\n\n// Perform map side join using the function above\nval weather_per_country_and_year = ...\n\n// Check that everything looks nice    \nweather_per_country_and_year.take(10).foreach(println)\n","dateUpdated":"Jun 17, 2016 7:56:35 PM","config":{"enabled":true,"graph":{"mode":"table","height":202,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508720_-851252444","id":"20160617-194148_1235270982","dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2959","focus":true},{"text":"%md\n# Aggregate Data\nNow we want to perform the analysis itself. We want to calculate the minimum and maximum wind speed and temperature per year and country. We will use the 'aggregateByKey' method of an RDD. This requires some custom state classes used during the aggregation. So our plan is as follows:\n\n1. Create a case class \"WeatherMinMax\" for holding minimum and maximum weather information, both for wind speed and for temperature\n2. Initialize all member variables of the WeatherMinMax class appropriately to represent \"no information so far\" state.\n3. Add a method \"reduce(data:WeatherData)\" which accepts a WeatherData instance and returns a new WeatherMinMax instance containing the merged information\n4. Add a method \"combine(other:WeatherMinMax)\" which accepts another WeatherMinMax object and returns a new WeatherMinMax instance containing the combined information\n5. Put all together by using the aggregateByKey method of the last RDD using the WeatherMinMax class for aggregation\n6. Print some results from the aggregated data\n \nPay attention to the validTemperature and validWindSpeed flags when merging information!","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508720_-851252444","id":"20160617-194148_1474978925","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Aggregate Data</h1>\n<p>Now we want to perform the analysis itself. We want to calculate the minimum and maximum wind speed and temperature per year and country. We will use the 'aggregateByKey' method of an RDD. This requires some custom state classes used during the aggregation. So our plan is as follows:</p>\n<ol>\n<li>Create a case class &ldquo;WeatherMinMax&rdquo; for holding minimum and maximum weather information, both for wind speed and for temperature</li>\n<li>Add a method &ldquo;reduce(data:WeatherData)&rdquo; which accepts a WeatherData instance and returns a new WeatherMinMax instance containing the merged information</li>\n<li>Add a method &ldquo;combine(other:WeatherMinMax)&rdquo; which accepts another WeatherMinMax object and returns a new WeatherMinMax instance containing the combined information</li>\n<li>Put all together by using the aggregateByKey method of the last RDD using the WeatherMinMax class for aggregation</li>\n<li>Print some results from the aggregated data</li>\n</ol>\n<p>Pay attention to the validTemperature and validWindSpeed flags when merging information!</p>\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2960"},{"text":"case class WeatherMinMax(\n    minTemperature:Float = 99999,\n    maxTemperature:Float = -99999,\n    minWindSpeed:Float = 99999,\n    maxWindSpeed:Float = -99999\n) {\n  // Reduce method for merging in another WeatherData entry\n  def reduce(other:WeatherData) = {\n    val minT = if(other.validTemperature) minTemperature.min(other.temperature) else minTemperature\n    val maxT = if(other.validTemperature) maxTemperature.max(other.temperature) else maxTemperature\n    val minW = if(other.validWindSpeed) minWindSpeed.min(other.windSpeed) else minWindSpeed\n    val maxW = if(other.validWindSpeed) maxWindSpeed.max(other.windSpeed) else maxWindSpeed\n    WeatherMinMax(minT,maxT,minW,maxW)\n  }\n  // Combine method for combining two WeatherMinMax instances\n  def combine(other:WeatherMinMax) = {\n    val minT = minTemperature.min(other.minTemperature)\n    val maxT = maxTemperature.max(other.maxTemperature)\n    val minW = minWindSpeed.min(other.minWindSpeed)\n    val maxW = maxWindSpeed.max(other.maxWindSpeed)\n    WeatherMinMax(minT,maxT,minW,maxW)\n  }\n}\n\nval weather_minmax = weather_per_country_and_year.aggregateByKey(WeatherMinMax())((x:WeatherMinMax,y:WeatherData) => x.reduce(y),(x:WeatherMinMax,y:WeatherMinMax) => x.combine(y))\n\nweather_minmax.take(30).foreach(println)\n","dateUpdated":"Jun 17, 2016 7:56:27 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508721_-851637193","id":"20160617-194148_1567055154","dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2961","focus":true},{"text":"%md\n# Pretty Printing\nSince Zeppelin supports some pretty printing using the magic \"%table\" keyword, we want to make use of it.","dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508721_-851637193","id":"20160617-194148_756526470","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Pretty Printing</h1>\n<p>Since Zeppelin supports some pretty printing using the magic &ldquo;%table&rdquo; keyword, we want to make use of it.</p>\n"},"dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2962"},{"text":"def mkString(p:Any*) = {\n  p.iterator.toList.mkString(\"\\t\")\n}\n\nprintln(\"%table\")\nprintln(\"Country\\tYear\\tTempMin\\tTempMax\\tWindMin\\tWindMax\")\nweather_minmax.collect().map(x => mkString(x._1._1,x._1._2,x._2.minTemperature,x._2.maxTemperature,x._2.minWindSpeed,x._2.maxWindSpeed)).foreach(println)\n","dateUpdated":"Jun 17, 2016 7:56:19 PM","config":{"enabled":true,"graph":{"mode":"multiBarChart","height":94,"optionOpen":false,"keys":[{"name":"Country","index":0,"aggr":"sum"}],"values":[{"name":"WindMin","index":4,"aggr":"min"},{"name":"WindMax","index":5,"aggr":"max"},{"name":"TempMax","index":3,"aggr":"max"},{"name":"TempMin","index":2,"aggr":"min"}],"groups":[{"name":"Year","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"Country","index":0,"aggr":"sum"},"yAxis":{"name":"Year","index":1,"aggr":"sum"}}},"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508721_-851637193","id":"20160617-194148_1837191050","dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2963","focus":true},{"dateUpdated":"Jun 17, 2016 7:41:48 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466192508721_-851637193","id":"20160617-194148_1150100784","dateCreated":"Jun 17, 2016 7:41:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2964"}],"name":"Weather Data Analysis with Broadcast Variable Exercise","id":"2BQABZ7QK","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}