{"paragraphs":[{"text":"%md\n# Weather Data Analytics\nThis notebook performs some basic weather data analytics using the Spark RDD interface.","dateUpdated":"2018-12-18T07:43:52+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Weather Data Analytics</h1>\n<p>This notebook performs some basic weather data analytics using the Spark RDD interface.</p>\n"}]},"apps":[],"jobName":"paragraph_1545119032230_-1583891066","id":"20160612-172712_662322978","dateCreated":"2018-12-18T07:43:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9705"},{"text":"%md\n# 1. Inspect Data\nAs a very first step, we should have a look at our data. We also define some global variables where the data is actually stored.","dateUpdated":"2018-12-18T07:53:38+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>1. Inspect Data</h1>\n<p>As a very first step, we should have a look at our data. We also define some global variables where the data is actually stored.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545119032235_-1585814811","id":"20160612-172712_1254548845","dateCreated":"2018-12-18T07:43:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9706","user":"anonymous","dateFinished":"2018-12-18T07:53:38+0000","dateStarted":"2018-12-18T07:53:38+0000"},{"text":"val storageLocation = \"s3://dimajix-training/data/weather\"","dateUpdated":"2018-12-18T07:43:52+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545119032238_-1586969057","id":"20160612-172712_1207991669","dateCreated":"2018-12-18T07:43:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9707"},{"text":"val weather_2011 = sc.textFile(storageLocation + \"/2011\")\n\n// Print the first 10 entries of weather_2011\n// YOUR CODE HERE","dateUpdated":"2018-12-18T07:43:52+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545119032240_-1576965586","id":"20160612-172712_405582145","dateCreated":"2018-12-18T07:43:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9708"},{"text":"%md\n# 2. Custom Classes\nFirst we need to define some custom classes which will hold the weather data and station data. Also included are some helper methods for extracting the data from the raw data.","dateUpdated":"2018-12-18T07:53:44+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>2. Custom Classes</h1>\n<p>First we need to define some custom classes which will hold the weather data and station data. Also included are some helper methods for extracting the data from the raw data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545119032243_-1576580837","id":"20160612-172712_566207539","dateCreated":"2018-12-18T07:43:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9710","user":"anonymous","dateFinished":"2018-12-18T07:53:44+0000","dateStarted":"2018-12-18T07:53:44+0000"},{"text":"def getFloat(str:String) : Float = {\n  if (str.isEmpty)\n    return Float.NaN\n  else if (str(0) == '+')\n    return str.substring(1).toFloat\n  else\n    return str.toFloat\n}\n\n// This class contains the actual measurement of a weather station at a specific point in time\ncase class WeatherData(\n    date:String,\n    time:String,\n    usaf:String,\n    wban:String,\n    validTemperature:Boolean,\n    temperature:Float,\n    validWindSpeed:Boolean,\n    windSpeed:Float\n)\n\n// This function can be used for extracting a WeatherData object from a single line of the weather data\ndef extractWeatherData(row:String) = {\n  val date = row.substring(15,23)\n  val time = row.substring(23,27)\n  val usaf = row.substring(4,10)\n  val wban = row.substring(10,15)\n  val airTemperatureQuality = row.charAt(92)\n  val airTemperature = row.substring(87,92)\n  val windSpeedQuality = row.charAt(69)\n  val windSpeed = row.substring(65,69)\n\n  WeatherData(date,time,usaf,wban,airTemperatureQuality == '1',airTemperature.toFloat/10,windSpeedQuality == '1',windSpeed.toFloat/10)\n}\n","dateUpdated":"2018-12-18T07:43:52+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545119032245_-1578889330","id":"20160612-172712_750068116","dateCreated":"2018-12-18T07:43:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9711"},{"text":"%md\n# 3. Load Weather Data\nNow we can load both Weather data and the station data using the helper functions defined above. The raw data can be found at storageLocation + \"/<<year>>\"\n\n1. Load data from raw text files for years 2003 to 2014 (you might start with a single year for perfomance reasons)\n2. Put all RDDs from all years into a single RDD using SparkContext.union method\n3. Transform the raw data into a meaningful RDD with WeatherData as object type. Use the extractWeatherData for this Transformation\n4. Have a look at the first couple of entries (say the first 10 entries)","dateUpdated":"2018-12-18T07:54:18+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>3. Load Weather Data</h1>\n<p>Now we can load both Weather data and the station data using the helper functions defined above. The raw data can be found at storageLocation + &ldquo;/&laquo;year&raquo;&rdquo;</p>\n<ol>\n  <li>Load data from raw text files for years 2003 to 2014 (you might start with a single year for perfomance reasons)</li>\n  <li>Put all RDDs from all years into a single RDD using SparkContext.union method</li>\n  <li>Transform the raw data into a meaningful RDD with WeatherData as object type. Use the extractWeatherData for this Transformation</li>\n  <li>Have a look at the first couple of entries (say the first 10 entries)</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1545119032248_-1580043577","id":"20160612-172712_2008652784","dateCreated":"2018-12-18T07:43:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9713","user":"anonymous","dateFinished":"2018-12-18T07:54:18+0000","dateStarted":"2018-12-18T07:54:18+0000"},{"text":"// 1.  Get RDDs for all years 2003 to 2014\n// YOUR CODE HERE\nval weather = ...\n\n// 2. Put all RDDs into a single one using the \"union\" method of the SparkContext\n// YOUR CODE HERE\n\n// 3. Now extract the WeatherData objects from the raw data using the extractWeatherData method defined above\n// YOUR CODE HERE\n\n// 4. Let's have a look at the first couple of entries\n// YOUR CODE HERE","dateUpdated":"2018-12-18T07:43:52+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545119032250_-1579274079","id":"20160612-172712_984409848","dateCreated":"2018-12-18T07:43:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9714"},{"text":"%md\n# 4. Create new Keys\nSince we want to aggregate the data by weather station and year, we need to create a new key \"usaf,wban,year\" from our weather measurement data. This can be done by one additional transformation. The resulting RDD should have elements of type \n\n    ((usaf,wban,year),weather)\n\nNote the nesting of tuples - this is required, because Spark only understands how to extract a key for grouping from pairs. Here our pair is \"usaf,wban,year\".","user":"anonymous","dateUpdated":"2018-12-18T07:54:36+0000","config":{"colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>4. Create new Keys</h1>\n<p>Since we want to aggregate the data by weather station and year, we need to create a new key &ldquo;usaf,wban,year&rdquo; from our weather measurement data. This can be done by one additional transformation. The resulting RDD should have elements of type </p>\n<pre><code>((usaf,wban,year),weather)\n</code></pre>\n<p>Note the nesting of tuples - this is required, because Spark only understands how to extract a key for grouping from pairs. Here our pair is &ldquo;usaf,wban,year&rdquo;.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545119032261_-1597357278","id":"20160612-172712_430126835","dateCreated":"2018-12-18T07:43:52+0000","dateStarted":"2018-12-18T07:54:36+0000","dateFinished":"2018-12-18T07:54:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9715"},{"text":"// Now we want to create a new key (station + year), so we cann groupings and aggregations\n// YOUR CODE HERE\nval weather_per_station_and_year = ...\n    \n// Check that everything looks nice    \n// YOUR CODE HERE\n","dateUpdated":"2018-12-18T07:48:17+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545119032262_-1596203031","id":"20160612-172712_711760043","dateCreated":"2018-12-18T07:43:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9716"},{"text":"%md\n# 5. Aggregate Data\nNow we want to perform the analysis itself. We want to calculate the minimum and maximum wind speed and temperature per year and country. We will use the 'aggregateByKey' method of an RDD. This requires some custom state classes used during the aggregation. So our plan is as follows:\n\n1. Create a case class \"WeatherMinMax\" for holding minimum and maximum weather information, both for wind speed and for temperature\n2. Initialize all member variables of the WeatherMinMax class appropriately to represent \"no information so far\" state.\n3. Add a method \"reduce(data:WeatherData)\" which accepts a WeatherData instance and returns a new WeatherMinMax instance containing the merged information\n4. Add a method \"combine(other:WeatherMinMax)\" which accepts another WeatherMinMax object and returns a new WeatherMinMax instance containing the combined information\n5. Put all together by using the aggregateByKey method of the last RDD using the WeatherMinMax class for aggregation\n6. Print some results from the aggregated data\n \nPay attention to the validTemperature and validWindSpeed flags when merging information!","dateUpdated":"2018-12-18T07:55:35+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>5. Aggregate Data</h1>\n<p>Now we want to perform the analysis itself. We want to calculate the minimum and maximum wind speed and temperature per year and country. We will use the &lsquo;aggregateByKey&rsquo; method of an RDD. This requires some custom state classes used during the aggregation. So our plan is as follows:</p>\n<ol>\n  <li>Create a case class &ldquo;WeatherMinMax&rdquo; for holding minimum and maximum weather information, both for wind speed and for temperature</li>\n  <li>Initialize all member variables of the WeatherMinMax class appropriately to represent &ldquo;no information so far&rdquo; state.</li>\n  <li>Add a method &ldquo;reduce(data:WeatherData)&rdquo; which accepts a WeatherData instance and returns a new WeatherMinMax instance containing the merged information</li>\n  <li>Add a method &ldquo;combine(other:WeatherMinMax)&rdquo; which accepts another WeatherMinMax object and returns a new WeatherMinMax instance containing the combined information</li>\n  <li>Put all together by using the aggregateByKey method of the last RDD using the WeatherMinMax class for aggregation</li>\n  <li>Print some results from the aggregated data</li>\n</ol>\n<p>Pay attention to the validTemperature and validWindSpeed flags when merging information!</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545119032263_-1596587780","id":"20160612-172712_1324064767","dateCreated":"2018-12-18T07:43:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9717","user":"anonymous","dateFinished":"2018-12-18T07:55:35+0000","dateStarted":"2018-12-18T07:55:35+0000"},{"text":"case class WeatherMinMax(\n    // YOUR CODE HERE\n    minTemperature:Float = ...,\n    maxTemperature:Float = ...,\n    minWindSpeed:Float = ...,\n    maxWindSpeed:Float = ...\n) {\n  // Reduce method for merging in another WeatherData entry\n  def reduce(other:WeatherData) : WeatherMinMax = {\n    // YOUR CODE HERE\n    val minT = ...\n    val maxT = ...\n    val minW = ...\n    val maxW = ...\n    WeatherMinMax(minT,maxT,minW,maxW)\n  }\n  // Combine method for combining two WeatherMinMax instances\n  def combine(other:WeatherMinMax) : WeatherMinMax = {\n    // YOUR CODE HERE\n    ...\n  }\n}\n\nval weather_minmax = weather_per_station_and_year.aggregateByKey(WeatherMinMax())((x:WeatherMinMax,y:WeatherData) => x.reduce(y),(x:WeatherMinMax,y:WeatherMinMax) => x.combine(y))\n\nweather_minmax.take(30).foreach(println)\n","dateUpdated":"2018-12-18T07:48:25+0000","config":{"colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545119032265_-1598896273","id":"20160612-172712_1330988755","dateCreated":"2018-12-18T07:43:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9718"},{"text":"%md\n# 6. Pretty Printing\nSince Zeppelin supports some pretty printing using the magic \"%table\" keyword, we want to make use of it.","dateUpdated":"2018-12-18T07:55:43+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>6. Pretty Printing</h1>\n<p>Since Zeppelin supports some pretty printing using the magic &ldquo;%table&rdquo; keyword, we want to make use of it.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545119032267_-1598126775","id":"20160612-172712_1738338133","dateCreated":"2018-12-18T07:43:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9719","user":"anonymous","dateFinished":"2018-12-18T07:55:43+0000","dateStarted":"2018-12-18T07:55:43+0000"},{"text":"def mkString(p:Any*) = {\n  p.iterator.toList.mkString(\"\\t\")\n}\n\nprintln(\"%table\")\nprintln(\"USAF\\tWBAN\\tYear\\tTempMin\\tTempMax\\tWindMin\\tWindMax\")\nweather_minmax.collect().map(x => mkString(x._1._1,x._1._2,x._1._3,x._2.minTemperature,x._2.maxTemperature,x._2.minWindSpeed,x._2.maxWindSpeed)).foreach(println)\n","dateUpdated":"2018-12-18T07:49:19+0000","config":{"colWidth":12,"enabled":true,"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"Country","index":0,"aggr":"sum"}],"values":[{"name":"WindMin","index":4,"aggr":"min"},{"name":"WindMax","index":5,"aggr":"max"},{"name":"TempMax","index":3,"aggr":"max"},{"name":"TempMin","index":2,"aggr":"min"}],"groups":[{"name":"Year","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"Country","index":0,"aggr":"sum"},"yAxis":{"name":"Year","index":1,"aggr":"sum"}}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545119032269_-1600435269","id":"20160612-172712_1881467543","dateCreated":"2018-12-18T07:43:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9720"},{"text":"","dateUpdated":"2018-12-18T07:43:52+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545119032270_-1599281022","id":"20160612-172712_128293126","dateCreated":"2018-12-18T07:43:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9721"}],"name":"Weather RDD Simple Analysis Exercise","id":"2DY9B84SU","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}