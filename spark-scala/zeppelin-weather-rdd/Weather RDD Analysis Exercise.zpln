{"paragraphs":[{"text":"%md\n# Weather Data Analytics\nThis notebook performs some basic weather data analytics using the Spark RDD interface.","dateUpdated":"2017-02-28T05:38:36-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101043_-703717895","id":"20160612-172712_662322978","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Weather Data Analytics</h1>\n<p>This notebook performs some basic weather data analytics using the Spark RDD interface.</p>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:18137","dateFinished":"2017-02-28T05:38:33-0800","dateStarted":"2017-02-28T05:38:33-0800","focus":true},{"text":"%md\n# First Look at Data\nAs a very first step, we should have a look at our data. We also define some global variables where the data is actually stored.","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101043_-703717895","id":"20160612-172712_1254548845","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>First Look at Data</h1>\n<p>As a very first step, we should have a look at our data. We also define some global variables where the data is actually stored.</p>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18138"},{"text":"val storageLocation = \"s3://dimajix-training/data/weather\"","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101043_-703717895","id":"20160612-172712_1207991669","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18139"},{"text":"val weather_2011 = sc.textFile(storageLocation + \"/2011\")\n\n// Print the first 10 entries of weather_2011\n// YOUR CODE HERE","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101043_-703717895","id":"20160612-172712_405582145","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18140"},{"text":"val isd_history = sc.textFile(storageLocation + \"/isd-history\")\n\n// Print the first 10 entries of isd_history\n// YOUR CODE HERE","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101043_-703717895","id":"20160612-172712_506015437","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18141"},{"text":"%md\n# Custom Classes\nFirst we need to define some custom classes which will hold the weather data and station data. Also included are some helper methods for extracting the data from the raw data.","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101044_-705641640","id":"20160612-172712_566207539","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Custom Classes</h1>\n<p>First we need to define some custom classes which will hold the weather data and station data. Also included are some helper methods for extracting the data from the raw data.</p>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18142"},{"text":"def getFloat(str:String) : Float = {\n  if (str.isEmpty)\n    return Float.NaN\n  else if (str(0) == '+')\n    return str.substring(1).toFloat\n  else\n    return str.toFloat\n}\n\n// This class contains the actual measurement of a weather station at a specific point in time\ncase class WeatherData(\n    date:String,\n    time:String,\n    usaf:String,\n    wban:String,\n    validTemperature:Boolean,\n    temperature:Float,\n    validWindSpeed:Boolean,\n    windSpeed:Float\n)\n\n// This function can be used for extracting a WeatherData object from a single line of the weather data\ndef extractWeatherData(row:String) = {\n  val date = row.substring(15,23)\n  val time = row.substring(23,27)\n  val usaf = row.substring(4,10)\n  val wban = row.substring(10,15)\n  val airTemperatureQuality = row.charAt(92)\n  val airTemperature = row.substring(87,92)\n  val windSpeedQuality = row.charAt(69)\n  val windSpeed = row.substring(65,69)\n\n  WeatherData(date,time,usaf,wban,airTemperatureQuality == '1',airTemperature.toFloat/10,windSpeedQuality == '1',windSpeed.toFloat/10)\n}\n","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101044_-705641640","id":"20160612-172712_750068116","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18143"},{"text":"// This class contains the weather station meta data\ncase class StationData(\n    usaf:String,\n    wban:String,\n    name:String,\n    country:String,\n    state:String,\n    icao:String,\n    latitude:Float,\n    longitude:Float,\n    elevation:Float,\n    date_begin:String,\n    date_end:String\n)\n\n// This method can be used for extracting a StationData object from a single line of the weather data\ndef extractStationData(row:String) = {\n  val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n  val latitude = getFloat(columns(6))\n  val longitude = getFloat(columns(7))\n  val elevation = getFloat(columns(8))\n  StationData(columns(0),columns(1),columns(2),columns(3),columns(4),columns(5),latitude,longitude,elevation,columns(9),columns(10))\n}\n","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101044_-705641640","id":"20160612-172712_665817722","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18144"},{"text":"%md\n# Load Weather Data\nNow we can load both Weather data and the station data using the helper functions defined above. The raw data can be found at storageLocation + \"/<<year>>\"\n\n1. Load data from raw text files for years 2003 to 2014 (you might start with a single year for perfomance reasons)\n2. Put all RDDs from all years into a single RDD using SparkContext.union method\n3. Transform the raw data into a meaningful RDD with WeatherData as object type. Use the extractWeatherData for this Transformation\n4. Have a look at the first couple of entries (say the first 10 entries)","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101044_-705641640","id":"20160612-172712_2008652784","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Weather Data</h1>\n<p>Now we can load both Weather data and the station data using the helper functions defined above. The raw data can be found at storageLocation + &ldquo;/&laquo;year&raquo;&rdquo;</p>\n<ol>\n<li>Load data from raw text files for years 2003 to 2014 (you might start with a single year for perfomance reasons)</li>\n<li>Put all RDDs from all years into a single RDD using SparkContext.union method</li>\n<li>Transform the raw data into a meaningful RDD with WeatherData as object type. Use the extractWeatherData for this Transformation</li>\n<li>Have a look at the first couple of entries (say the first 10 entries)</li>\n</ol>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18145"},{"text":"// 1.  Get RDDs for all years 2003 to 2014\n// YOUR CODE HERE\nval weather = ...\n\n// 2. Put all RDDs into a single one using the \"union\" method of the SparkContext\n// YOUR CODE HERE\n\n// 3. Now extract the WeatherData objects from the raw data using the extractWeatherData method defined above\n// YOUR CODE HERE\n\n// 4. Let's have a look at the first couple of entries\n// YOUR CODE HERE","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101044_-705641640","id":"20160612-172712_984409848","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18146"},{"text":"%md\n# Load Station Meta Data\nWe also need the weather station meta data, which can be found at \"storageLocation/isd-history\". Again we perform the following steps to load the data:\n\n1. Load data from raw text file at storageLocation/isd-history\n2. Get first row of the RDD, the header line\n3. Filter away header line\n4. Transform the raw data into a meaningful RDD with StationData as object type. Use the extractStationData for this Transformation\n5. Have a look at the first couple of entries (say the first 10 entries)\n","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101044_-705641640","id":"20160612-172712_762298848","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Station Meta Data</h1>\n<p>We also need the weather station meta data, which can be found at &ldquo;storageLocation/isd-history&rdquo;. Again we perform the following steps to load the data:</p>\n<ol>\n<li>Load data from raw text file at storageLocation/isd-history</li>\n<li>Get first row of the RDD, the header line</li>\n<li>Filter away header line</li>\n<li>Transform the raw data into a meaningful RDD with StationData as object type. Use the extractStationData for this Transformation</li>\n<li>Have a look at the first couple of entries (say the first 10 entries)</li>\n</ol>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18147"},{"text":"// 1. Load raw station meta data\n// YOUR CODE HERE\nval isd_raw = ...\n\n// 2. Get header, so we can ignore it\n// YOUR CODE HERE\nval isd_head = ...\n\n// 3. & 4. Fetch all rows from the raw data, which are not like isd_head. And extract the StationData using the extractStationData function\nval isd = ...\n\n// 5. Let's have a look at the first couple of entries\n// YOUR CODE HERE\n","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101044_-705641640","id":"20160612-172712_69300334","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18148"},{"text":"%md\n# Join Data\nWe wanto to perform some simple analytics on the weather data, but we need some station details (i.e. the country) for the anylsis. Joining Spark RDDs is always done with a PairRDD (i.e. containing pair tuples), where the first element of a pair is used as the join key. So our plan looks as follows for perfoming the join:\n\n1. Create a new RDD from the weather RDD, now with entries (usaf+wban,WeatherData). This can be done using the keyBy method\n2. Create a new RDD from the statiion RDD, now with entries (usaf+wban,StationData). This can be done using the keyBy method\n3. Join both RDDs using the \"join\" method\n4. Print the first couple of entries of the joined RDD","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101045_-706026389","id":"20160612-172712_794576864","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Join Data</h1>\n<p>We wanto to perform some simple analytics on the weather data, but we need some station details (i.e. the country) for the anylsis. Joining Spark RDDs is always done with a PairRDD (i.e. containing pair tuples), where the first element of a pair is used as the join key. So our plan looks as follows for perfoming the join:</p>\n<ol>\n<li>Create a new RDD from the weather RDD, now with entries (usaf+wban,WeatherData). This can be done using the keyBy method</li>\n<li>Create a new RDD from the statiion RDD, now with entries (usaf+wban,StationData). This can be done using the keyBy method</li>\n<li>Join both RDDs using the &ldquo;join&rdquo; method</li>\n<li>Print the first couple of entries of the joined RDD</li>\n</ol>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18149"},{"text":"// 1. Create an indexed version of the weather by adding a key. The key should be the stations usaf + wban\n// YOUR CODE HERE\nval weather_idx = ...\n\n// 2. Now create the same key for the weather station meta data (usaf + wban)\n// YOUR CODE HERE\nval isd_idx = ...\n\n// 3. Finally we can join both RDDs. A join will always be performed on the first entry of a pair.\n// YOUR CODE HERE\nval weather_per_country = ...\n\n// 4. Check that everything looks nice    \n// YOUR CODE HERE\n","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101045_-706026389","id":"20160612-172712_1400036202","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18150"},{"text":"%md\n# Caching Data\n\nYou might want to cache the joined data in order to accelerate downstream processing","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101045_-706026389","id":"20160617-163830_28469238","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Caching Data</h1>\n<p>You might want to cache the joined data in order to accelerate downstream processing</p>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18151"},{"text":"// YOUR CODE HERE FOR CACHING weather_per_country","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101045_-706026389","id":"20160617-163854_803409761","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18152"},{"text":"%md\n# Create new Keys\nSince we want to aggregate the data by country and year, we need to create a new key \"country,year\" from our joined data. This can be done by one additional transformation. The resulting RDD should have elements of type \n\n    ((country,year),weather)\n\nNote the nesting of tuples - this is required, because Spark only understands how to extract a key for grouping from pairs. Here our pair is \"country,year\".\n\nPay attention to the layout of the elements in joined_weather, as can been see from the output above.","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101045_-706026389","id":"20160612-172712_430126835","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Create new Keys</h1>\n<p>Since we want to aggregate the data by country and year, we need to create a new key &ldquo;country,year&rdquo; from our joined data. This can be done by one additional transformation. The resulting RDD should have elements of type</p>\n<pre><code>((country,year),weather)\n</code></pre>\n<p>Note the nesting of tuples - this is required, because Spark only understands how to extract a key for grouping from pairs. Here our pair is &ldquo;country,year&rdquo;.</p>\n<p>Pay attention to the layout of the elements in joined_weather, as can been see from the output above.</p>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18153"},{"text":"// Now we want to create a new key (station + year), so we cann groupings and aggregations\n// YOUR CODE HERE\nval weather_per_country_and_year = ...\n    \n// Check that everything looks nice    \n// YOUR CODE HERE\n","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101045_-706026389","id":"20160612-172712_711760043","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18154"},{"text":"%md\n# Aggregate Data\nNow we want to perform the analysis itself. We want to calculate the minimum and maximum wind speed and temperature per year and country. We will use the 'aggregateByKey' method of an RDD. This requires some custom state classes used during the aggregation. So our plan is as follows:\n\n1. Create a case class \"WeatherMinMax\" for holding minimum and maximum weather information, both for wind speed and for temperature\n2. Initialize all member variables of the WeatherMinMax class appropriately to represent \"no information so far\" state.\n3. Add a method \"reduce(data:WeatherData)\" which accepts a WeatherData instance and returns a new WeatherMinMax instance containing the merged information\n4. Add a method \"combine(other:WeatherMinMax)\" which accepts another WeatherMinMax object and returns a new WeatherMinMax instance containing the combined information\n5. Put all together by using the aggregateByKey method of the last RDD using the WeatherMinMax class for aggregation\n6. Print some results from the aggregated data\n \nPay attention to the validTemperature and validWindSpeed flags when merging information!","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101045_-706026389","id":"20160612-172712_1324064767","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Aggregate Data</h1>\n<p>Now we want to perform the analysis itself. We want to calculate the minimum and maximum wind speed and temperature per year and country. We will use the 'aggregateByKey' method of an RDD. This requires some custom state classes used during the aggregation. So our plan is as follows:</p>\n<ol>\n<li>Create a case class &ldquo;WeatherMinMax&rdquo; for holding minimum and maximum weather information, both for wind speed and for temperature</li>\n<li>Initialize all member variables of the WeatherMinMax class appropriately to represent &ldquo;no information so far&rdquo; state.</li>\n<li>Add a method &ldquo;reduce(data:WeatherData)&rdquo; which accepts a WeatherData instance and returns a new WeatherMinMax instance containing the merged information</li>\n<li>Add a method &ldquo;combine(other:WeatherMinMax)&rdquo; which accepts another WeatherMinMax object and returns a new WeatherMinMax instance containing the combined information</li>\n<li>Put all together by using the aggregateByKey method of the last RDD using the WeatherMinMax class for aggregation</li>\n<li>Print some results from the aggregated data</li>\n</ol>\n<p>Pay attention to the validTemperature and validWindSpeed flags when merging information!</p>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18155"},{"text":"case class WeatherMinMax(\n    // YOUR CODE HERE\n    minTemperature:Float = ...,\n    maxTemperature:Float = ...,\n    minWindSpeed:Float = ...,\n    maxWindSpeed:Float = ...\n) {\n  // Reduce method for merging in another WeatherData entry\n  def reduce(other:WeatherData) : WeatherMinMax = {\n    // YOUR CODE HERE\n    val minT = ...\n    val maxT = ...\n    val minW = ...\n    val maxW = ...\n    WeatherMinMax(minT,maxT,minW,maxW)\n  }\n  // Combine method for combining two WeatherMinMax instances\n  def combine(other:WeatherMinMax) : WeatherMinMax = {\n    // YOUR CODE HERE\n    ...\n  }\n}\n\nval weather_minmax = weather_per_country_and_year.aggregateByKey(WeatherMinMax())((x:WeatherMinMax,y:WeatherData) => x.reduce(y),(x:WeatherMinMax,y:WeatherMinMax) => x.combine(y))\n\nweather_minmax.take(30).foreach(println)\n","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101046_-704872142","id":"20160612-172712_1330988755","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18156"},{"text":"%md\n# Pretty Printing\nSince Zeppelin supports some pretty printing using the magic \"%table\" keyword, we want to make use of it.","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101046_-704872142","id":"20160612-172712_1738338133","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Pretty Printing</h1>\n<p>Since Zeppelin supports some pretty printing using the magic &ldquo;%table&rdquo; keyword, we want to make use of it.</p>\n"},"dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18157"},{"text":"def mkString(p:Any*) = {\n  p.iterator.toList.mkString(\"\\t\")\n}\n\nprintln(\"%table\")\nprintln(\"Country\\tYear\\tTempMin\\tTempMax\\tWindMin\\tWindMax\")\nweather_minmax.collect().map(x => mkString(x._1._1,x._1._2,x._2.minTemperature,x._2.maxTemperature,x._2.minWindSpeed,x._2.maxWindSpeed)).foreach(println)\n","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"Country","index":0,"aggr":"sum"}],"values":[{"name":"WindMin","index":4,"aggr":"min"},{"name":"WindMax","index":5,"aggr":"max"},{"name":"TempMax","index":3,"aggr":"max"},{"name":"TempMin","index":2,"aggr":"min"}],"groups":[{"name":"Year","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"Country","index":0,"aggr":"sum"},"yAxis":{"name":"Year","index":1,"aggr":"sum"}}},"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101046_-704872142","id":"20160612-172712_1881467543","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18158"},{"text":"","dateUpdated":"2017-02-28T05:38:21-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488289101046_-704872142","id":"20160612-172712_128293126","dateCreated":"2017-02-28T05:38:21-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18159"}],"name":"Weather RDD Analysis Exercise","id":"2C9WMJC76","angularObjects":{"2C73P7NJN:shared_process":[],"2C7KU6EWG:shared_process":[],"2C8H7AG7Q:shared_process":[],"2CANY5QMM:shared_process":[],"2C7YM9SBT:shared_process":[],"2CAHZM5EW:shared_process":[],"2CA194TC4:shared_process":[],"2C85Z5A3J:shared_process":[],"2C77BBC6M:shared_process":[],"2C9T8R64M:shared_process":[],"2C82H3SUX:shared_process":[],"2C7W1UTSM:shared_process":[],"2C99CAHNC:shared_process":[],"2C7VKNJZ3:shared_process":[],"2C7MNAP62:shared_process":[],"2C8SJ4SC1:shared_process":[],"2C8273BS9:shared_process":[],"2CA9V89Q3:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}