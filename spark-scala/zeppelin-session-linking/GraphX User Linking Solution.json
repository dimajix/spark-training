{"paragraphs":[{"text":"%md\n# 1. Load Data\nWe load the data that we have prepared in the last exercise. Only you know where you have stored it, so don't ask me :)","dateUpdated":"2018-06-25T19:27:45+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>1. Load Data</h1>\n<p>We load the data that we have prepared in the last exercise. Only you know where you have stored it, so don't ask me :)</p>\n"}]},"apps":[],"jobName":"paragraph_1529954865921_-189801393","id":"20160610-195019_770962390","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:533"},{"text":"val sessions = spark.read.parquet(\"data/sessions\")\n\nsessions.printSchema()","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sessions: org.apache.spark.sql.DataFrame = [name: string, session: string ... 1 more field]\nroot\n |-- name: string (nullable = true)\n |-- session: string (nullable = true)\n |-- linking_key: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1529954865930_-191725138","id":"20160611-090912_1498120381","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:534"},{"text":"%md\n# 2. Create Indices usable for GraphX\nIn GraphX all Vertices are identified by 64bit Long values. But of course, our data is String based. Therefore we need to create an Index, which assigns a unique ID to every value which is to be used as a Vertex.\n\nWe will use both the sessions and the linking keys for vertices, so we need to create a common index for both key types.\n\nSo what we eventually want to do is the following:\n1. Get all session keys from the data\n2. Get all linking keys from the data\n3. Union these keys together (they are prefixed with sesn_ and cdk_, so they won't interfere)\n4. Make set of keys distinct\n5. Assign unique ID to each key using `zipWithUniqueId`. Theoretically the Spark DataFrame function `monotonically_increasing_id()` should also do the job, but it doesn't seem to produce stable results.\n\nNote that `zipWithUniqueId` only guarantees stable results between recomputations when the ordering of the elements is deterministic. Therefore we first sort all records before generating IDs.","dateUpdated":"2018-06-25T19:30:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>2. Create Indices usable for GraphX</h1>\n<p>In GraphX all Vertices are identified by 64bit Long values. But of course, our data is String based. Therefore we need to create an Index, which assigns a unique ID to every value which is to be used as a Vertex.</p>\n<p>We will use both the sessions and the linking keys for vertices, so we need to create a common index for both key types.</p>\n<p>So what we eventually want to do is the following:<br/>1. Get all session keys from the data<br/>2. Get all linking keys from the data<br/>3. Union these keys together (they are prefixed with sesn_ and cdk_, so they won&rsquo;t interfere)<br/>4. Make set of keys distinct<br/>5. Assign unique ID to each key using <code>zipWithUniqueId</code>. Theoretically the Spark DataFrame function <code>monotonically_increasing_id()</code> should also do the job, but it doesn&rsquo;t seem to produce stable results.</p>\n<p>Note that <code>zipWithUniqueId</code> only guarantees stable results between recomputations when the ordering of the elements is deterministic. Therefore we first sort all records before generating IDs.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529954865933_-194418380","id":"20160611-160136_407714042","dateCreated":"2018-06-25T19:27:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:535","user":"anonymous","dateFinished":"2018-06-25T19:30:39+0000","dateStarted":"2018-06-25T19:30:38+0000"},{"text":"// Get session keys from data, add one more column marking this key as being a session key\nval sessionKeys = sessions.select($\"session\" as \"key\", lit(true) as \"is_sessionkey\")\n// Get linking keys from data, add one more column marking this key as not being a session key\nval linkingKeys = sessions.select($\"linking_key\" as \"key\", lit(false) as \"is_sessionkey\")\n// Put both sets of keys together, and make entries distinct (drop duplicates)\nval keys = sessionKeys.unionAll(linkingKeys).distinct()\n\n// Create stable unique IDs. It is important to sort the data before calling zipWithIndex!\nval indexedKeys = keys.as[(String,Boolean)].rdd\n    .sortBy(_._1)\n    .zipWithUniqueId\n    .cache\n    .toDF\n    .select($\"_1._1\" as \"key\", $\"_1._2\" as \"is_sessionkey\", $\"_2\" as \"index\")","dateUpdated":"2018-06-25T19:27:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"key","index":0,"aggr":"sum"}],"values":[{"name":"is_sessionkey","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"key","index":0,"aggr":"sum"},"yAxis":{"name":"is_sessionkey","index":1,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sessionKeys: org.apache.spark.sql.DataFrame = [key: string, is_sessionkey: boolean]\nlinkingKeys: org.apache.spark.sql.DataFrame = [key: string, is_sessionkey: boolean]\nwarning: there was one deprecation warning; re-run with -deprecation for details\nkeys: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [key: string, is_sessionkey: boolean]\nindexedKeys: org.apache.spark.sql.DataFrame = [key: string, is_sessionkey: boolean ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1529954865937_-183645411","id":"20160611-090954_780732037","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:536"},{"text":"z.show(indexedKeys.limit(10))","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"key\tis_sessionkey\tindex\ncdk_0003X6MVGEjbaqv99fPED0KtyhZjgnoM\tfalse\t0\ncdk_000ExDVc3VLKzPHSmggGBbiVABYkxTpi\tfalse\t200\ncdk_000GGE3eFGOextrETnYx0YHrzuSBhqVu\tfalse\t400\ncdk_000OUiHQ1kV9KcqiuwBakloNJRvtLUV3\tfalse\t600\ncdk_000VZ92b0OgdulQpTXTYjEskhQA5C9El\tfalse\t800\ncdk_000u21lfinjYXYu26oxKnPrPo8eTbgCu\tfalse\t1000\ncdk_000vhuAkQg0RZrlPhKshZtAN0PexoMTM\tfalse\t1200\ncdk_0010gVp75IDorUJWnFL8HI65CTwucT2d\tfalse\t1400\ncdk_0014XB3ISnHflVHnQ5vtZXPGvsVnAfM6\tfalse\t1600\ncdk_001H9NeM8bhy6R38UpRDZukp0ak4mAhH\tfalse\t1800\n"}]},"apps":[],"jobName":"paragraph_1529954865941_-185184407","id":"20160611-101412_1451752564","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:537"},{"text":"%md\n# 3. Create GraphX Representation\nSince we want to use GraphX for finding all connected keys, we need to prepare our data. We already created an appropriate index set for all vertices and edges, now we need to transform our original (relational) data into a GraphX representation. This will be done in three steps:\n1. Create Verticies representing linking keys and sessions\n2. Create Edges representing linking key usage in sessions\n3. Create Graph from Vertices and Edges","dateUpdated":"2018-06-25T19:27:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>3. Create GraphX Representation</h1>\n<p>Since we want to use GraphX for finding all connected keys, we need to prepare our data. We already created an appropriate index set for all vertices and edges, now we need to transform our original (relational) data into a GraphX representation. This will be done in three steps:</p>\n<ol>\n<li>Create Verticies representing linking keys and sessions</li>\n<li>Create Edges representing linking key usage in sessions</li>\n<li>Create Graph from Vertices and Edges</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1529954865943_-184414909","id":"20180624-165550_384780347","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:538"},{"text":"%md\n## 3.1 Create Vertices\n\nNow we want to create vertices for GraphX. What is the set of vertices? Actually it's the whole set of keys, so every entry in our DataFrame 'indexedKeys' should be added to vertices. And that is pretty easy, since the required VertexID is just the 'index' column.\n\nWe simply use Spark Dataset operations to extract the Vertex ID and key using a `select` operation, the we cast the result into a Tuple using the Dataset `as` operation. Finally we extract the RDD from the Dataset, which will be our Vertices RDD.","dateUpdated":"2018-06-25T19:27:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>3.1 Create Vertices</h2>\n<p>Now we want to create vertices for GraphX. What is the set of vertices? Actually it's the whole set of keys, so every entry in our DataFrame 'indexedKeys' should be added to vertices. And that is pretty easy, since the required VertexID is just the 'index' column.</p>\n<p>We simply use Spark Dataset operations to extract the Vertex ID and key using a <code>select</code> operation, the we cast the result into a Tuple using the Dataset <code>as</code> operation. Finally we extract the RDD from the Dataset, which will be our Vertices RDD.</p>\n"}]},"apps":[],"jobName":"paragraph_1529954865945_-186723402","id":"20160611-101933_2130257741","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:539"},{"text":"// Create vertices from the indexedKeys. \nval vertices = indexedKeys.select($\"index\", $\"key\").as[(Long,String)].rdd\n\nvertices.take(10).foreach(println)","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"vertices: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[43] at rdd at <console>:26\n(0,cdk_0003X6MVGEjbaqv99fPED0KtyhZjgnoM)\n(200,cdk_000ExDVc3VLKzPHSmggGBbiVABYkxTpi)\n(400,cdk_000GGE3eFGOextrETnYx0YHrzuSBhqVu)\n(600,cdk_000OUiHQ1kV9KcqiuwBakloNJRvtLUV3)\n(800,cdk_000VZ92b0OgdulQpTXTYjEskhQA5C9El)\n(1000,cdk_000u21lfinjYXYu26oxKnPrPo8eTbgCu)\n(1200,cdk_000vhuAkQg0RZrlPhKshZtAN0PexoMTM)\n(1400,cdk_0010gVp75IDorUJWnFL8HI65CTwucT2d)\n(1600,cdk_0014XB3ISnHflVHnQ5vtZXPGvsVnAfM6)\n(1800,cdk_001H9NeM8bhy6R38UpRDZukp0ak4mAhH)\n"}]},"apps":[],"jobName":"paragraph_1529954865947_-185953904","id":"20160611-101911_208120114","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:540"},{"text":"%md\n## 3.2 Create Edges\n\nNow that we have the vertices, we also need the Edges. Since the Edges actually make up the topology of the graph, they are obviously a little bit more complex than vertices. We want to create an Edge for every connection between a session and a linking key. We'll do that with the following approach:\n\n1. Join indexedKeys to sessions, replacing session key by the appropriate 'index' entry in the indexedKeys\n2. Join indexedKeys to the result, now replacing the linking key by the corresponding 'index' entry in indexedKeys\n3. Now you should have a DataFrame with two Long values, one for the session and one for the linking key.\n4. Each of these tuples need to be mapped to a GraphX Edge(cdkIndex, sessionIndex, 0)","dateUpdated":"2018-06-25T19:27:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>3.2 Create Edges</h2>\n<p>Now that we have the vertices, we also need the Edges. Since the Edges actually make up the topology of the graph, they are obviously a little bit more complex than vertices. We want to create an Edge for every connection between a session and a linking key. We'll do that with the following approach:</p>\n<ol>\n<li>Join indexedKeys to sessions, replacing session key by the appropriate 'index' entry in the indexedKeys</li>\n<li>Join indexedKeys to the result, now replacing the linking key by the corresponding 'index' entry in indexedKeys</li>\n<li>Now you should have a DataFrame with two Long values, one for the session and one for the linking key.</li>\n<li>Each of these tuples need to be mapped to a GraphX Edge(cdkIndex, sessionIndex, 0)</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1529954865949_-188262398","id":"20160611-102114_62388765","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:541"},{"text":"import org.apache.spark.graphx.Edge\n\n// 1. Replace session keys by unique ID in indexed keys, keep linking key for next step\nval resolvedSession = sessions.join(indexedKeys, indexedKeys(\"key\") === sessions(\"session\"))\n    .select(\n        indexedKeys(\"index\") as \"session_idx\",\n        sessions(\"linking_key\")\n    )\n    \n// 2. Replace linking key by unique ID in indexed keys, keep ID from previous step\nval resolvedAll = resolvedSession.join(indexedKeys, indexedKeys(\"key\") === resolvedSession(\"linking_key\"))\n    .select(\n        indexedKeys(\"index\") as \"linking_idx\",\n        resolvedSession(\"session_idx\")\n    )\n    \n// 3. Now you should have a DataFrame with columns for the ID of session and the ID for linking key\n\n// 4. Map all entries to Edge(session_id, cdk_id)\nval edges = resolvedAll.map(e => Edge[Long]( e.getAs[Long](0), e.getAs[Long](1), 0)).rdd","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.graphx.Edge\nresolvedSession: org.apache.spark.sql.DataFrame = [session_idx: bigint, linking_key: string]\nresolvedAll: org.apache.spark.sql.DataFrame = [linking_idx: bigint, session_idx: bigint]\nedges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Long]] = MapPartitionsRDD[64] at rdd at <console>:30\n"}]},"apps":[],"jobName":"paragraph_1529954865952_-201728609","id":"20160611-102127_1509360684","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:542"},{"text":"%md\n## 3.3 Create Graph\n\nNow we have everything together, we can create a GraphX Graph. This is now trivial, only the constructor needs to be called.","dateUpdated":"2018-06-25T19:27:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>3.3 Create Graph</h2>\n<p>Now we have everything together, we can create a GraphX Graph. This is now trivial, only the constructor needs to be called.</p>\n"}]},"apps":[],"jobName":"paragraph_1529954865956_-203267605","id":"20160611-102713_1849984677","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:543"},{"text":"import org.apache.spark.graphx.Graph\n\nval sessionGraph = Graph(vertices, edges)","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.graphx.Graph\nsessionGraph: org.apache.spark.graphx.Graph[String,Long] = org.apache.spark.graphx.impl.GraphImpl@20520602\n"}]},"apps":[],"jobName":"paragraph_1529954865959_-202882856","id":"20160611-102724_62333210","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:544"},{"text":"%md\n# 4. Find Connected Components\n\nNow comes the next interesting step. We want to find all connected components in our Graph. This can be done using some simple functionality provided by GraphX called ConnectedComponents. It will return a new Graph with the vertex properties (i.e. the value attached to each Vertex) replaced by an ID identifying the component. The ID is selected as follows (from documentation):\n\n    Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. ","dateUpdated":"2018-06-25T19:27:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>4. Find Connected Components</h1>\n<p>Now comes the next interesting step. We want to find all connected components in our Graph. This can be done using some simple functionality provided by GraphX called ConnectedComponents. It will return a new Graph with the vertex properties (i.e. the value attached to each Vertex) replaced by an ID identifying the component. The ID is selected as follows (from documentation):</p>\n<pre><code>Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex.\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1529954865963_-204421852","id":"20160611-102906_472720527","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:545"},{"text":"import org.apache.spark.graphx.lib.ConnectedComponents\n\nval connectedGraph = ConnectedComponents.run(sessionGraph)","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.graphx.lib.ConnectedComponents\nconnectedGraph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Long] = org.apache.spark.graphx.impl.GraphImpl@1f34761a\n"}]},"apps":[],"jobName":"paragraph_1529954865967_-205960847","id":"20160611-102932_656235616","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:546"},{"text":"%md\n# 5. Map Back Session Keys\n\nNow we have a new graph, where each Vertex has an ID which maps to the original session key or linking key. The value of each Vertex denotes which component the Vertex (and therefore session or linking key) belongs to. By looking up all original session keys and keeping the component ID, we can find all sessions which belong together (those session keys having the same component ID belong to the same user).\n\nSo our plan looks as follows:\n1. Convert vertices from Graph back into a DataFrame with two columns 'original_idx' and 'component_id'\n2. Join indexedKeys to the DataFrame on original_idx === index, look up session key\n3. Keep only session keys (so we can put sessions together. Otherwise we could also put linking keys together)\n4. Extract session key and component id, forget other columns\n\nThe result is a DataFrame with a component ID for every session. Now let's rename component_id to 'user_handle', which represents our mental concept of the table","dateUpdated":"2018-06-25T19:27:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>5. Map Back Session Keys</h1>\n<p>Now we have a new graph, where each Vertex has an ID which maps to the original session key or linking key. The value of each Vertex denotes which component the Vertex (and therefore session or linking key) belongs to. By looking up all original session keys and keeping the component ID, we can find all sessions which belong together (those session keys having the same component ID belong to the same user).</p>\n<p>So our plan looks as follows:</p>\n<ol>\n<li>Convert vertices from Graph back into a DataFrame with two columns 'original_idx' and 'component_id'</li>\n<li>Join indexedKeys to the DataFrame on original_idx === index, look up session key</li>\n<li>Keep only session keys (so we can put sessions together. Otherwise we could also put linking keys together)</li>\n<li>Extract session key and component id, forget other columns</li>\n</ol>\n<p>The result is a DataFrame with a component ID for every session. Now let's rename component_id to 'user_handle', which represents our mental concept of the table</p>\n"}]},"apps":[],"jobName":"paragraph_1529954865972_-197111623","id":"20160611-123436_336815516","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:547"},{"text":"// 1. Convert Vertices back into a DataFrame. The first entry is the index, the second is the value (the component ID)\nval components = connectedGraph.vertices.toDF.select($\"_1\" as \"index\", $\"_2\" as \"component\").cache()\n\n// 2. Join the components with the indexedKeys DataFrame, so we get one original key (either session key or linking key)\n// 3. Keep only session keys (so we can put sessions together. Otherwise we could also put linking keys together)\n// 4. Extract session key and component id, forget other columns\nval sessionMapping = components.join(indexedKeys, components(\"index\") === indexedKeys(\"index\"))\n    .select(\n        indexedKeys(\"key\") as \"original_key\",\n        indexedKeys(\"is_sessionkey\"),\n        components(\"component\")\n    )\n    .filter($\"is_sessionkey\")\n    .select(\n        $\"original_key\" as \"session\",\n        $\"component\" as \"user_handle\"\n    )\n    .cache()","dateUpdated":"2018-06-25T19:39:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"components: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [index: bigint, component: bigint]\nsessionMapping: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [session: string, user_handle: bigint]\n"}]},"apps":[],"jobName":"paragraph_1529954865976_-198650618","id":"20160611-123902_2093568729","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:548"},{"text":"%md\n## 6. Check Results\nDo the results make sense? Let's have a look. We still have the original user names stored in our original sessions, let's join that data together with the sessionMapping. Let's see if all sessions belonging to a specific user do also have the same component ID.\n\n1. Make tuples (session, name) from original sessions data set distinct\n2. Join the result to sessionMapping\n3. Extract user name and user handle\n4. Sort result by user handle, fetch some entries => Do all entries of a user handle have the same user name?","dateUpdated":"2018-06-25T19:27:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>6. Check Results</h2>\n<p>Do the results make sense? Let's have a look. We still have the original user names stored in our original sessions, let's join that data together with the sessionMapping. Let's see if all sessions belonging to a specific user do also have the same component ID.</p>\n<ol>\n<li>Make tuples (session, name) from original sessions data set distinct</li>\n<li>Join the result to sessionMapping</li>\n<li>Extract user name and user handle</li>\n<li>Sort result by user handle, fetch some entries => Do all entries of a user handle have the same user name?</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1529954865978_-197881120","id":"20160611-124505_1367806648","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:549"},{"text":"val sessionToUser = sessions.select($\"session\",$\"name\").distinct()\n\nval combinedUsers = sessionMapping\n    .join(sessionToUser, sessionMapping(\"session\") === sessionToUser(\"session\"))\n    .select(\n        sessionMapping(\"session\"),\n        sessionMapping(\"user_handle\"),\n        sessionToUser(\"name\")\n    )\n    .cache()\n    \nz.show(combinedUsers.orderBy($\"user_handle\").limit(200))","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"session","index":0,"aggr":"sum"}],"values":[{"name":"user_handle","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"session","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sessionToUser: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [session: string, name: string]\ncombinedUsers: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [session: string, user_handle: bigint ... 1 more field]\n"},{"type":"TABLE","data":"session\tuser_handle\tname\nsesn_gOJyq0pDLevMObfp71nG7WsDr3r6zkOW\t0\tBORIS FOELSCH\nsesn_uLqSftrp2uHYkrQN9acuz5CiKagZ7Yve\t0\tBORIS FOELSCH\nsesn_mlxcHhEgqW0DHMMZzbhyu78ZP0TFlDOY\t0\tBORIS FOELSCH\nsesn_Ic8z3smftMitlXJKxk5BEitedzIxpG3g\t0\tBORIS FOELSCH\nsesn_FihJuHA9lNEDve7HzKC0vZwNYahwiFl5\t0\tBORIS FOELSCH\nsesn_xDAlO0MKudHkEKx9hbM2aBqV3Sd1L2Ox\t0\tBORIS FOELSCH\nsesn_jzPzj8smypJGKgXF1YpSjgPnV8Lbjhrw\t0\tBORIS FOELSCH\nsesn_vFUbyLo37NW8soZFowNwL3j5aap6mTC8\t0\tBORIS FOELSCH\nsesn_ADUOrMylJkWhJEvUv9513bJAd1JYk3go\t0\tBORIS FOELSCH\nsesn_4zW2E0Kq8mnARdffY6oUVOdd5WDs4ajK\t0\tBORIS FOELSCH\nsesn_1gyZg9o763kK4ayMSoYX1oSXmQvKw6Lz\t0\tBORIS FOELSCH\nsesn_S2T5ah2AeRxyNkghvdFiXHVXvwxnkOV7\t0\tBORIS FOELSCH\nsesn_NfGBL0Lj6xBCyih8qLNg4A0xN3H5BTjg\t0\tBORIS FOELSCH\nsesn_8254m9ZLyNehaQfRM5bejGNfncEoU4h7\t0\tBORIS FOELSCH\nsesn_kYMR7V9ucw4MROkrl0ZVW7D5ilSynrdv\t0\tBORIS FOELSCH\nsesn_xJPv7Yqg13qCMEDKShGimlmLiq4yakLq\t0\tBORIS FOELSCH\nsesn_cy5qtUQZpnIFTr2v2cd6gLkEzY3hEC7F\t0\tBORIS FOELSCH\nsesn_kMpEu6cCm8LEhKaXBCNg4BwVWKCHSH1G\t0\tBORIS FOELSCH\nsesn_ngomk5xUzen3qwD1rqpDuIyO9nrVBzr6\t0\tBORIS FOELSCH\nsesn_siATlsjmKE9RqptM9Kz1AfgwwXnTACc8\t0\tBORIS FOELSCH\nsesn_WohBAdRYMMP0v0zmtmls4xpe2E7AQ4HJ\t0\tBORIS FOELSCH\nsesn_4CHKUUfgWlUasCRbTnSvIwhycqfBTy4Y\t0\tBORIS FOELSCH\nsesn_z0VSi0g0jtbKXwoAebKRgBHqqbMaxGDR\t0\tBORIS FOELSCH\nsesn_P78mX8U7cBYbWA8HxxgT2fo7DtnGEWXN\t0\tBORIS FOELSCH\nsesn_zeKiyDToh2SwItdUY6LGtzyXWHut9zYE\t0\tBORIS FOELSCH\nsesn_FHfAE05hkPJAUJS0tGwTLN49SRBAvVCT\t0\tBORIS FOELSCH\nsesn_yoS16UeYVitFP2rBheUY4wUkeARSO1e4\t0\tBORIS FOELSCH\nsesn_szRVg8rdfetxd3cKeus2NSkfZzcXoZSs\t0\tBORIS FOELSCH\nsesn_DWw5rEkdwmg2jyNVNVjE29CajcMIHXxo\t0\tBORIS FOELSCH\nsesn_AGOc8yQ2PjWd6YXj17r2GIvDlUs2dLyx\t0\tBORIS FOELSCH\nsesn_R46OG087rgXvIWh0SCX66Hxe6y8KkUbK\t0\tBORIS FOELSCH\nsesn_FvBJ5ozZ6Abh2v90WGdFIZqpjonTaaMY\t0\tBORIS FOELSCH\nsesn_C40AWy0plvhLvEnpjz5Zh5kdjXIp3CBF\t0\tBORIS FOELSCH\nsesn_1lJVa5wGECcwToEk5kxcHGvZyNrRAfEG\t0\tBORIS FOELSCH\nsesn_PZXISy7BgxVwVoINGTCwcTukQFu0MqMc\t0\tBORIS FOELSCH\nsesn_gWBFjjMWKvKYSD3cxtrBsiei7vgWHn9X\t0\tBORIS FOELSCH\nsesn_20Ve7Qi9y30hQMPFpcL1fts6IavHHxUe\t0\tBORIS FOELSCH\nsesn_Z6dWKPK2pNsDpuZAdKQaYujWs8Yj4NRp\t0\tBORIS FOELSCH\nsesn_gvOLtj1W3KAvBSfIZz4CR78OLgGI1SQA\t0\tBORIS FOELSCH\nsesn_o0k670v6KJfbWZlk7LDSspIWwa0yGqga\t0\tBORIS FOELSCH\nsesn_FsLz27kZ0DteoUlAUvXVVJ1XSwqxJ04u\t0\tBORIS FOELSCH\nsesn_vI8zNVXRLcqxl2xiziOvaa2YcQO5PVtZ\t0\tBORIS FOELSCH\nsesn_hkIUPmrH6tAvicvSGQ9YK2yWQ3X4y5uu\t0\tBORIS FOELSCH\nsesn_ORuJBfdlZ6VAGBAyV7haOpM6JkLwf7RI\t0\tBORIS FOELSCH\nsesn_dCxwyaPliz3utTzvnyQlFccsp7ezHdWR\t0\tBORIS FOELSCH\nsesn_fR4fXTJq7wisacjEZsGseIOo8kMMzIYC\t0\tBORIS FOELSCH\nsesn_1qVisiphAkITjKwQzFKCHlkT40Fx2l6P\t0\tBORIS FOELSCH\nsesn_9Vs0mHHJN3mFr24QAZNbfY2FDTGU2Zx0\t0\tBORIS FOELSCH\nsesn_CxccFLeiAVNVZE8rsVbReBuTuSLzWAVh\t0\tBORIS FOELSCH\nsesn_xWL421Dpd2z0LLoHffFgPQTWKLHXlAu8\t1\tORVILLE HAUSKEN\nsesn_CRsusJe0hckgw8rZfWWHUT0GUb53dRP0\t1\tORVILLE HAUSKEN\nsesn_WCr9VSUJTDVwR6wdDNLmrfNA1P4fGJES\t1\tORVILLE HAUSKEN\nsesn_YzRCWwMvvN0SlGtflWHS9AA0A6mDjQH3\t1\tORVILLE HAUSKEN\nsesn_8U402bBk0Dguz7PQur2kV88dvx8BnVsJ\t1\tORVILLE HAUSKEN\nsesn_ERJGxX7DNSrHlpoNvvdE6nU0V7RNZH53\t1\tORVILLE HAUSKEN\nsesn_Gh27XYNLoQmyjUmxo6tdZzX86KLO6vfx\t1\tORVILLE HAUSKEN\nsesn_AboqsD59rGn6F2JrXruaB4xy0Jqj9a1M\t1\tORVILLE HAUSKEN\nsesn_renmAkLkHRihr9xJrm1sTGlbZuLe7YOo\t1\tORVILLE HAUSKEN\nsesn_d7r2mak7MqXLnQMXwHfOONVwgg63GJV3\t1\tORVILLE HAUSKEN\nsesn_ct4Go25KB1RJA7pnpvh6jjpIYncouwSE\t1\tORVILLE HAUSKEN\nsesn_ERF92Kf3GZUYLduJifCxx7CP9q8t8nGc\t1\tORVILLE HAUSKEN\nsesn_u5D2QeYvlxh8E8GoSWn5tQJZSlhGTdkL\t1\tORVILLE HAUSKEN\nsesn_6nnDY0dRwAV6AhsTDepmmagxNkxugz3B\t1\tORVILLE HAUSKEN\nsesn_nIuKOm3tUH5IkRm0KRsxolGCscc6nD76\t1\tORVILLE HAUSKEN\nsesn_dBFYolGdYfETWlIuFHNTgcoBJDiE92lN\t1\tORVILLE HAUSKEN\nsesn_4FyHZVNH1a0dILScco6saxpSF1FytNeb\t1\tORVILLE HAUSKEN\nsesn_qZuRs99g8odha7LVMCWaDNHciuBrjGGT\t1\tORVILLE HAUSKEN\nsesn_Ed5IZLonIkj3FHM1xe4JQCXCNLrjIG3J\t1\tORVILLE HAUSKEN\nsesn_387PZiFzSfn75FnBFqaH9IKRmqaGbFXZ\t1\tORVILLE HAUSKEN\nsesn_9FRypz4g6yRXN0b71QFD9OCswOlHytG1\t1\tORVILLE HAUSKEN\nsesn_KwzcIDqtxVWi325rQkxCJBPK8t95gf2K\t1\tORVILLE HAUSKEN\nsesn_eZIkjdIPb3zPqwt2mlopdVF8VFdMEjLH\t1\tORVILLE HAUSKEN\nsesn_9diAco8WQW6zgHX9yBOlhb286udbx1iR\t1\tORVILLE HAUSKEN\nsesn_DatcHoGwkfaoiJavnX4L6qxsP3x3Rwf3\t1\tORVILLE HAUSKEN\nsesn_gpYuLf9lNOcqOyypQwkR38wt81qjsJOB\t1\tORVILLE HAUSKEN\nsesn_mqkoks0gR1Ii7yz04TCVrvO8AiU9Wv9H\t1\tORVILLE HAUSKEN\nsesn_9T0xB64l2PzYNFzQPRNWg3EfcfSadUtP\t1\tORVILLE HAUSKEN\nsesn_nqQyG7xdz0EsYCql8d9bBMTON3G7UDG6\t1\tORVILLE HAUSKEN\nsesn_M5Db1sbe9PjEDyF42xt8eqz6CJlPULdE\t1\tORVILLE HAUSKEN\nsesn_u4ZIRbmRVraVxWcj8SlXG6PxToQfrSZt\t1\tORVILLE HAUSKEN\nsesn_4gin7rPqJogvBCeRxbyeCEGAftAIgP10\t1\tORVILLE HAUSKEN\nsesn_cxenHXBujZhknVZP77w9yX36dneOBVA5\t1\tORVILLE HAUSKEN\nsesn_TIaKVrMtOGpNG76Mbq7y48oTXDtptB0p\t1\tORVILLE HAUSKEN\nsesn_AyU0LCd3WOCPzNynB9auUUBFfxFGSyjV\t1\tORVILLE HAUSKEN\nsesn_IMgIn6ldDXDJPD9Ib0CBhdA7KNUUHjKB\t2\tRICARDO DEBUTTS\nsesn_nMDD8gPToSlKL1UL6wTAnaUYPOppuugt\t2\tRICARDO DEBUTTS\nsesn_YvlUI3n8Eryk53G23waw5XIjNy5WyOsd\t2\tRICARDO DEBUTTS\nsesn_OuzZuLeld0RToSl08JraX5gA8AzqQqIO\t2\tRICARDO DEBUTTS\nsesn_kPDBr3n2bIPMq9sYYY2r2I9beFpS8Y4W\t2\tRICARDO DEBUTTS\nsesn_WD8XEhHk9VeaL33Eh91sKOQlGOHt8y3i\t2\tRICARDO DEBUTTS\nsesn_bqldlJNr72cmkUOD3zlYuzUGIdzLPNDf\t2\tRICARDO DEBUTTS\nsesn_pGUgYjb58H7zOrEb8HZgWQ9PJbRw0JDv\t2\tRICARDO DEBUTTS\nsesn_lTIioeKUESsRlOKrjVIZvtpZ24WBFbQa\t2\tRICARDO DEBUTTS\nsesn_n0cOmdRMUN84tIKx9QiTDM46cLxQOgSy\t2\tRICARDO DEBUTTS\nsesn_GIWPvgRuj9UjbzcFGPzRvKtIkz5wLhkA\t2\tRICARDO DEBUTTS\nsesn_GVJEybYVo9j2SXED2w41sPh9sz3mSOCO\t2\tRICARDO DEBUTTS\nsesn_x1Kv68C1hWTAvvt1qRtil84T8TEtHMX3\t2\tRICARDO DEBUTTS\nsesn_EmRw2daTzBzipOLP223MbN41psUIjzZ3\t2\tRICARDO DEBUTTS\nsesn_BMrw76uqLVH1S1mfMAgcrAO9NhtmC14c\t2\tRICARDO DEBUTTS\nsesn_9urZld0VJkcD178hz4V1HOFW7XF0qCKp\t2\tRICARDO DEBUTTS\nsesn_W7LTElzUAj7Jpnb7jVPdCu3dTXk9qPgt\t2\tRICARDO DEBUTTS\nsesn_HG03hjgRvlJB83wAxUqHM1zpF4yf2rH9\t2\tRICARDO DEBUTTS\nsesn_s3zEJcK3MLMRTtCKQAqOEW3JPuh3Afds\t2\tRICARDO DEBUTTS\nsesn_LtARd0hoTVXogORTg3te3L3vPIgOjvog\t2\tRICARDO DEBUTTS\nsesn_ZLIxvIyMF2dNDjjeFbDBPee84Gvp9D2l\t2\tRICARDO DEBUTTS\nsesn_iwiU6Nzx8XG8yRtXbiNx4FlYXUZncgbg\t2\tRICARDO DEBUTTS\nsesn_PMrnVVQnPWBuwThnVGqIAEcm8Z9ONLL2\t2\tRICARDO DEBUTTS\nsesn_2hA3cN6d4zMwtkDrQ8uHRN3fotmdCeXK\t2\tRICARDO DEBUTTS\nsesn_ssYOGg5npOUm1NB6DeKvFIBikZTGzyDc\t2\tRICARDO DEBUTTS\nsesn_AXK5s6Iyg3HLMHZbFAYq2Ig8tRglwQlX\t2\tRICARDO DEBUTTS\nsesn_ajPJ0eeMbMGxrobZwUXRVGVeX3LMqlYg\t2\tRICARDO DEBUTTS\nsesn_4JfhTPxIqGGNrjQmzEys05ILW2inxuQN\t2\tRICARDO DEBUTTS\nsesn_KMXzteChy0WkTLfbd8LbZSMgLKV4dKqS\t2\tRICARDO DEBUTTS\nsesn_TiyzRdrDFyxe0h06cWutnaeOApVsqh3x\t2\tRICARDO DEBUTTS\nsesn_CeMfVfPKf0OwvnXhVu5ypEKJ8P9tivX5\t2\tRICARDO DEBUTTS\nsesn_jx9qtjJy5WGkoeUr2I6NSZvnfh3bxpVn\t2\tRICARDO DEBUTTS\nsesn_JbHglWQKDwxis2DFZZUMoGOeXH9WJtQ3\t2\tRICARDO DEBUTTS\nsesn_4VVsK9IdbbfoIHEj1dxUhqXlSRInvZo8\t2\tRICARDO DEBUTTS\nsesn_ZBtVBNAlHNHsIcXMseOOuFgpEVh0CH6H\t2\tRICARDO DEBUTTS\nsesn_3HM2G1aqmnZqGQnGau3hoNmS7otaVF7f\t2\tRICARDO DEBUTTS\nsesn_yHVENefVrGXp39C9rlOwmEGmxa64GMo1\t2\tRICARDO DEBUTTS\nsesn_Fgca1EWYS2DughQRjeyl2a0kDix1mGKP\t2\tRICARDO DEBUTTS\nsesn_Sx3nb9CpdIZeiMc12EQLUzrrXLWyWBCS\t2\tRICARDO DEBUTTS\nsesn_0dos33T3wLjIge1bAr7pFx2RDbY8muVt\t2\tRICARDO DEBUTTS\nsesn_bc7FJvBnfHZ5rERYu3r6oHv2fF61ZA8r\t2\tRICARDO DEBUTTS\nsesn_ZO9Jb4MYOqrCLBEnSlhtqRp2lKCt8gac\t2\tRICARDO DEBUTTS\nsesn_8N9sjCyKJ14DwmRXTyCgtyOVtHzxLXGn\t2\tRICARDO DEBUTTS\nsesn_dQvKMkRMCW67c5kdm4mnynHldRy6qvav\t2\tRICARDO DEBUTTS\nsesn_tUZE8MhGah3NUkti7AVvh8h8ylZV5jPM\t2\tRICARDO DEBUTTS\nsesn_d3FNJk8IDcI5fgdeyRSqcymSDBH3i58B\t2\tRICARDO DEBUTTS\nsesn_ZojmOh5R9UX5ol3PuSPqfnhNyIEhs9Vd\t2\tRICARDO DEBUTTS\nsesn_HglurNqS82OigSfBFDVdYYKWw1SySlkd\t2\tRICARDO DEBUTTS\nsesn_rdvO5F4LKx1yCDq0uD6wie09vsaTHMmL\t2\tRICARDO DEBUTTS\nsesn_mfqrkyfLnEEJCbHOkJWHkgulFePlveLf\t2\tRICARDO DEBUTTS\nsesn_wHuRHNXsCCXqmaVXnlWxrBfZ6yEuiKJf\t2\tRICARDO DEBUTTS\nsesn_tILn40mGy7p4QxKM415UkIonOdUPi41M\t2\tRICARDO DEBUTTS\nsesn_IcIfBVjRVQIGQ82GpLk9Sodfnwl2bOME\t2\tRICARDO DEBUTTS\nsesn_wv3ji9BHhvCvxKDpFeWizNLAoqgz9hqT\t2\tRICARDO DEBUTTS\nsesn_mTvupQStaQ7UR3XSKHNNwqBhSuF5PwEd\t2\tRICARDO DEBUTTS\nsesn_nxJ1ZcTQ6URkjl7BSocIkHJ7fCaatT6C\t2\tRICARDO DEBUTTS\nsesn_sVcT4dGp5n603LpImjVq7mHVnsMj6PeL\t2\tRICARDO DEBUTTS\nsesn_zKb9deYCprSjZNxdHojTJ85Zw5QODDZU\t2\tRICARDO DEBUTTS\nsesn_HSYJFbRokM7n26vzSrHoDtDoM2gz4Rn9\t2\tRICARDO DEBUTTS\nsesn_nrytYCS25G4JbPWpajmQ9UaZXs4WTXKb\t2\tRICARDO DEBUTTS\nsesn_ObEkIxNe0vozoUxwzysyDwPvu3SMZGNd\t2\tRICARDO DEBUTTS\nsesn_9xzuYC52Ka9vv8VMdRo8yMvYvT6zRUQ3\t2\tRICARDO DEBUTTS\nsesn_wAb7g5KMrihUQKzoyITVrlpUoAuJCwNC\t2\tRICARDO DEBUTTS\nsesn_uEArkW7Ol4AtJja6VIegVgpXcyJqMBWp\t2\tRICARDO DEBUTTS\nsesn_c0LySD10cl3cEzcXlIU3BoxnvuuxHhai\t2\tRICARDO DEBUTTS\nsesn_dLrZV2fWzyqRVJOGQqhlivCMJfYwVZTx\t2\tRICARDO DEBUTTS\nsesn_ymaD0cTRHwZnGlZq6xOTEK2ztVJqteLO\t2\tRICARDO DEBUTTS\nsesn_RgodEXuyChGARg4T6PIpxhAvCvjFD9ft\t2\tRICARDO DEBUTTS\nsesn_1P8bl4TGaK586GSrvS1LfRWAq9IzdBo5\t2\tRICARDO DEBUTTS\nsesn_8eNYIYGIwgn7moiJraV9Fny8bHzfkGNI\t2\tRICARDO DEBUTTS\nsesn_Fq2egx0ZjIv0rgWfIlj4Qa39aBkXqxAc\t2\tRICARDO DEBUTTS\nsesn_yDqhyzJ67DZ9lHiqSntV1yLGwveZrqM6\t2\tRICARDO DEBUTTS\nsesn_Y6eMwrCKHrOuEhd7m8VJb7adIPsOMLOJ\t3\tWILEY ROESSLE\nsesn_m5bjjR1a9CCZTWrbF45m37GYnh3GoccU\t3\tWILEY ROESSLE\nsesn_mK52HHe1YKAhM7h9k99uZlrBk4MFa7wB\t3\tWILEY ROESSLE\nsesn_EWGyxo3Yzvu2BbBco3hhNId7m8OU1gfg\t3\tWILEY ROESSLE\nsesn_E92mFtXzVFbRIWdOfmQb4pTvCzpNJvJH\t3\tWILEY ROESSLE\nsesn_X16n0ERhZukZyCfLdudVdzYh04CaEx6l\t3\tWILEY ROESSLE\nsesn_FgKQn3ADW7s59w1qsNNErEyjZIr53ZQg\t3\tWILEY ROESSLE\nsesn_BLsUy9yAV2XfksyL2wVvjLLS0oDDi13G\t3\tWILEY ROESSLE\nsesn_BDuWt7uOfDUZCQrggosbv9wHURxTlIHA\t3\tWILEY ROESSLE\nsesn_Rf0iT0uKE3xkUWf3cEIqXo6dHxWgv4Ka\t3\tWILEY ROESSLE\nsesn_sOIbw9HJrvBzaPmVvw7lG4O381E0231B\t3\tWILEY ROESSLE\nsesn_6BzmfFTNstDJh2M4puIeSuPTPae5b2TZ\t3\tWILEY ROESSLE\nsesn_WXRG66QZVRoVD2QCjCVBEKrglZFfotDc\t3\tWILEY ROESSLE\nsesn_kwdgMvGziWEyqVQy2JsSFhNCFQFGxNvp\t3\tWILEY ROESSLE\nsesn_NqArogXUzWujf8qeVvgXJu15qKs6UQL2\t3\tWILEY ROESSLE\nsesn_qHOXnctVimXNmXWRr4zckwbzjz3ukrfs\t3\tWILEY ROESSLE\nsesn_u5YP3EAC3RL59iYOMnaokEXVAMt39D1M\t3\tWILEY ROESSLE\nsesn_L2KkYqTS6p1EaeVp7ZoQpHOjIEy2eprx\t3\tWILEY ROESSLE\nsesn_pWi1cKuLhAQRCDbSxyFyE1mW8motWZuW\t3\tWILEY ROESSLE\nsesn_C0S5ar8HjCbMN2s66zPnipEkbeHSySTn\t3\tWILEY ROESSLE\nsesn_IDu1gBpJ5hkeUKkc5NcjdtwgQ90Jm9ef\t3\tWILEY ROESSLE\nsesn_bpunqBDePgvjoADuVffe25rK4vvJFDJH\t3\tWILEY ROESSLE\nsesn_lFYZOWg1jdDqvuVJAQnMHdWH9I1fAtJn\t3\tWILEY ROESSLE\nsesn_oIAZYW58GsY06g1bKHpIGniPcSplYoXw\t3\tWILEY ROESSLE\nsesn_GsP6KhdTxoL4vyt6Mr7YW30spZ6YhYkZ\t3\tWILEY ROESSLE\nsesn_gc59KdblP34YOe6qngyUIiRhLsMoKxTd\t3\tWILEY ROESSLE\nsesn_PESFx72IusxgFFegYT5hfD3jFKt9V8sV\t3\tWILEY ROESSLE\nsesn_88ZHtsvHOe09sSdCLgY4TMLOHE0s4GO7\t3\tWILEY ROESSLE\nsesn_wbpqd4y9VAyttS526JLUOwn59Gyz4gsm\t3\tWILEY ROESSLE\nsesn_vDYDvbbnDuZlQmt2jrYfnXMEATauKYWW\t3\tWILEY ROESSLE\nsesn_JpYoCCI2ZMzAo1sa82XDWtcH3XtvycrH\t3\tWILEY ROESSLE\nsesn_48xsSI7OsNhA34wMpYT5VJojk9Bb2Cqw\t3\tWILEY ROESSLE\nsesn_afht4WJai1Nl77X6o9ZsPFvOCsYLGfA9\t3\tWILEY ROESSLE\nsesn_wf3eYSy5Nz94nHH47SRQGwcwcvZl7KYH\t3\tWILEY ROESSLE\nsesn_BdQDiZoVJi4aSMSojkUG55wZPnF6YeLY\t3\tWILEY ROESSLE\nsesn_boG4ITlJote2ouAUBOLHJLvHk3x5l6R1\t3\tWILEY ROESSLE\nsesn_6HlBCibKBEnPF6A11W8d0P3qIicvppHZ\t3\tWILEY ROESSLE\nsesn_a1TckZ64lK3k7SEuM6YI8dZex8BURy32\t3\tWILEY ROESSLE\nsesn_qZoD2uIVIHYCnJqoch97KafvPclp2Xme\t3\tWILEY ROESSLE\nsesn_PxJkuG8BmHBBCA4m3xYAMNikOH2HcA1t\t3\tWILEY ROESSLE\nsesn_aj8RWzDM3iHkeIDyH8ObILivivMfTdun\t3\tWILEY ROESSLE\nsesn_dh3MNztBwr2nQKt1UE2bJJeqytN7Pdet\t3\tWILEY ROESSLE\nsesn_GHjVDviSPYjl4SQ3lajKZyXaA3aav2jB\t3\tWILEY ROESSLE\nsesn_dLebdKglTIGIqjnW9WZ1DPD32BUuPdhj\t3\tWILEY ROESSLE\n"}]},"apps":[],"jobName":"paragraph_1529954865981_-200574363","id":"20160611-124521_33806293","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:550"},{"text":"%md\n## Find Connected Users with Multiple Names\nNormally all sessions of user should be connected together, this means that a single user handle should map to a single name. Let's check this via some aggregation","dateUpdated":"2018-06-25T19:27:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Find Connected Users with Multiple Names</h2>\n<p>Normally all sessions of user should be connected together, this means that a single user handle should map to a single name. Let's check this via some aggregation</p>\n"}]},"apps":[],"jobName":"paragraph_1529954865984_-214040574","id":"20180625-191356_56147580","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:551"},{"text":"val result = combinedUsers\n    .groupBy($\"user_handle\").agg(countDistinct($\"name\") as \"name_count\")\n    .where($\"name_count\" > 1)\n\nz.show(result.limit(100))","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_handle: bigint, name_count: bigint]\n"},{"type":"TABLE","data":"user_handle\tname_count\n"}]},"apps":[],"jobName":"paragraph_1529954865988_-215579570","id":"20180624-175057_1873045425","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:552"},{"text":"%md\n## Find Names with disconnected Sessions\nIt can well happen that not all sessions of a single users are really connected together. Let's check that.","dateUpdated":"2018-06-25T19:27:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Find Names with disconnected Sessions</h2>\n<p>It can well happen that not all sessions of a single users are really connected together. Let's check that.</p>\n"}]},"apps":[],"jobName":"paragraph_1529954865993_-217503314","id":"20180625-191547_734155092","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:553"},{"text":"val result = combinedUsers\n    .groupBy($\"name\").agg(countDistinct($\"user_handle\") as \"handle_count\")\n    .where($\"handle_count\" > 1)\n\nz.show(result.limit(100))","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, handle_count: bigint]\n"},{"type":"TABLE","data":"name\thandle_count\nALVA LUTTERLOH\t2\nWES RYNKOWSKI\t2\nKAREN MAILHOT\t3\nDORSEY PLIER\t2\nCHINA CONSTAS\t2\nMARSHA CORPUS\t2\nFAUSTINO TAUBEL\t2\nJULIANNE BARLOTTA\t2\nLURLINE DEMNY\t2\nDELORAS NESBETT\t2\nLOVETTA SEVERIO\t2\nLAWERENCE JANIK\t2\nNAKIA INGLISH\t2\nDENIS WIZE\t2\nBRAD PYSHER\t2\nARIEL SINDLER\t2\nDANA CARACCIOLD\t2\nKELVIN LUNG\t2\nULYSSES REHORST\t2\nLANNY KERSLAKE\t2\nRONNY MARDIAN\t2\nGITA WERTS\t2\nJEREMIAH DURLER\t5\nPEARLY LADA\t2\nBETHEL MAESHIRO\t2\nGAYLORD SIGARAN\t2\nEMILE BEZANSON\t2\nBEA LECHNIR\t2\nKAROL ROZEWSKI\t2\nCOLIN MERL\t2\nLUPE PALE\t2\nYAHAIRA PANT\t2\nORETHA SECCO\t2\nLONNIE HOLTMEIER\t2\nCHI NEWLING\t2\nJOANNIE TAPANES\t2\nDANIELE MCCONAUGHY\t2\nMAILE MOAT\t2\nJOSUE FOLLAND\t2\nDELSIE GILLIARD\t3\nAILENE MARTINEZORTIZ\t2\nSTACEY MECHURA\t2\nCLAUDE KISS\t2\nNAKITA LAVATO\t2\nROSIE TRICKEL\t2\nDENEEN AKIMOTO\t2\nSAMUAL KHADEMI\t2\nALEJANDRO SWATTS\t2\nLURLINE LAFLIN\t2\nMOHAMMED GRANADINO\t2\nPATTI QUEBEDO\t2\nTAD FEIRICK\t2\nMARSHALL TANCREDI\t2\nPAMELIA LABOSCO\t2\nDARRELL CURZON\t2\nMATT DAMIN\t2\nEMILIO GRUMLEY\t2\nCODY CALZARETTA\t2\nEUSEBIO BANDOLA\t2\nJASMINE BATESOLE\t2\nTHERESA KUHL\t2\nDAWNA OMANOVIC\t2\nTAMISHA RUSSAK\t2\nNERY CAMIT\t2\nJOE DEVORA\t2\nTOMMY DAVOLT\t2\nBEAU APPLEHANS\t2\nKANDICE LYCETT\t2\nZANE DROLL\t3\nCOURTNEY STITZER\t2\nHORTENSE RUSSEL\t2\nJASMIN GALI\t2\nSCOT YINGER\t2\nJEFF MEPHAM\t2\nEFREN TEBBEN\t3\nCARYL GRUNIG\t2\nHARRIETTE ROSATO\t2\nMICHIKO VANGIESON\t2\nARNOLDO SPECKS\t2\nDAVE RODREGEZ\t2\nRUSSELL SCULLARK\t3\nTORRI BOITER\t2\nCLARK HENSE\t2\nGERARDO MAHABIR\t3\nKYONG EHRLER\t2\nDYAN ARBOGAST\t3\nPATRICK GEILE\t2\nISAIAS CORRADINI\t2\nLEWIS WUDEL\t2\nLENARD DUSING\t2\nANTONY BURISH\t2\nBENEDICT SWIMMER\t2\nDARELL ABDELMESSIH\t2\nDAMON SICKING\t2\nPETER KUELPER\t2\nMARGARET DELLAPIETRA\t2\nCOLEEN CURREA\t2\nLINWOOD OLAFSSON\t2\nCHEYENNE TRANCOSO\t2\nMITCHELL KRETH\t2\n"}]},"apps":[],"jobName":"paragraph_1529954865996_-218657561","id":"20180624-175402_1192604169","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:554"},{"text":"result.count()","dateUpdated":"2018-06-25T19:27:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res51: Long = 3415\n"}]},"apps":[],"jobName":"paragraph_1529954865997_-219042310","id":"20180625-191638_1356484345","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:555"},{"text":"%md\n# Cleanup Memory","dateUpdated":"2018-06-25T19:27:46+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Cleanup Memory</h1>\n"}]},"apps":[],"jobName":"paragraph_1529954865999_-218272812","id":"20160611-101803_1360438852","dateCreated":"2018-06-25T19:27:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:556"},{"text":"sc.getPersistentRDDs.foreach { entry => entry._2.unpersist() }","dateUpdated":"2018-06-25T19:27:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1529954866000_-207884592","id":"20160611-101823_1003062743","dateCreated":"2018-06-25T19:27:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:557"},{"dateUpdated":"2018-06-25T19:27:46+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529954866001_-208269341","id":"20160611-104058_1531646874","dateCreated":"2018-06-25T19:27:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:558"}],"name":"GraphX User Linking Solution","id":"2DK6SUE1W","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}