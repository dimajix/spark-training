{"paragraphs":[{"text":"%md\n# Load Dependencies\nFirst we have to load additional dependencies that we want to use. In this case we want to use a CSV parser.","dateUpdated":"Jun 11, 2016 3:59:09 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228149_-1252760307","id":"20160611-153348_581646545","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Dependencies</h1>\n<p>First we have to load additional dependencies that we want to use. In this case we want to use a CSV parser.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:09 PM","dateFinished":"Jun 11, 2016 3:59:09 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:52"},{"text":"%dep\nz.load(\"com.databricks:spark-csv_2.10:1.4.0\")","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228150_-1251606060","id":"20160611-153348_387920671","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"text":"%md\n# Pretty Printing\n\nAs usual, we want to format DataFrames for Zeppelin. Put in your solution here.","dateUpdated":"Jun 11, 2016 3:36:34 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228150_-1251606060","id":"20160611-153348_22365205","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Pretty Printing</h1>\n<p>As usual, we want to format DataFrames for Zeppelin. Put in your solution here.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:36:32 PM","dateFinished":"Jun 11, 2016 3:36:32 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"text":"// Put your solution for the implicit method 'toZeppelin' in here\n\n","dateUpdated":"Jun 11, 2016 3:37:09 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228150_-1251606060","id":"20160611-153348_82362210","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"text":"%md\n# 1. Option A: Load Names into Hive\nWe have some name lists from US Census 1990 and US Census 2000. We simply put them into Hive for easy access.","dateUpdated":"Jun 11, 2016 3:59:17 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228150_-1251606060","id":"20160611-153348_1282194802","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>1. Option A: Load Names into Hive</h1>\n<p>We have some name lists from US Census 1990 and US Census 2000. We simply put them into Hive for easy access.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:18 PM","dateFinished":"Jun 11, 2016 3:59:18 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"%sql\ncreate database if not exists training","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":94,"optionOpen":false,"keys":[{"name":"result","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"result","index":0,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228150_-1251606060","id":"20160611-153348_1997123216","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"text":"%sql\ncreate external table if not exists training.last_names(\n    name string,\n    rank int,\n    count int,\n    prop100k double,\n    cum_prop100k double,\n    pctwhite string,\n    pctblack string,\n    pctapi string,\n    pctaian string,\n    pct2prace string,\n    pcthispanic string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE\nlocation 's3://is24-data-dev-spark-training/data/names/last_all_c2000'","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228151_-1251990809","id":"20160611-153348_480488702","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"text":"%sql\nselect * from training.last_names limit 10","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228151_-1251990809","id":"20160611-153348_1035101794","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"text":"%sql\ncreate external table if not exists training.first_names_male(\n    name string,\n    prop double,\n    cum_prop double,\n    rank int\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE\nlocation 's3://is24-data-dev-spark-training/data/names/first_male_c1990'","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228151_-1251990809","id":"20160611-153348_190827411","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"text":"%sql\nselect * from training.first_names_male limit 10","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228151_-1251990809","id":"20160611-153348_1043211706","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"text":"%sql\ncreate external table if not exists training.first_names_female(\n    name string,\n    prop double,\n    cum_prop double,\n    rank int\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE\nlocation 's3://is24-data-dev-spark-training/data/names/first_female_c1990'","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228152_-1253914554","id":"20160611-153348_1202648634","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"text":"%sql\nselect * from training.first_names_female limit 10","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228152_-1253914554","id":"20160611-153348_799506074","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"text":"%md\n# 1. Option B: Load Data from Files\n\nProbably the more realistic way to load data in this specific case is to read it directly as a CSV file. Moreover we specifically added a dependency at the beginning, so we can use spark-csv to load CSV data without much hassle.","dateUpdated":"Jun 11, 2016 3:59:23 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228152_-1253914554","id":"20160611-153348_493406439","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>1. Option B: Load Data from Files</h1>\n<p>Probably the more realistic way to load data in this specific case is to read it directly as a CSV file. Moreover we specifically added a dependency at the beginning, so we can use spark-csv to load CSV data without much hassle.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:23 PM","dateFinished":"Jun 11, 2016 3:59:23 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"import org.apache.spark.sql.types._\n\nval lastNamesSchema = StructType(Array(\n    StructField(\"name\", StringType, true),\n    StructField(\"rank\", IntegerType, true),\n    StructField(\"count\", IntegerType, true),\n    StructField(\"prop100k\", DoubleType, true),\n    StructField(\"cum_prop100k\", DoubleType, true),\n    StructField(\"pctwhite\", StringType, true),\n    StructField(\"pctblack\", StringType, true),\n    StructField(\"pctapi\", StringType, true),\n    StructField(\"pctaian\", StringType, true),\n    StructField(\"pct2prace\", StringType, true),\n    StructField(\"pcthispanic\", StringType, true)\n    ))\nval lastNames = sqlContext.read\n    .format(\"com.databricks.spark.csv\") \n    .option(\"header\", \"true\")  // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .load(\"s3://is24-data-dev-spark-training/data/names/last_all_c2000\")","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228152_-1253914554","id":"20160611-153348_1247659206","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"lastNames.toZeppelin(10)","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228153_-1254299303","id":"20160611-153348_1377945934","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"text":"val firstNamesSchema = StructType(Array(\n    StructField(\"name\", StringType, true),\n    StructField(\"prop\", DoubleType, true),\n    StructField(\"cum_pro\", DoubleType, true),\n    StructField(\"rank\", IntegerType, true)\n    ))\nval firstNamesMale = sqlContext.read\n    .format(\"com.databricks.spark.csv\") \n    .option(\"header\", \"false\")\n    .schema(firstNamesSchema)\n    .load(\"s3://is24-data-dev-spark-training/data/names/first_male_c1990\")\nval firstNamesFemale = sqlContext.read\n    .format(\"com.databricks.spark.csv\") \n    .option(\"header\", \"false\")\n    .schema(firstNamesSchema)\n    .load(\"s3://is24-data-dev-spark-training/data/names/first_female_c1990\")    ","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228153_-1254299303","id":"20160611-153348_673663490","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"text":"firstNamesMale.toZeppelin(10)","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228153_-1254299303","id":"20160611-153348_728489446","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"text":"%md\n# 2. Create Distribution for various Parameters\n\nNow we need to have some nice random distributions for three different use cases:\n\n* Number of CDKs per user\n* Number of Sessions per user\n* Number of CDKs used per session\n\nWe will use a lognormal distribution for that","dateUpdated":"Jun 11, 2016 3:59:28 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228153_-1254299303","id":"20160611-153348_917847375","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>2. Create Distribution for various Parameters</h1>\n<p>Now we need to have some nice random distributions for three different use cases:</p>\n<ul>\n<li>Number of CDKs per user</li>\n<li>Number of Sessions per user</li>\n<li>Number of CDKs used per session</li>\n</ul>\n<p>We will use a lognormal distribution for that</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:28 PM","dateFinished":"Jun 11, 2016 3:59:28 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"text":"%md\n## Lognormal Distribution\n\nThe following definition provides us with a lognormal distribution.","dateUpdated":"Jun 11, 2016 3:59:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228154_-1253145056","id":"20160611-153348_885795224","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Lognormal Distribution</h2>\n<p>The following definition provides us with a lognormal distribution.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:30 PM","dateFinished":"Jun 11, 2016 3:59:30 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"text":"val lognormal = sqlContext.udf.register(\"lognormal\", (a:Double) => scala.math.exp(a*scala.util.Random.nextGaussian()) )","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228154_-1253145056","id":"20160611-153348_1203688764","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"text":"%md\n## Number of Linking Keys per User\n\nFirst let's create a random distribution which will be used for sampling the number of linking keys (or cross device key, CDKs) per user. A linking key could be an e-mail address, a user-id of a web-site, a phone number etc. Realistically you'll probably find hashed data for these values, but nontheless that would be good enough for our use case. We assume that a user has a couple of these IDs, normally maybe 10 - 20.","dateUpdated":"Jun 11, 2016 3:59:33 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228154_-1253145056","id":"20160611-153348_1101101808","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Number of Linking Keys per User</h2>\n<p>First let's create a random distribution which will be used for sampling the number of linking keys (or cross device key, CDKs) per user. A linking key could be an e-mail address, a user-id of a web-site, a phone number etc. Realistically you'll probably find hashed data for these values, but nontheless that would be good enough for our use case. We assume that a user has a couple of these IDs, normally maybe 10 - 20.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:33 PM","dateFinished":"Jun 11, 2016 3:59:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"text":"sc.parallelize(1 to 100000, 100).toDF\n    .select((lit(14.0)*lognormal(lit(0.3))) cast IntegerType  as \"count\")\n    .select(when($\"count\" < lit(200),$\"count\") otherwise lit(200) as \"count\" )\n    .groupBy($\"count\").count().toZeppelin()","dateUpdated":"Jun 11, 2016 3:38:01 PM","config":{"enabled":true,"graph":{"mode":"multiBarChart","height":414,"optionOpen":false,"keys":[{"name":"count","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"count","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228154_-1253145056","id":"20160611-153348_1351500857","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"text":"%md\n# Create Distribution for Number of Sessions\n\nEach user will create multiple web browser sessions. Some users only have a few sessions, while other have more sessions. We assume that a typical user has around 40 sessions, but there are also more extreme users with more sessions (for example those users who always clear their cookies will generate much more sessions - but this won't be modelled very good here.)","dateUpdated":"Jun 11, 2016 3:59:36 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228154_-1253145056","id":"20160611-153348_740369280","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Create Distribution for Number of Sessions</h1>\n<p>Each user will create multiple web browser sessions. Some users only have a few sessions, while other have more sessions. We assume that a typical user has around 40 sessions, but there are also more extreme users with more sessions (for example those users who always clear their cookies will generate much more sessions - but this won't be modelled very good here.)</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:36 PM","dateFinished":"Jun 11, 2016 3:59:36 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"text":"sc.parallelize(1 to 100000, 100).toDF\n    .select((lit(50.0)*lognormal(lit(0.4))) cast IntegerType  as \"count\")\n    .select(when($\"count\" < lit(2000),$\"count\") otherwise lit(2000) as \"count\" )\n    .groupBy($\"count\").count().toZeppelin()","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"count","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"count","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228155_-1253529805","id":"20160611-153348_866238242","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:75"},{"text":"%md\n# Create Distribution of Number of Linking Keys per Session\n\nNow each user has around 40 linking keys - it would be too easy if he used all those keys in all his sessions. Therefore we assume that only very few of these keys will actually be used in each session, something like 1-4.","dateUpdated":"Jun 11, 2016 3:59:38 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228155_-1253529805","id":"20160611-153348_1074490281","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Create Distribution of Number of Linking Keys per Session</h1>\n<p>Now each user has around 40 linking keys - it would be too easy if he used all those keys in all his sessions. Therefore we assume that only very few of these keys will actually be used in each session, something like 1-4.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:38 PM","dateFinished":"Jun 11, 2016 3:59:38 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:76"},{"text":"sc.parallelize(1 to 100000, 100).toDF\n    .select((lit(2.7)*lognormal(lit(0.3))) cast IntegerType  as \"count\")\n    .select(when($\"count\" < lit(4),$\"count\") otherwise lit(4) as \"count\" )\n    .groupBy($\"count\").count().toZeppelin()","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"count","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"count","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228155_-1253529805","id":"20160611-153348_768165144","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:77"},{"text":"%md\n# 3. Create a Random Hash Generator\n\nWe will need lots of unique session IDs and cross device keys. We will create pseudo random hashes for that. In order to create predictable results, we use an explicit seed for random number generation. Otherwise we might get different results everytime when the RDD is evaluated. And this might happen any time out of our control, even multiple times within a single action. And that would be really bad!","dateUpdated":"Jun 11, 2016 3:59:42 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228155_-1253529805","id":"20160611-153348_2027373091","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>3. Create a Random Hash Generator</h1>\n<p>We will need lots of unique session IDs and cross device keys. We will create pseudo random hashes for that. In order to create predictable results, we use an explicit seed for random number generation. Otherwise we might get different results everytime when the RDD is evaluated. And this might happen any time out of our control, even multiple times within a single action. And that would be really bad!</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:42 PM","dateFinished":"Jun 11, 2016 3:59:42 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:78"},{"text":"val random_hash = sqlContext.udf.register(\"random_hash\", (seed:Long) => new scala.util.Random(seed).alphanumeric.take(32).mkString(\"\"))","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228155_-1253529805","id":"20160611-153348_2083997135","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:79"},{"text":"%md\n## Test Hash Generator\nWe simple generate some data, and inspect the result. Hopefully we like it.","dateUpdated":"Jun 11, 2016 3:59:45 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228156_-1255453549","id":"20160611-153348_281565375","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Test Hash Generator</h2>\n<p>We simple generate some data, and inspect the result. Hopefully we like it.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:45 PM","dateFinished":"Jun 11, 2016 3:59:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"text":"val hashes = sc.parallelize(1 to 10000000, 100).toDF\n    .select(random_hash($\"_1\") as \"hash\")\n    .cache()\n    \nhashes.toZeppelin(10)","dateUpdated":"Jun 11, 2016 3:40:49 PM","config":{"enabled":true,"graph":{"mode":"table","height":294,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228156_-1255453549","id":"20160611-153348_1362783284","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:81"},{"text":"%md\n## Check Quality of Randomness\n\nSince the hash values will serve us as unique IDs, they better should be unique. Let's check that.\n\n1. Group hashes by hash value\n2. Count number of entries per group\n3. Count number of results with a count higher than 1\n","dateUpdated":"Jun 11, 2016 3:42:23 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228156_-1255453549","id":"20160611-153348_1385558933","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Check Quality of Randomness</h2>\n<p>Since the hash values will serve us as unique IDs, they better should be unique. Let's check that.</p>\n<ol>\n<li>Group hashes by hash value</li>\n<li>Count number of entries per group</li>\n<li>Count number of results with a count higher than 1</li>\n</ol>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:42:21 PM","dateFinished":"Jun 11, 2016 3:42:21 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:82"},{"text":"// Check that hash values are unique. They should be.","dateUpdated":"Jun 11, 2016 3:42:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228156_-1255453549","id":"20160611-153348_1923802510","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:83"},{"text":"%md\n# 4. Create Random Name Generator\n\nJust for convenience, we also want to have user names. This will help us checking the results. We create random user names by using some name lists from US Census 1990 and US Census 2000. We then combine random first names with random last names for generating names.","dateUpdated":"Jun 11, 2016 3:59:49 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228157_-1255838298","id":"20160611-153348_834996338","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>4. Create Random Name Generator</h1>\n<p>Just for convenience, we also want to have user names. This will help us checking the results. We create random user names by using some name lists from US Census 1990 and US Census 2000. We then combine random first names with random last names for generating names.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:49 PM","dateFinished":"Jun 11, 2016 3:59:49 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:84"},{"text":"%md\n## Load Names\nNow we load names from previously registered Hive tables. We store the results in broadcast variables for later processing.","dateUpdated":"Jun 11, 2016 3:59:51 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228157_-1255838298","id":"20160611-153348_2125303268","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Load Names</h2>\n<p>Now we load names from previously registered Hive tables. We store the results in broadcast variables for later processing.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:51 PM","dateFinished":"Jun 11, 2016 3:59:51 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85"},{"text":"// Load names from Hive tables - or from file data.\nval lastNames = ...\nval firstNamesMale = ...\nval firstNamesFemale = ...\n\n// Extract only the names from all data sets and put the result into broadcast variables which will simply hold an Array of names.\nval lastNamesBc = ...\nval firstNamesMaleBc = ...\nval firstNamesFemaleBc = ...\n","dateUpdated":"Jun 11, 2016 3:43:46 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228157_-1255838298","id":"20160611-153348_876524725","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:86"},{"text":"%md\n## Create Random Names\nNow we create random names from first names and last names. We could use a UNION of male and female first names. But since we have more female first names than male ones, this would result in skewed data with a significant overrepresentation of women. Not important, but we do care :) So we first roll a dice for selecting the gender, and then we randomly select a name from the appropriate list.","dateUpdated":"Jun 11, 2016 3:59:57 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228157_-1255838298","id":"20160611-153348_13234655","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Create Random Names</h2>\n<p>Now we create random names from first names and last names. We could use a UNION of male and female first names. But since we have more female first names than male ones, this would result in skewed data with a significant overrepresentation of women. Not important, but we do care :) So we first roll a dice for selecting the gender, and then we randomly select a name from the appropriate list.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 3:59:57 PM","dateFinished":"Jun 11, 2016 3:59:57 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:87"},{"text":"import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n\n// Create a UDF for creating a pseudo random first name. Use the given seed to initialize a Random number generator, so we get deterministic results.\nval randomFirstName = sqlContext.udf.register(\"randomFirstName\", (seed:Long) => {\n    val rnd = new scala.util.Random(100*seed)\n\n    // Roll dice if name should be male or female\n    val isMale = rnd.nextBoolean()\n    \n    // Depending on the result return either a random male or a random female name.\n    // Use the broadcast variables defined above to access the array of available names\n})\n\n// Create a UDF for creating a pseudo random last name. Use the given seed to initialize a Random number generator, so we get deterministic results.\nval randomLastName = ...\n\n// Create lets say 10,000,000 different names, each having a first_name and a last_name. The values for the first and last name should be \n// generated using the UDFs defined above.\n//\n// 1. Create a DataFrame containing numbers 1 to 10,000,000 in a single column\n// 2. Call this column 'index'\n// 3. Use this column to generate names via the two UDFs defined above\nval people = ...\n","dateUpdated":"Jun 11, 2016 3:50:11 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228158_-1254684052","id":"20160611-153348_1244030944","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:88"},{"text":"people.toZeppelin(10)","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228158_-1254684052","id":"20160611-153348_527288616","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:89"},{"text":"%md\n# 5. Create Test Data\n\n1. Create a list of users (create a list of names)\n2. For every user, randomly create number of CDKs, number of Sessions\n3. For every session, randomly create number of used CDKs and select CDKs from user CDKs\n4. Store Data","dateUpdated":"Jun 11, 2016 4:00:00 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228158_-1254684052","id":"20160611-153348_1622241590","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>5. Create Test Data</h1>\n<ol>\n<li>Create a list of users (create a list of names)</li>\n<li>For every user, randomly create number of CDKs, number of Sessions</li>\n<li>For every session, randomly create number of used CDKs and select CDKs from user CDKs</li>\n<li>Store Data</li>\n</ol>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 4:00:00 PM","dateFinished":"Jun 11, 2016 4:00:00 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90"},{"text":"val randomSessions = sqlContext.udf.register(\"randomSession\", (seed:Long) => {\n    // Create deterministic RNG using given seed\n    val rnd = new scala.util.Random(seed)\n    \n    // Locally define lognormal distribution using random number generator defined above\n    def lognormal(a:Double) : Double = scala.math.exp(a*rnd.nextGaussian()) \n    \n    // Generate CDKs for specific user. Make sure user has at least one CDK\n    val numCdks = scala.math.max((14.0*lognormal(0.3)).toInt, 1)\n    val cdks = 1 to numCdks map { i => \"cdk_\" + rnd.alphanumeric.take(32).mkString(\"\") }\n    \n    // Generate sessions for user\n    val numSessions = (50.0*lognormal(0.4)).toInt\n    1 to numSessions flatMap { i:Int => {\n        // Create random session ID\n        val session = \"sesn_\" + rnd.alphanumeric.take(32).mkString(\"\")\n        // Generate CDK entries for session\n        val usedCdks = (2.7*lognormal(0.3)).toInt\n        1 to usedCdks map { i => (session, cdks(rnd.nextInt(numCdks))) }\n    }}\n})\n\n","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228158_-1254684052","id":"20160611-153348_562020818","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:91"},{"text":"// First create a DataFrame of 200,000 users. We call this DF 'people'. Since we could have double entries, we make the user names\n// unique by selecting the entry with the highest 'index' value.\n// 1. Create a DataFrame with a single column numbers 1 to 200,000\n// 2. Call this column 'index'\n// 3. Create random names, and concatenate first name and last name into a single column 'name'\n// 4. Group result by name, aggregate index to be maximum per group\n// 5. Resulting DataFrame should have two columns 'name' and 'index'\nval people = ...\n\n// Now for every user create a bunch of sessions using the UDF defined above.\nval sessions = people.select($\"name\", explode(randomSessions($\"index\")) as \"session_data\")\n    .select($\"name\", $\"session_data._1\" as \"session\", $\"session_data._2\" as \"cdk\")\n    .cache()\n    \n// Let's have a look at the result    \nsessions.toZeppelin(12)","dateUpdated":"Jun 11, 2016 3:55:33 PM","config":{"enabled":true,"graph":{"mode":"table","height":294,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228158_-1254684052","id":"20160611-153348_1250808620","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:92"},{"text":"%md\n# Check Uniqueness of Session IDs\n\nOf course every session key should be unique. We test this - but our data might have the same session key multiple times per user with different CDKs.\n\nIdea is to perform the following steps:\n1. Select only name and session from the data\n2. Make these tuples distinct\n3. Count the number of occurances of each session. Eaach session should belong to exactly one user, so all sessions should be distinct.\n4. Filter the result, such that only sessions with a count of higher than 1 are left. These are illegal sessions with multiple users\n5. Count the number of such illegal sessions.\n","dateUpdated":"Jun 11, 2016 4:00:04 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228159_-1255068800","id":"20160611-153348_623055857","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Check Uniqueness of Session IDs</h1>\n<p>Of course every session key should be unique. We test this - but our data might have the same session key multiple times per user with different CDKs.</p>\n<p>Idea is to perform the following steps:</p>\n<ol>\n<li>Select only name and session from the data</li>\n<li>Make these tuples distinct</li>\n<li>Count the number of occurances of each session. Eaach session should belong to exactly one user, so all sessions should be distinct.</li>\n<li>Filter the result, such that only sessions with a count of higher than 1 are left. These are illegal sessions with multiple users</li>\n<li>Count the number of such illegal sessions.</li>\n</ol>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 4:00:04 PM","dateFinished":"Jun 11, 2016 4:00:04 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:93"},{"text":"// As described above, check that different users always have different sessions\n// 1. Select only the columns 'name' and 'session' from the data\n// 2. Make these tuples distinct\n// 3. Count the number of occurances of each session. Eaach session should belong to exactly one user, so all sessions should be distinct.\n// 4. Filter the result, such that only sessions with a count of higher than 1 are left. These are illegal sessions with multiple users\n// 5. Count the number of such illegal sessions.\n//\n","dateUpdated":"Jun 11, 2016 3:58:04 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228159_-1255068800","id":"20160611-153348_569525827","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:94"},{"text":"sessions.count()","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228159_-1255068800","id":"20160611-153348_685959710","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95"},{"text":"%md\n# Save Result\n\nNow since we are happy with our session data, let's save it in a file. I would use Parquet, but maybe you want to use something else, like ORC or Avro. I do not recommend to use JSON, though.","dateUpdated":"Jun 11, 2016 4:00:07 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228159_-1255068800","id":"20160611-153348_1232628874","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Save Result</h1>\n<p>Now since we are happy with our session data, let's save it in a file. I would use Parquet, but maybe you want to use something else, like ORC or Avro. I do not recommend to use JSON, though.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 4:00:07 PM","dateFinished":"Jun 11, 2016 4:00:07 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:96"},{"text":"// Save Data somewhere, we need it!","dateUpdated":"Jun 11, 2016 3:58:04 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228159_-1255068800","id":"20160611-153348_18742709","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:97"},{"text":"%md\n# Cleanup Memory\n\nIn case we cached some DataFrames or RDDs, let's unpersist these RDDs to free up memory in Executors.","dateUpdated":"Jun 11, 2016 4:00:10 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228160_1187548725","id":"20160611-153348_1476097701","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Cleanup Memory</h1>\n<p>In case we cached some DataFrames or RDDs, let's unpersist these RDDs to free up memory in Executors.</p>\n"},"dateCreated":"Jun 11, 2016 3:33:48 PM","dateStarted":"Jun 11, 2016 4:00:10 PM","dateFinished":"Jun 11, 2016 4:00:10 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:98"},{"text":"sc.getPersistentRDDs.foreach { entry => entry._2.unpersist() }","dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228160_1187548725","id":"20160611-153348_1042970563","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:99"},{"dateUpdated":"Jun 11, 2016 3:33:48 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465659228160_1187548725","id":"20160611-153348_1444725527","dateCreated":"Jun 11, 2016 3:33:48 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:100"}],"name":"GraphX Test Data Generation Exercise","id":"2BP251GPQ","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}