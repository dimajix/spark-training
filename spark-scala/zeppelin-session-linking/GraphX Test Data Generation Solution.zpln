{"paragraphs":[{"text":"%md\n# 1. Load Names\nFirst we have to load first and last names. We will use some data from US Census 1990 and US Census 2000. The data is stored as CSV files and available in the dimajix S3 bucket.\n\nTechnically we have two different options\n* Create Hive tables for the names\n* Read the data directory in Spark","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>1. Load Names</h1>\n<p>First we have to load first and last names. We will use some data from US Census 1990 and US Census 2000. The data is stored as CSV files and available in the dimajix S3 bucket.</p>\n<p>Technically we have two different options<br/>* Create Hive tables for the names<br/>* Read the data directory in Spark</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529949856448_-773208179","id":"20180623-153151_1068362481","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7254"},{"text":"%md\n## 1. Option A: Load Names into Hive\nWe have some name lists from US Census 1990 and US Census 2000. We simply put them into Hive for easy access.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1. Option A: Load Names into Hive</h2>\n<p>We have some name lists from US Census 1990 and US Census 2000. We simply put them into Hive for easy access.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529949856454_-773977677","id":"20160611-072113_1665991785","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7255"},{"text":"%sql\nCREATE DATABASE IF NOT EXISTS training","user":"anonymous","dateUpdated":"2018-06-25T18:04:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":94,"optionOpen":false,"keys":[{"name":"result","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"result","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":""}]},"apps":[],"jobName":"paragraph_1529949856454_-773977677","id":"20160611-071353_321679255","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:04:29+0000","dateFinished":"2018-06-25T18:05:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7256"},{"text":"%sql\nCREATE EXTERNAL TABLE IF NOT EXISTS training.last_names(\n    name string,\n    rank int,\n    count int,\n    prop100k double,\n    cum_prop100k double,\n    pctwhite string,\n    pctblack string,\n    pctapi string,\n    pctaian string,\n    pct2prace string,\n    pcthispanic string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE\nLOCATION 's3://dimajix-training/data/names/last_all_c2000'","user":"anonymous","dateUpdated":"2018-06-25T18:05:04+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"table","height":84,"optionOpen":false}}},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":""}]},"apps":[],"jobName":"paragraph_1529949856454_-773977677","id":"20160611-071455_851596766","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:04+0000","dateFinished":"2018-06-25T18:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7257"},{"text":"%sql\nselect * from training.last_names limit 10","dateUpdated":"2018-06-25T18:04:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\trank\tcount\tprop100k\tcum_prop100k\tpctwhite\tpctblack\tpctapi\tpctaian\tpct2prace\tpcthispanic\nname\tnull\tnull\tnull\tnull\tpctwhite\tpctblack\tpctapi\tpctaian\tpct2prace\tpcthispanic\nSMITH\t1\t2376206\t880.85\t880.85\t73.35\t22.22\t0.40\t0.85\t1.63\t1.56\nJOHNSON\t2\t1857160\t688.44\t1569.3\t61.55\t33.80\t0.42\t0.91\t1.82\t1.50\nWILLIAMS\t3\t1534042\t568.66\t2137.96\t48.52\t46.72\t0.37\t0.78\t2.01\t1.60\nBROWN\t4\t1380145\t511.62\t2649.58\t60.71\t34.54\t0.41\t0.83\t1.86\t1.64\nJONES\t5\t1362755\t505.17\t3154.75\t57.69\t37.73\t0.35\t0.94\t1.85\t1.44\nMILLER\t6\t1127803\t418.07\t3572.82\t85.81\t10.41\t0.42\t0.63\t1.31\t1.43\nDAVIS\t7\t1072335\t397.51\t3970.33\t64.73\t30.77\t0.40\t0.79\t1.73\t1.58\nGARCIA\t8\t858289\t318.17\t4288.5\t6.17\t0.49\t1.43\t0.58\t0.51\t90.81\nRODRIGUEZ\t9\t804240\t298.13\t4586.62\t5.52\t0.54\t0.58\t0.24\t0.41\t92.70\n"}]},"apps":[],"jobName":"paragraph_1529949856455_-774362426","id":"20160611-090216_266483336","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7258"},{"text":"%sql\nCREATE EXTERNAL TABLE IF NOT EXISTS training.first_names_male(\n    name string,\n    prop double,\n    cum_prop double,\n    rank int\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE\nLOCATION 's3://dimajix-training/data/names/first_male_c1990'","user":"anonymous","dateUpdated":"2018-06-25T18:05:09+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"table","height":84,"optionOpen":false}}},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":""}]},"apps":[],"jobName":"paragraph_1529949856455_-774362426","id":"20160611-071409_1031345079","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:10+0000","dateFinished":"2018-06-25T18:05:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7259"},{"text":"%sql\nselect * from training.first_names_male limit 10","user":"anonymous","dateUpdated":"2018-06-25T18:05:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tprop\tcum_prop\trank\nJAMES\t3.318\t3.318\t1\nJOHN\t3.271\t6.589\t2\nROBERT\t3.143\t9.732\t3\nMICHAEL\t2.629\t12.361\t4\nWILLIAM\t2.451\t14.812\t5\nDAVID\t2.363\t17.176\t6\nRICHARD\t1.703\t18.878\t7\nCHARLES\t1.523\t20.401\t8\nJOSEPH\t1.404\t21.805\t9\nTHOMAS\t1.38\t23.185\t10\n"}]},"apps":[],"jobName":"paragraph_1529949856455_-774362426","id":"20160611-090237_874224002","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:13+0000","dateFinished":"2018-06-25T18:05:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7260"},{"text":"%sql\nCREATE EXTERNAL TABLE IF NOT EXISTS training.first_names_female(\n    name string,\n    prop double,\n    cum_prop double,\n    rank int\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE\nLOCATION 's3://dimajix-training/data/names/first_female_c1990'","user":"anonymous","dateUpdated":"2018-06-25T18:05:16+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"table","height":84,"optionOpen":false}}},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":""}]},"apps":[],"jobName":"paragraph_1529949856456_-776286171","id":"20160611-071425_54663654","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:16+0000","dateFinished":"2018-06-25T18:05:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7261"},{"text":"%sql\nselect * from training.first_names_female limit 10","user":"anonymous","dateUpdated":"2018-06-25T18:05:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tprop\tcum_prop\trank\nMARY\t2.629\t2.629\t1\nPATRICIA\t1.073\t3.702\t2\nLINDA\t1.035\t4.736\t3\nBARBARA\t0.98\t5.716\t4\nELIZABETH\t0.937\t6.653\t5\nJENNIFER\t0.932\t7.586\t6\nMARIA\t0.828\t8.414\t7\nSUSAN\t0.794\t9.209\t8\nMARGARET\t0.768\t9.976\t9\nDOROTHY\t0.727\t10.703\t10\n"}]},"apps":[],"jobName":"paragraph_1529949856456_-776286171","id":"20160611-090136_8285786","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:28+0000","dateFinished":"2018-06-25T18:05:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7262"},{"text":"%md\n## 1. Option B: Load Data from Files\n\nProbably the more realistic way to load data in this specific case is to read it directly as a CSV file. Moreover we specifically added a dependency at the beginning, so we can use spark-csv to load CSV data without much hassle.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1. Option B: Load Data from Files</h2>\n<p>Probably the more realistic way to load data in this specific case is to read it directly as a CSV file. Moreover we specifically added a dependency at the beginning, so we can use spark-csv to load CSV data without much hassle.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529949856456_-776286171","id":"20160611-090328_1681519857","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7263"},{"text":"import org.apache.spark.sql.types._\n\nval lastNames = spark.read\n    .option(\"header\", \"true\")  // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .csv(\"s3://dimajix-training/data/names/last_all_c2000\")","user":"anonymous","dateUpdated":"2018-06-25T18:05:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\nlastNames: org.apache.spark.sql.DataFrame = [name: string, rank: int ... 9 more fields]\n"}]},"apps":[],"jobName":"paragraph_1529949856456_-776286171","id":"20160611-090346_1660255795","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:29+0000","dateFinished":"2018-06-25T18:05:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7264"},{"text":"z.show(lastNames.limit(10))","dateUpdated":"2018-06-25T18:04:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\trank\tcount\tprop100k\tcum_prop100k\tpctwhite\tpctblack\tpctapi\tpctaian\tpct2prace\tpcthispanic\nSMITH\t1\t2376206\t880.85\t880.85\t73.35\t22.22\t0.40\t0.85\t1.63\t1.56\nJOHNSON\t2\t1857160\t688.44\t1569.3\t61.55\t33.80\t0.42\t0.91\t1.82\t1.50\nWILLIAMS\t3\t1534042\t568.66\t2137.96\t48.52\t46.72\t0.37\t0.78\t2.01\t1.60\nBROWN\t4\t1380145\t511.62\t2649.58\t60.71\t34.54\t0.41\t0.83\t1.86\t1.64\nJONES\t5\t1362755\t505.17\t3154.75\t57.69\t37.73\t0.35\t0.94\t1.85\t1.44\nMILLER\t6\t1127803\t418.07\t3572.82\t85.81\t10.41\t0.42\t0.63\t1.31\t1.43\nDAVIS\t7\t1072335\t397.51\t3970.33\t64.73\t30.77\t0.40\t0.79\t1.73\t1.58\nGARCIA\t8\t858289\t318.17\t4288.5\t6.17\t0.49\t1.43\t0.58\t0.51\t90.81\nRODRIGUEZ\t9\t804240\t298.13\t4586.62\t5.52\t0.54\t0.58\t0.24\t0.41\t92.70\nWILSON\t10\t783051\t290.27\t4876.9\t69.72\t25.32\t0.46\t1.03\t1.74\t1.73\n"}]},"apps":[],"jobName":"paragraph_1529949856457_-776670919","id":"20160611-090416_1548897248","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7265"},{"text":"val firstNamesSchema = StructType(Array(\n        StructField(\"name\", StringType, true),\n        StructField(\"prop\", DoubleType, true),\n        StructField(\"cum_pro\", DoubleType, true),\n        StructField(\"rank\", IntegerType, true)\n    ))\nval firstNamesMale = spark.read\n    .option(\"header\", \"false\")\n    .schema(firstNamesSchema)\n    .csv(\"s3://dimajix-training/data/names/first_male_c1990\")\nval firstNamesFemale = spark.read\n    .option(\"header\", \"false\")\n    .schema(firstNamesSchema)\n    .csv(\"s3://dimajix-training/data/names/first_female_c1990\")    ","user":"anonymous","dateUpdated":"2018-06-25T18:05:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"firstNamesSchema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(prop,DoubleType,true), StructField(cum_pro,DoubleType,true), StructField(rank,IntegerType,true))\nfirstNamesMale: org.apache.spark.sql.DataFrame = [name: string, prop: double ... 2 more fields]\nfirstNamesFemale: org.apache.spark.sql.DataFrame = [name: string, prop: double ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1529949856457_-776670919","id":"20160611-090402_2075590603","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:34+0000","dateFinished":"2018-06-25T18:05:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7266"},{"text":"z.show(firstNamesMale.limit(10))","user":"anonymous","dateUpdated":"2018-06-25T18:05:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tprop\tcum_pro\trank\nJAMES\t3.318\t3.318\t1\nJOHN\t3.271\t6.589\t2\nROBERT\t3.143\t9.732\t3\nMICHAEL\t2.629\t12.361\t4\nWILLIAM\t2.451\t14.812\t5\nDAVID\t2.363\t17.176\t6\nRICHARD\t1.703\t18.878\t7\nCHARLES\t1.523\t20.401\t8\nJOSEPH\t1.404\t21.805\t9\nTHOMAS\t1.38\t23.185\t10\n"}]},"apps":[],"jobName":"paragraph_1529949856457_-776670919","id":"20160611-090430_130374012","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:39+0000","dateFinished":"2018-06-25T18:05:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7267"},{"text":"z.show(firstNamesFemale.limit(10))","user":"anonymous","dateUpdated":"2018-06-25T18:05:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tprop\tcum_pro\trank\nMARY\t2.629\t2.629\t1\nPATRICIA\t1.073\t3.702\t2\nLINDA\t1.035\t4.736\t3\nBARBARA\t0.98\t5.716\t4\nELIZABETH\t0.937\t6.653\t5\nJENNIFER\t0.932\t7.586\t6\nMARIA\t0.828\t8.414\t7\nSUSAN\t0.794\t9.209\t8\nMARGARET\t0.768\t9.976\t9\nDOROTHY\t0.727\t10.703\t10\n"}]},"apps":[],"jobName":"paragraph_1529949856458_-775516673","id":"20180624-162452_1813773519","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:42+0000","dateFinished":"2018-06-25T18:05:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7268"},{"text":"%md\n# 2. Create Distribution for various Parameters\n\nNow we need to have some nice random distributions for three different use cases:\n\n* Number of Linking Keys per user\n* Number of Sessions per user\n* Number of Linking Keys used per session\n\nWe will use a lognormal distribution for that","dateUpdated":"2018-06-25T18:04:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>2. Create Distribution for various Parameters</h1>\n<p>Now we need to have some nice random distributions for three different use cases:</p>\n<ul>\n  <li>Number of Linking Keys per user</li>\n  <li>Number of Sessions per user</li>\n  <li>Number of Linking Keys used per session</li>\n</ul>\n<p>We will use a lognormal distribution for that</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529949856458_-775516673","id":"20160610-202154_549406477","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7269"},{"text":"%md\n## Lognormal Distribution\n\nThe following definition provides us with a lognormal distribution.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Lognormal Distribution</h2>\n<p>The following definition provides us with a lognormal distribution.</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856458_-775516673","id":"20160611-071627_398009319","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7270"},{"text":"val lognormal = spark.udf.register(\"lognormal\", (a:Double) => scala.math.exp(a*scala.util.Random.nextGaussian()) )","user":"anonymous","dateUpdated":"2018-06-25T18:05:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lognormal: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(DoubleType)))\n"}]},"apps":[],"jobName":"paragraph_1529949856459_-775901422","id":"20160611-071706_949405453","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:05:57+0000","dateFinished":"2018-06-25T18:05:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7271"},{"text":"%md\n## Number of Linking Keys per User\n\nFirst let's create a random distribution which will be used for sampling the number of linking keys (or cross device key, CDKs) per user. A linking key could be an e-mail address, a user-id of a web-site, a phone number etc. Realistically you'll probably find hashed data for these values, but nontheless that would be good enough for our use case. We assume that a user has a couple of these IDs, normally maybe 10 - 20.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Number of Linking Keys per User</h2>\n<p>First let's create a random distribution which will be used for sampling the number of linking keys (or cross device key, CDKs) per user. A linking key could be an e-mail address, a user-id of a web-site, a phone number etc. Realistically you'll probably find hashed data for these values, but nontheless that would be good enough for our use case. We assume that a user has a couple of these IDs, normally maybe 10 - 20.</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856459_-775901422","id":"20160611-071717_1832632178","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7272"},{"text":"val dist = sc.parallelize(1 to 100000, 100).toDF\n    .select((lit(14.0)*lognormal(lit(0.3))) cast IntegerType  as \"value\")\n    .select(when($\"value\" < lit(200),$\"value\") otherwise lit(200) as \"value\" )\n    .groupBy($\"value\").count()\n    \nz.show(dist)","user":"anonymous","dateUpdated":"2018-06-25T18:06:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"count","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"count","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}}},{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dist: org.apache.spark.sql.DataFrame = [value: int, count: bigint]\n"},{"type":"TABLE","data":"value\tcount\n31\t113\n34\t48\n28\t277\n26\t529\n27\t386\n12\t9933\n22\t1571\n47\t1\n52\t1\n13\t10033\n16\t7024\n6\t832\n3\t2\n20\t2884\n40\t3\n5\t193\n19\t3769\n41\t4\n15\t7956\n43\t4\n37\t18\n17\t5733\n9\t5987\n35\t35\n4\t28\n8\t3931\n23\t1321\n39\t10\n7\t2077\n10\t7975\n50\t1\n45\t1\n38\t7\n25\t703\n24\t998\n29\t196\n21\t2183\n32\t79\n11\t9158\n33\t58\n14\t9095\n42\t2\n30\t139\n18\t4685\n36\t17\n"}]},"apps":[],"jobName":"paragraph_1529949856459_-775901422","id":"20160609-184610_1177338345","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:06:01+0000","dateFinished":"2018-06-25T18:06:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7273"},{"text":"%md\n# Create Distribution for Number of Sessions\n\nEach user will create multiple web browser sessions. Some users only have a few sessions, while other have more sessions. We assume that a typical user has around 40 sessions, but there are also more extreme users with more sessions (for example those users who always clear their cookies will generate much more sessions - but this won't be modelled very good here.)","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Create Distribution for Number of Sessions</h1>\n<p>Each user will create multiple web browser sessions. Some users only have a few sessions, while other have more sessions. We assume that a typical user has around 40 sessions, but there are also more extreme users with more sessions (for example those users who always clear their cookies will generate much more sessions - but this won't be modelled very good here.)</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856459_-775901422","id":"20160610-202215_1185091312","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7274"},{"text":"val dist = sc.parallelize(1 to 100000, 100).toDF\n    .select((lit(50.0)*lognormal(lit(0.4))) cast IntegerType  as \"value\")\n    .select(when($\"value\" < lit(2000),$\"value\") otherwise lit(2000) as \"value\" )\n    .groupBy($\"value\").count()\n    \nz.show(dist)","user":"anonymous","dateUpdated":"2018-06-25T18:06:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"count","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"count","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}}},{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dist: org.apache.spark.sql.DataFrame = [value: int, count: bigint]\n"},{"type":"TABLE","data":"value\tcount\n148\t15\n31\t1635\n85\t480\n137\t31\n65\t1191\n53\t1769\n133\t43\n78\t627\n322\t1\n108\t128\n155\t6\n34\t1853\n211\t2\n193\t1\n115\t117\n126\t49\n101\t198\n81\t570\n28\t1287\n183\t3\n76\t721\n27\t1154\n26\t1054\n44\t2227\n159\t7\n103\t213\n236\t1\n12\t18\n91\t345\n222\t1\n128\t46\n22\t575\n209\t2\n122\t64\n93\t337\n157\t11\n232\t2\n190\t1\n111\t102\n47\t2105\n140\t14\n177\t4\n132\t32\n152\t14\n185\t1\n146\t11\n206\t1\n52\t1935\n212\t1\n182\t3\n13\t35\n16\t134\n86\t462\n6\t1\n168\t4\n142\t26\n40\t2079\n20\t406\n164\t6\n169\t5\n139\t33\n94\t300\n57\t1578\n54\t1785\n120\t102\n96\t264\n48\t2092\n163\t5\n191\t4\n19\t314\n92\t315\n64\t1216\n117\t72\n41\t2226\n154\t10\n43\t2182\n15\t113\n112\t103\n165\t9\n189\t2\n179\t1\n37\t2005\n61\t1395\n127\t56\n88\t428\n197\t2\n107\t145\n202\t2\n17\t189\n9\t1\n72\t886\n175\t6\n35\t1986\n196\t1\n217\t1\n114\t96\n173\t6\n59\t1553\n55\t1690\n100\t212\n161\t3\n39\t2108\n23\t714\n49\t2070\n176\t2\n162\t8\n130\t47\n84\t505\n136\t33\n87\t457\n171\t7\n51\t1908\n194\t2\n69\t1078\n129\t44\n166\t7\n97\t259\n234\t1\n63\t1306\n77\t755\n102\t214\n10\t5\n50\t1996\n267\t1\n45\t2097\n216\t2\n38\t2094\n261\t1\n82\t585\n181\t4\n167\t5\n80\t594\n73\t875\n25\t943\n113\t111\n24\t822\n160\t8\n70\t927\n62\t1382\n121\t79\n125\t55\n156\t8\n143\t34\n95\t311\n29\t1417\n226\t1\n21\t472\n98\t248\n195\t3\n32\t1678\n60\t1490\n90\t344\n75\t731\n203\t1\n221\t2\n141\t22\n151\t11\n200\t3\n145\t21\n56\t1658\n109\t141\n170\t6\n105\t163\n58\t1633\n188\t3\n33\t1812\n11\t18\n204\t1\n83\t502\n110\t146\n150\t12\n71\t938\n68\t1088\n106\t166\n116\t79\n147\t23\n14\t64\n198\t2\n123\t65\n158\t15\n199\t1\n135\t30\n42\t2183\n119\t77\n79\t612\n149\t12\n131\t58\n118\t75\n124\t52\n30\t1536\n99\t219\n184\t4\n66\t1204\n46\t2097\n67\t1083\n186\t1\n174\t4\n172\t4\n153\t8\n144\t25\n18\t253\n74\t789\n138\t29\n104\t199\n180\t2\n134\t35\n36\t1981\n89\t429\n"}]},"apps":[],"jobName":"paragraph_1529949856460_-777825166","id":"20160610-200207_1931802005","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:06:05+0000","dateFinished":"2018-06-25T18:06:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7275"},{"text":"%md\n# Create Distribution of Number of Linking Keys per Session\n\nNow each user has around 40 linking keys - it would be too easy if he used all those keys in all his sessions. Therefore we assume that only very few of these keys will actually be used in each session, something like 1-4.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Create Distribution of Number of Linking Keys per Session</h1>\n<p>Now each user has around 40 linking keys - it would be too easy if he used all those keys in all his sessions. Therefore we assume that only very few of these keys will actually be used in each session, something like 1-4.</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856460_-777825166","id":"20160610-202307_2101754016","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7276"},{"text":"val dist = sc.parallelize(1 to 100000, 100).toDF\n    .select((lit(2.7)*lognormal(lit(0.3))) cast IntegerType  as \"value\")\n    .select(when($\"value\" < lit(4),$\"value\") otherwise lit(4) as \"value\" )\n    .groupBy($\"value\").count()\n    \nz.show(dist)","user":"anonymous","dateUpdated":"2018-06-25T18:06:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"count","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"count","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}}},{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dist: org.apache.spark.sql.DataFrame = [value: int, count: bigint]\n"},{"type":"TABLE","data":"value\tcount\n1\t14263\n6\t305\n3\t24186\n5\t1421\n4\t16284\n8\t11\n7\t50\n10\t1\n2\t43441\n0\t38\n"}]},"apps":[],"jobName":"paragraph_1529949856461_-778209915","id":"20160610-203634_1330406317","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:06:10+0000","dateFinished":"2018-06-25T18:06:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7277"},{"text":"%md\n# 3. Create a Random Hash Generator\n\nWe will need lots of unique session IDs and cross device keys. We will create pseudo random hashes for that. In order to create predictable results, we use an explicit seed for random number generation. Otherwise we might get different results everytime when the RDD is evaluated. And this might happen any time out of our control, even multiple times within a single action. And that would be really bad!","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>3. Create a Random Hash Generator</h1>\n<p>We will need lots of unique session IDs and cross device keys. We will create pseudo random hashes for that. In order to create predictable results, we use an explicit seed for random number generation. Otherwise we might get different results everytime when the RDD is evaluated. And this might happen any time out of our control, even multiple times within a single action. And that would be really bad!</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856461_-778209915","id":"20160610-205224_57087976","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7278"},{"text":"val random_hash = spark.udf.register(\"random_hash\", (seed:Long) => new scala.util.Random(seed).alphanumeric.take(32).mkString(\"\"))","user":"anonymous","dateUpdated":"2018-06-25T18:06:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"random_hash: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(LongType)))\n"}]},"apps":[],"jobName":"paragraph_1529949856461_-778209915","id":"20160610-203653_372271661","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:06:20+0000","dateFinished":"2018-06-25T18:06:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7279"},{"text":"%md\n## Test Hash Generator\nWe simple generate some data, and inspect the result. Hopefully we like it.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Test Hash Generator</h2>\n<p>We simple generate some data, and inspect the result. Hopefully we like it.</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856462_-777055668","id":"20160611-071811_1123157937","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7280"},{"text":"val hashes = sc.parallelize(1 to 10000000, 100).toDF\n    .select(random_hash($\"value\") as \"hash\")\n    .cache()\n    \nz.show(hashes.limit(10))","user":"anonymous","dateUpdated":"2018-06-25T18:06:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"hashes: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [hash: string]\n"},{"type":"TABLE","data":"hash\nNAvZuGESoIJ7hbqOIsAV4iWta9qh1yp4\noC8rHI6rDAiYSgMKHP6b4NlWG8UDdo5A\n0C27oI94jXZkXyB0ID8ZPq1zinxNmren\nwAlGZFP2OPwiuYISHIEQ51IHv9B9R7Wz\n78gU6HSFwnptxq6AI7FPQRYkNocJZC7c\nZAsmTHHeKfCIkvf4HgBUP8nN2oIrB0Oi\nkAp00GMrs24WoDTmHUDRkY3pUUj0I5xL\ng8W9kEdpXuSSAnYEIXJJOjM8hpxnxLpX\nr6SPJFf25HKeG6NuJMLIjCaa9WOw7POA\nJ8dieGVQTAh53AvqIvHMjtpCnV4SiEgG\n"}]},"apps":[],"jobName":"paragraph_1529949856462_-777055668","id":"20160610-204902_167725991","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:06:23+0000","dateFinished":"2018-06-25T18:06:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7281"},{"text":"%md\n## Check Quality of Randomness\n\nSince the hash values will serve us as unique IDs, they better should be unique. Let's check that.\n\n1. Group hashes by hash value\n2. Count number of entries per group\n3. Count number of results with a count higher than 1","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Check Quality of Randomness</h2>\n<p>Since the hash values will serve us as unique IDs, they better should be unique. Let's check that.</p>\n<ol>\n<li>Group hashes by hash value</li>\n<li>Count number of entries per group</li>\n<li>Count number of results with a count higher than 1</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1529949856462_-777055668","id":"20160610-205247_419442079","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7282"},{"text":"hashes\n    .groupBy(\"hash\")\n    .count()\n    .filter(col(\"count\") > 1)\n    .count()","user":"anonymous","dateUpdated":"2018-06-25T18:06:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res11: Long = 0\n"}]},"apps":[],"jobName":"paragraph_1529949856462_-777055668","id":"20160610-205029_467377335","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:06:28+0000","dateFinished":"2018-06-25T18:07:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7283"},{"text":"%md\n# 4. Create Random Name Generator\n\nJust for convenience, we also want to have user names. This will help us checking the results. We create random user names by using some name lists from US Census 1990 and US Census 2000. We then combine random first names with random last names for generating names.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>4. Create Random Name Generator</h1>\n<p>Just for convenience, we also want to have user names. This will help us checking the results. We create random user names by using some name lists from US Census 1990 and US Census 2000. We then combine random first names with random last names for generating names.</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856463_-777440417","id":"20160610-210156_2120151249","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7284"},{"text":"%md\n## Load Names\nNow we load names from previously registered Hive tables. We store the results in broadcast variables for later processing.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Load Names</h2>\n<p>Now we load names from previously registered Hive tables. We store the results in broadcast variables for later processing.</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856463_-777440417","id":"20160611-070605_859155850","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7285"},{"text":"// Load names from Hive tables - or comment out if using file data.\nval lastNames = spark.read.table(\"training.last_names\")\nval firstNamesMale = spark.read.table(\"training.first_names_male\")\nval firstNamesFemale = spark.read.table(\"training.first_names_female\")\n\n// Extract only the names from all data sets and put the result into broadcast variables which will simply hold an Array of names.\nval lastNamesBc = sc.broadcast(lastNames.select(\"name\").map(_.getString(0)).collect())\nval firstNamesMaleBc = sc.broadcast(firstNamesMale.select(\"name\").map(_.getString(0)).collect())\nval firstNamesFemaleBc = sc.broadcast(firstNamesFemale.select(\"name\").map(_.getString(0)).collect())\n","user":"anonymous","dateUpdated":"2018-06-25T18:06:33+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lastNames: org.apache.spark.sql.DataFrame = [name: string, rank: int ... 9 more fields]\nfirstNamesMale: org.apache.spark.sql.DataFrame = [name: string, prop: double ... 2 more fields]\nfirstNamesFemale: org.apache.spark.sql.DataFrame = [name: string, prop: double ... 2 more fields]\nlastNamesBc: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(36)\nfirstNamesMaleBc: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(39)\nfirstNamesFemaleBc: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(42)\n"}]},"apps":[],"jobName":"paragraph_1529949856463_-777440417","id":"20160611-065436_1713338353","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:06:33+0000","dateFinished":"2018-06-25T18:07:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7286"},{"text":"%md\n## Create Random Names\nNow we create random names from first names and last names. We could use a UNION of male and female first names. But since we have more female first names than male ones, this would result in skewed data with a significant overrepresentation of women. Not important, but we do care :) So we first roll a dice for selecting the gender, and then we randomly select a name from the appropriate list.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Create Random Names</h2>\n<p>Now we create random names from first names and last names. We could use a UNION of male and female first names. But since we have more female first names than male ones, this would result in skewed data with a significant overrepresentation of women. Not important, but we do care :) So we first roll a dice for selecting the gender, and then we randomly select a name from the appropriate list.</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856463_-777440417","id":"20160611-070633_493197002","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7287"},{"text":"// Create a UDF for creating a pseudo random first name. Use the given seed to initialize a Random number generator, so we get deterministic results.\nval randomFirstName = spark.udf.register(\"randomFirstName\", (seed:Long) => {\n    val rnd = new scala.util.Random(100*seed)\n    val firstNamesMale = firstNamesMaleBc.value\n    val firstNamesFemale = firstNamesFemaleBc.value\n    val numFirstNamesMale = firstNamesMale.length\n    val numFirstNamesFemale = firstNamesFemale.length\n\n    // Randomly select female or male first name\n    if (rnd.nextBoolean()) \n        firstNamesMale(rnd.nextInt(numFirstNamesMale))\n    else\n        firstNamesFemale(rnd.nextInt(numFirstNamesFemale))\n})\n\n// Create a UDF for creating a pseudo random last name. Use the given seed to initialize a Random number generator, so we get deterministic results.\nval randomLastName = spark.udf.register(\"randomLastName\", (seed:Long) => {\n    val rnd = new scala.util.Random(100*seed + 1)\n    val lastNames = lastNamesBc.value\n    val numLastNames = lastNames.length\n\n    lastNames(rnd.nextInt(numLastNames))\n})\n\n// Create lets say 10,000,000 different names, each having a first_name and a last_name. The values for the first and last name should be \n// generated using the UDFs defined above.\nval people = sc.parallelize(1 to 10000000, 100).toDF\n    .select($\"value\" as \"index\")\n    .select($\"index\", randomFirstName($\"index\").as(\"first_name\"), randomLastName($\"index\").as(\"last_name\"))\n","user":"anonymous","dateUpdated":"2018-06-25T18:08:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"randomFirstName: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(LongType)))\nrandomLastName: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(LongType)))\npeople: org.apache.spark.sql.DataFrame = [index: int, first_name: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1529949856464_-767052197","id":"20160611-065408_339835201","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:08:06+0000","dateFinished":"2018-06-25T18:08:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7288"},{"text":"z.show(people.limit(10))","user":"anonymous","dateUpdated":"2018-06-25T18:07:07+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"index\tfirst_name\tlast_name\n1\tJESUS\tAWA\n2\tDEXTER\tGOODWATER\n3\tDUNCAN\tREDIC\n4\tSANTOS\tZADA\n5\tELTON\tZIMBARDI\n6\tCAMERON\tVASINA\n7\tSCOT\tNAVON\n8\tABRAM\tSCHOLL\n9\tWESLEY\tMECKER\n10\tRON\tESKRIDGE\n"}]},"apps":[],"jobName":"paragraph_1529949856464_-767052197","id":"20160611-070228_2114369793","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:07:17+0000","dateFinished":"2018-06-25T18:07:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7289"},{"text":"%md\n# 5. Create Test Data\n\n1. Create a list of users (create a list of names)\n2. For every user, randomly create number of Linkiing Keys, number of Sessions\n3. For every session, randomly create number of used Linking Keys and select Linking Keys from users set of Linking Keys\n4. Store Data","dateUpdated":"2018-06-25T18:04:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>5. Create Test Data</h1>\n<ol>\n  <li>Create a list of users (create a list of names)</li>\n  <li>For every user, randomly create number of Linkiing Keys, number of Sessions</li>\n  <li>For every session, randomly create number of used Linking Keys and select Linking Keys from users set of Linking Keys</li>\n  <li>Store Data</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1529949856464_-767052197","id":"20160611-070543_251913145","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7290"},{"text":"val randomSessions = spark.udf.register(\"randomSession\", (seed:Long) => {\n    // Create deterministic RNG using given seed\n    val rnd = new scala.util.Random(seed)\n    \n    // Locally define lognormal distribution using random number generator defined above\n    def lognormal(a:Double) : Double = scala.math.exp(a*rnd.nextGaussian()) \n    \n    // Generate linking keys for specific user. Make sure user has at least one linking key\n    val numKeys = scala.math.max((14.0*lognormal(0.3)).toInt, 1)\n    val keys = 1 to numKeys map { i => \"cdk_\" + rnd.alphanumeric.take(32).mkString(\"\") }\n    \n    // Generate sessions for user\n    val numSessions = (50.0*lognormal(0.4)).toInt\n    1 to numSessions flatMap { i:Int => {\n        // Create random session ID\n        val session = \"sesn_\" + rnd.alphanumeric.take(32).mkString(\"\")\n        // Generate CDK entries for session\n        val usedKeys = (2.7*lognormal(0.3)).toInt\n        1 to usedKeys map { i => (session, keys(rnd.nextInt(numKeys))) }\n    }}\n})\n\n","user":"anonymous","dateUpdated":"2018-06-25T18:07:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"randomSessions: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StructType(StructField(_1,StringType,true), StructField(_2,StringType,true)),true),Some(List(LongType)))\n"}]},"apps":[],"jobName":"paragraph_1529949856465_-767436946","id":"20160611-073542_554602526","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:07:21+0000","dateFinished":"2018-06-25T18:07:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7291"},{"text":"// First create a DataFrame of 100,000 users. We call this DF 'people'. Since we could have double entries, we make the user names\n// unique by selecting the entry with the highest 'index' value.\nval people = sc.parallelize(1 to 100000, 100).toDF\n    .select($\"value\" as \"index\")\n    .select($\"index\", concat(randomFirstName($\"index\"), lit(\" \"), randomLastName($\"index\")).as(\"name\"))\n    .groupBy($\"name\").max(\"index\")\n    .select($\"name\",$\"max(index)\".as(\"index\"))\n\n// Now for every user create a bunch of sessions using the UDF defined above.\nval sessions = people.select($\"name\", explode(randomSessions($\"index\")) as \"session_data\")\n    .select($\"name\", $\"session_data._1\" as \"session\", $\"session_data._2\" as \"linking_key\")\n    .cache()\n\n// Let's have a look at the result    \nz.show(sessions.limit(100))","user":"anonymous","dateUpdated":"2018-06-25T18:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"people: org.apache.spark.sql.DataFrame = [name: string, index: int]\nsessions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, session: string ... 1 more field]\n"},{"type":"TABLE","data":"name\tsession\tlinking_key\nKELLE ROCKENSTEIN\tsesn_uF1RTu4pw49p7Co9YHrp1UK3GoWRekWN\tcdk_FHDU16ldcalposc07E1uk3vRwbFPE0De\nKELLE ROCKENSTEIN\tsesn_uF1RTu4pw49p7Co9YHrp1UK3GoWRekWN\tcdk_uhXAFNdS6hNk1TXwH5M66GfRgqfNE8PC\nKELLE ROCKENSTEIN\tsesn_uF1RTu4pw49p7Co9YHrp1UK3GoWRekWN\tcdk_2ohu1cQ6yu8DuTo2x3DouBIkzoLwP1zt\nKELLE ROCKENSTEIN\tsesn_Nm1FwSE46RSZH3tJkNx5merYrpF8iJTv\tcdk_oJN5lPP1YQOE6z8jqiADFmCHrw6cxv2k\nKELLE ROCKENSTEIN\tsesn_Nm1FwSE46RSZH3tJkNx5merYrpF8iJTv\tcdk_mz5srH5z7trN7ZtFRC09ODNQYqEkugrd\nKELLE ROCKENSTEIN\tsesn_Nm1FwSE46RSZH3tJkNx5merYrpF8iJTv\tcdk_Rdp58ZiaHJExKhV6X15ayxcGoWVgDrKw\nKELLE ROCKENSTEIN\tsesn_v53Bvj4H0euVVHgSR9V4rQHepNhQmRTB\tcdk_FHDU16ldcalposc07E1uk3vRwbFPE0De\nKELLE ROCKENSTEIN\tsesn_pa9TBxfcPSA1h5Gfye0KpvroRtwOz2Hy\tcdk_P5JVmD18HldgjGJuYZek4JBh2EG1NLsq\nKELLE ROCKENSTEIN\tsesn_pa9TBxfcPSA1h5Gfye0KpvroRtwOz2Hy\tcdk_S3umj01mY9WIJ2ft55f9VX5RjhNN3Afo\nKELLE ROCKENSTEIN\tsesn_6O31KeCIIKun0cmbh7HYNAxDrjI51kYZ\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_6O31KeCIIKun0cmbh7HYNAxDrjI51kYZ\tcdk_P5JVmD18HldgjGJuYZek4JBh2EG1NLsq\nKELLE ROCKENSTEIN\tsesn_6O31KeCIIKun0cmbh7HYNAxDrjI51kYZ\tcdk_oJN5lPP1YQOE6z8jqiADFmCHrw6cxv2k\nKELLE ROCKENSTEIN\tsesn_FwEWjqYLYFthC1Q3fcWBktzcQsqsH6D7\tcdk_uhXAFNdS6hNk1TXwH5M66GfRgqfNE8PC\nKELLE ROCKENSTEIN\tsesn_FwEWjqYLYFthC1Q3fcWBktzcQsqsH6D7\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_FwEWjqYLYFthC1Q3fcWBktzcQsqsH6D7\tcdk_hAjnaUO7z4Ig60Qclj45RKBgvbTrcvnl\nKELLE ROCKENSTEIN\tsesn_u0PVniNadQcl4b8idLTG4I0Pdon140f8\tcdk_FHDU16ldcalposc07E1uk3vRwbFPE0De\nKELLE ROCKENSTEIN\tsesn_u0PVniNadQcl4b8idLTG4I0Pdon140f8\tcdk_M4qx3Dfpt5XvGIRaSozPOMwSDkQ1oJYD\nKELLE ROCKENSTEIN\tsesn_FlJqaeS03hzEspeaZDvRiPyuxHOEQfMJ\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_FlJqaeS03hzEspeaZDvRiPyuxHOEQfMJ\tcdk_P5JVmD18HldgjGJuYZek4JBh2EG1NLsq\nKELLE ROCKENSTEIN\tsesn_0HqLcSVtWhHnFqCbt9wOAbEZ9FG9MhDf\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_0HqLcSVtWhHnFqCbt9wOAbEZ9FG9MhDf\tcdk_ugyz6Rh3vVPzcxgUKNhVftNToAqRwEUW\nKELLE ROCKENSTEIN\tsesn_0HqLcSVtWhHnFqCbt9wOAbEZ9FG9MhDf\tcdk_FHDU16ldcalposc07E1uk3vRwbFPE0De\nKELLE ROCKENSTEIN\tsesn_hG0t3A4dS3mRCoe0qTQIQ41Np5Ru2Zuz\tcdk_FHDU16ldcalposc07E1uk3vRwbFPE0De\nKELLE ROCKENSTEIN\tsesn_hG0t3A4dS3mRCoe0qTQIQ41Np5Ru2Zuz\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_v7pZPYuB5gB5YuoKjbEDXHdKeKX28iyN\tcdk_M4qx3Dfpt5XvGIRaSozPOMwSDkQ1oJYD\nKELLE ROCKENSTEIN\tsesn_v7pZPYuB5gB5YuoKjbEDXHdKeKX28iyN\tcdk_Rdp58ZiaHJExKhV6X15ayxcGoWVgDrKw\nKELLE ROCKENSTEIN\tsesn_v7pZPYuB5gB5YuoKjbEDXHdKeKX28iyN\tcdk_AKwVnbn7mOoQStjmoQ9stZjYYSwv3P8q\nKELLE ROCKENSTEIN\tsesn_AEI88DTuv0U8xenhm4T8oaSlt9Np9UPg\tcdk_oJN5lPP1YQOE6z8jqiADFmCHrw6cxv2k\nKELLE ROCKENSTEIN\tsesn_AEI88DTuv0U8xenhm4T8oaSlt9Np9UPg\tcdk_AKwVnbn7mOoQStjmoQ9stZjYYSwv3P8q\nKELLE ROCKENSTEIN\tsesn_UyhQTHyfKZnx0jPX6ctRDAec0sEHxEug\tcdk_M4qx3Dfpt5XvGIRaSozPOMwSDkQ1oJYD\nKELLE ROCKENSTEIN\tsesn_UyhQTHyfKZnx0jPX6ctRDAec0sEHxEug\tcdk_vb1pzJYSXGUrZ4CMTYwKj74AMhaTSGdL\nKELLE ROCKENSTEIN\tsesn_LDam0wzzTkAzsYaZw9noM5ZqfVQBLVP6\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_LDam0wzzTkAzsYaZw9noM5ZqfVQBLVP6\tcdk_zMZAQys3S0tZaq5EHUBdBLenHsuOp0lB\nKELLE ROCKENSTEIN\tsesn_LDam0wzzTkAzsYaZw9noM5ZqfVQBLVP6\tcdk_vb1pzJYSXGUrZ4CMTYwKj74AMhaTSGdL\nKELLE ROCKENSTEIN\tsesn_wsM7yiwqNoWM6CKnWfmFH4RhRJVog9cv\tcdk_mz5srH5z7trN7ZtFRC09ODNQYqEkugrd\nKELLE ROCKENSTEIN\tsesn_wsM7yiwqNoWM6CKnWfmFH4RhRJVog9cv\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_wsM7yiwqNoWM6CKnWfmFH4RhRJVog9cv\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_yBfeZhWxSxJvMGIcQ6QWj2xB0JQRCset\tcdk_mz5srH5z7trN7ZtFRC09ODNQYqEkugrd\nKELLE ROCKENSTEIN\tsesn_yBfeZhWxSxJvMGIcQ6QWj2xB0JQRCset\tcdk_oJN5lPP1YQOE6z8jqiADFmCHrw6cxv2k\nKELLE ROCKENSTEIN\tsesn_yBfeZhWxSxJvMGIcQ6QWj2xB0JQRCset\tcdk_zMZAQys3S0tZaq5EHUBdBLenHsuOp0lB\nKELLE ROCKENSTEIN\tsesn_dwsIctlNXV4usgkKKHk8IVAvTmKAd7zD\tcdk_S3umj01mY9WIJ2ft55f9VX5RjhNN3Afo\nKELLE ROCKENSTEIN\tsesn_dwsIctlNXV4usgkKKHk8IVAvTmKAd7zD\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_pWPGvMku0k6nluwjzgTnqb3m6A6YvyB9\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_pWPGvMku0k6nluwjzgTnqb3m6A6YvyB9\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_pWPGvMku0k6nluwjzgTnqb3m6A6YvyB9\tcdk_mz5srH5z7trN7ZtFRC09ODNQYqEkugrd\nKELLE ROCKENSTEIN\tsesn_pWPGvMku0k6nluwjzgTnqb3m6A6YvyB9\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_fqkmrRRthEtpNeHU4pDWBqbjZhiyAunF\tcdk_2ohu1cQ6yu8DuTo2x3DouBIkzoLwP1zt\nKELLE ROCKENSTEIN\tsesn_fqkmrRRthEtpNeHU4pDWBqbjZhiyAunF\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_fqkmrRRthEtpNeHU4pDWBqbjZhiyAunF\tcdk_ugyz6Rh3vVPzcxgUKNhVftNToAqRwEUW\nKELLE ROCKENSTEIN\tsesn_blkO4T3oRRdMh4oZwt29GmgJSQhpATw9\tcdk_uhXAFNdS6hNk1TXwH5M66GfRgqfNE8PC\nKELLE ROCKENSTEIN\tsesn_blkO4T3oRRdMh4oZwt29GmgJSQhpATw9\tcdk_zMZAQys3S0tZaq5EHUBdBLenHsuOp0lB\nKELLE ROCKENSTEIN\tsesn_At1Z5uBsxCCSb8K41L4SLP2GX4CvlfuA\tcdk_vb1pzJYSXGUrZ4CMTYwKj74AMhaTSGdL\nKELLE ROCKENSTEIN\tsesn_At1Z5uBsxCCSb8K41L4SLP2GX4CvlfuA\tcdk_AKwVnbn7mOoQStjmoQ9stZjYYSwv3P8q\nKELLE ROCKENSTEIN\tsesn_Qnf3jonOLkuilS0SnxSwuZ7Hx1X88VbW\tcdk_P5JVmD18HldgjGJuYZek4JBh2EG1NLsq\nKELLE ROCKENSTEIN\tsesn_Qnf3jonOLkuilS0SnxSwuZ7Hx1X88VbW\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_Qnf3jonOLkuilS0SnxSwuZ7Hx1X88VbW\tcdk_2ohu1cQ6yu8DuTo2x3DouBIkzoLwP1zt\nKELLE ROCKENSTEIN\tsesn_xB6pCfXMtHkPZpU1vha56XjoXKBuyles\tcdk_hAjnaUO7z4Ig60Qclj45RKBgvbTrcvnl\nKELLE ROCKENSTEIN\tsesn_xB6pCfXMtHkPZpU1vha56XjoXKBuyles\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_xB6pCfXMtHkPZpU1vha56XjoXKBuyles\tcdk_oJN5lPP1YQOE6z8jqiADFmCHrw6cxv2k\nKELLE ROCKENSTEIN\tsesn_pl3yRxlZsBc7cASP2mDRdTuoLYYRWEGt\tcdk_PPZgndn1mX3XphUAUZNLgfRxc3yT4tkT\nKELLE ROCKENSTEIN\tsesn_pl3yRxlZsBc7cASP2mDRdTuoLYYRWEGt\tcdk_PPZgndn1mX3XphUAUZNLgfRxc3yT4tkT\nKELLE ROCKENSTEIN\tsesn_pl3yRxlZsBc7cASP2mDRdTuoLYYRWEGt\tcdk_hAjnaUO7z4Ig60Qclj45RKBgvbTrcvnl\nKELLE ROCKENSTEIN\tsesn_G3U7CxinnhrKRHZymTBNSFzdwgxVUn3V\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_G3U7CxinnhrKRHZymTBNSFzdwgxVUn3V\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_ajk70hg0OygNOn40N25P1eqS314okoyy\tcdk_S3umj01mY9WIJ2ft55f9VX5RjhNN3Afo\nKELLE ROCKENSTEIN\tsesn_ajk70hg0OygNOn40N25P1eqS314okoyy\tcdk_mz5srH5z7trN7ZtFRC09ODNQYqEkugrd\nKELLE ROCKENSTEIN\tsesn_6Swu5G9RCf5X8MI4xcShoGSvqkSdY4rh\tcdk_Rdp58ZiaHJExKhV6X15ayxcGoWVgDrKw\nKELLE ROCKENSTEIN\tsesn_B8AXefAjb2IZ3LWoTwE3rhZY2K2HLrGZ\tcdk_S3umj01mY9WIJ2ft55f9VX5RjhNN3Afo\nKELLE ROCKENSTEIN\tsesn_B8AXefAjb2IZ3LWoTwE3rhZY2K2HLrGZ\tcdk_hAjnaUO7z4Ig60Qclj45RKBgvbTrcvnl\nKELLE ROCKENSTEIN\tsesn_O3Z8SWUg1D8sfihO4JDqLDEVbYuN4GkU\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_O3Z8SWUg1D8sfihO4JDqLDEVbYuN4GkU\tcdk_jDNpxZbqQ00ptnJYFj3bgYN4EO0QiU36\nKELLE ROCKENSTEIN\tsesn_O3Z8SWUg1D8sfihO4JDqLDEVbYuN4GkU\tcdk_ugyz6Rh3vVPzcxgUKNhVftNToAqRwEUW\nKELLE ROCKENSTEIN\tsesn_O3Z8SWUg1D8sfihO4JDqLDEVbYuN4GkU\tcdk_zMZAQys3S0tZaq5EHUBdBLenHsuOp0lB\nKELLE ROCKENSTEIN\tsesn_O3Z8SWUg1D8sfihO4JDqLDEVbYuN4GkU\tcdk_zMZAQys3S0tZaq5EHUBdBLenHsuOp0lB\nKELLE ROCKENSTEIN\tsesn_LFeu9QPIvpGUBOQDrPMxiAdozpaQisDl\tcdk_FHDU16ldcalposc07E1uk3vRwbFPE0De\nKELLE ROCKENSTEIN\tsesn_LFeu9QPIvpGUBOQDrPMxiAdozpaQisDl\tcdk_uhXAFNdS6hNk1TXwH5M66GfRgqfNE8PC\nKELLE ROCKENSTEIN\tsesn_LFeu9QPIvpGUBOQDrPMxiAdozpaQisDl\tcdk_M4qx3Dfpt5XvGIRaSozPOMwSDkQ1oJYD\nKELLE ROCKENSTEIN\tsesn_MDddlA60QWp4xsz169e4xjPQAEe3hraB\tcdk_S3umj01mY9WIJ2ft55f9VX5RjhNN3Afo\nKELLE ROCKENSTEIN\tsesn_MDddlA60QWp4xsz169e4xjPQAEe3hraB\tcdk_M4qx3Dfpt5XvGIRaSozPOMwSDkQ1oJYD\nKELLE ROCKENSTEIN\tsesn_MDddlA60QWp4xsz169e4xjPQAEe3hraB\tcdk_2ohu1cQ6yu8DuTo2x3DouBIkzoLwP1zt\nKELLE ROCKENSTEIN\tsesn_MDddlA60QWp4xsz169e4xjPQAEe3hraB\tcdk_zMZAQys3S0tZaq5EHUBdBLenHsuOp0lB\nKELLE ROCKENSTEIN\tsesn_siKhJC84Y6eyY8EN8e80z8pUwL0GR1Wi\tcdk_PEU2DEn5w0aPrQyubHbb8cImNKyt3Zyr\nKELLE ROCKENSTEIN\tsesn_6hx884jclSJeGMrE4O4QCBgtS7ZUezbs\tcdk_ugyz6Rh3vVPzcxgUKNhVftNToAqRwEUW\nKELLE ROCKENSTEIN\tsesn_6hx884jclSJeGMrE4O4QCBgtS7ZUezbs\tcdk_mz5srH5z7trN7ZtFRC09ODNQYqEkugrd\nKELLE ROCKENSTEIN\tsesn_6hx884jclSJeGMrE4O4QCBgtS7ZUezbs\tcdk_mz5srH5z7trN7ZtFRC09ODNQYqEkugrd\nKELLE ROCKENSTEIN\tsesn_opKBp8gsjI7OG1os241C70izq2wcfNDn\tcdk_zMZAQys3S0tZaq5EHUBdBLenHsuOp0lB\nKELLE ROCKENSTEIN\tsesn_opKBp8gsjI7OG1os241C70izq2wcfNDn\tcdk_S3umj01mY9WIJ2ft55f9VX5RjhNN3Afo\nKELLE ROCKENSTEIN\tsesn_vF5E3tNwuxOxcBmhZriAXNOHPb44P7yd\tcdk_vb1pzJYSXGUrZ4CMTYwKj74AMhaTSGdL\nKELLE ROCKENSTEIN\tsesn_vF5E3tNwuxOxcBmhZriAXNOHPb44P7yd\tcdk_vb1pzJYSXGUrZ4CMTYwKj74AMhaTSGdL\nKELLE ROCKENSTEIN\tsesn_1Uu8w3TtgQuCFQozPcBbdDPioydP2oOP\tcdk_P5JVmD18HldgjGJuYZek4JBh2EG1NLsq\nKELLE ROCKENSTEIN\tsesn_1Uu8w3TtgQuCFQozPcBbdDPioydP2oOP\tcdk_mz5srH5z7trN7ZtFRC09ODNQYqEkugrd\nKELLE ROCKENSTEIN\tsesn_1Uu8w3TtgQuCFQozPcBbdDPioydP2oOP\tcdk_mz5srH5z7trN7ZtFRC09ODNQYqEkugrd\nKELLE ROCKENSTEIN\tsesn_1Uu8w3TtgQuCFQozPcBbdDPioydP2oOP\tcdk_zMZAQys3S0tZaq5EHUBdBLenHsuOp0lB\nTHOMAS SPRAY\tsesn_aaSv29QoTiQ7m8hm2UWy1Z3Xrm91QP8k\tcdk_dLI5tFHX0FAwO2GughqJrYSiexbRnCUI\nTHOMAS SPRAY\tsesn_aaSv29QoTiQ7m8hm2UWy1Z3Xrm91QP8k\tcdk_mTyIP5Vfh3oeHztqXvR2YmCexv7yflhV\nTHOMAS SPRAY\tsesn_aaSv29QoTiQ7m8hm2UWy1Z3Xrm91QP8k\tcdk_88p2D0eSAVfx2icJWJI0YMaD4zTZodAn\nTHOMAS SPRAY\tsesn_P4C8zhbHzhkm64o7D281REkMRvaM2aY5\tcdk_dLI5tFHX0FAwO2GughqJrYSiexbRnCUI\nTHOMAS SPRAY\tsesn_P4C8zhbHzhkm64o7D281REkMRvaM2aY5\tcdk_xP3NPxgi5TYGAYvahbjI6wmWhokfrWrf\nTHOMAS SPRAY\tsesn_p4P5pf0UKnAMM97hrZvhbK42MlkfWibg\tcdk_Zabs6HveuhfpzYsXaeDbOVO4bHOat0FE\nTHOMAS SPRAY\tsesn_p4P5pf0UKnAMM97hrZvhbK42MlkfWibg\tcdk_88p2D0eSAVfx2icJWJI0YMaD4zTZodAn\n"}]},"apps":[],"jobName":"paragraph_1529949856465_-767436946","id":"20160611-072454_1554728841","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:08:28+0000","dateFinished":"2018-06-25T18:08:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7292"},{"text":"%md\n# Check Uniqueness of Session IDs\n\nOf course every session key should be unique. We test this - but our data might have the same session key multiple times per user with different CDKs.\n\nIdea is to perform the following steps:\n1. Select only name and session from the data\n2. Make these tuples distinct\n3. Count the number of occurances of each session. Eaach session should belong to exactly one user, so all sessions should be distinct.\n4. Filter the result, such that only sessions with a count of higher than 1 are left. These are illegal sessions with multiple users\n5. Count the number of such illegal sessions.\n","dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Check Uniqueness of Session IDs</h1>\n<p>Of course every session key should be unique. We test this - but our data might have the same session key multiple times per user with different CDKs.</p>\n<p>Idea is to perform the following steps:</p>\n<ol>\n<li>Select only name and session from the data</li>\n<li>Make these tuples distinct</li>\n<li>Count the number of occurances of each session. Eaach session should belong to exactly one user, so all sessions should be distinct.</li>\n<li>Filter the result, such that only sessions with a count of higher than 1 are left. These are illegal sessions with multiple users</li>\n<li>Count the number of such illegal sessions.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1529949856466_-766282699","id":"20160611-085048_1983995929","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7293"},{"text":"sessions.select($\"name\",$\"session\")\n    .distinct()\n    .groupBy($\"session\").count()\n    .filter($\"count\">1)\n    .count()","user":"anonymous","dateUpdated":"2018-06-25T18:08:33+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res35: Long = 0\n"}]},"apps":[],"jobName":"paragraph_1529949856466_-766282699","id":"20160611-074300_1949463897","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:08:33+0000","dateFinished":"2018-06-25T18:09:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7294"},{"text":"sessions.count()","user":"anonymous","dateUpdated":"2018-06-25T18:09:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res36: Long = 12472508\n"}]},"apps":[],"jobName":"paragraph_1529949856466_-766282699","id":"20160611-075652_881962885","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:09:24+0000","dateFinished":"2018-06-25T18:09:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7295"},{"text":"%md\n# 6. Save Result\n\nNow since we are happy with our session data, let's save it in a file. I would use Parquet, but maybe you want to use something else, like CSV, ORC or Avro. I do not recommend to use JSON, though.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>6. Save Result</h1>\n<p>Now since we are happy with our session data, let's save it in a file. I would use Parquet, but maybe you want to use something else, like CSV, ORC or Avro. I do not recommend to use JSON, though.</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856466_-766282699","id":"20160611-085635_445219987","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7296"},{"text":"sessions.write.mode(\"overwrite\").parquet(\"data/sessions\")","user":"anonymous","dateUpdated":"2018-06-25T18:09:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1529949856467_-766667448","id":"20160611-085121_1797782377","dateCreated":"2018-06-25T18:04:16+0000","dateStarted":"2018-06-25T18:09:48+0000","dateFinished":"2018-06-25T18:10:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7297"},{"text":"%md\n# 8. Cleanup Memory\n\nIn case we cached some DataFrames or RDDs, let's unpersist these RDDs to free up memory in Executors.","dateUpdated":"2018-06-25T18:04:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>8. Cleanup Memory</h1>\n<p>In case we cached some DataFrames or RDDs, let's unpersist these RDDs to free up memory in Executors.</p>\n"}]},"apps":[],"jobName":"paragraph_1529949856467_-766667448","id":"20160611-085609_516211283","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7298"},{"text":"sc.getPersistentRDDs.foreach { entry => entry._2.unpersist() }","dateUpdated":"2018-06-25T18:04:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1529949856467_-766667448","id":"20160611-080140_239855178","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7299"},{"dateUpdated":"2018-06-25T18:04:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529949856468_-768591193","id":"20160611-085320_898645245","dateCreated":"2018-06-25T18:04:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7300"}],"name":"GraphX Test Data Generation Solution","id":"2DK4XU6P8","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}