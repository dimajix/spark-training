{"paragraphs":[{"text":"%md\n# 0. Add Dependencies\n\nBefore we start working with Spark and Kafka, we need to add the Spark Kafka SQL dependency to Zeppelin:\n```\norg.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2 exclude: net.jpountz.lz4:lz4\n```","user":"anonymous","dateUpdated":"2018-12-15T08:37:20+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>0. Add Dependencies</h1>\n<p>Before we start working with Spark and Kafka, we need to add the Spark Kafka SQL dependency to Zeppelin:</p>\n<pre><code>org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2 exclude: net.jpountz.lz4:lz4\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862589937_1827678435","id":"20181215-082949_1267204707","dateCreated":"2018-12-15T08:29:49+0000","dateStarted":"2018-12-15T08:30:01+0000","dateFinished":"2018-12-15T08:30:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6602"},{"text":"%spark.dep\nz.reset()\nz.load(\"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2\").exclude(\"net.jpountz.lz4:lz4\")\nz.load(\"mysql:mysql-connector-java:6.0.6\")","user":"anonymous","dateUpdated":"2018-12-15T08:30:17+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862605568_-2172010","id":"20181215-083005_1117122094","dateCreated":"2018-12-15T08:30:05+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6603"},{"text":"%md\n# 1. Load Station Data\n\nNow we load the station meta data using traditional SparkSQL DataFrame methods. Since the meta data is stored as a simple CSV, this should be simple. Nevertheless we will explicitly specify a schema, since we do not want to rely on the automatic type inference from Spark.\n","user":"anonymous","dateUpdated":"2018-12-15T08:30:21+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>1. Load Station Data</h1>\n<p>Now we load the station meta data using traditional SparkSQL DataFrame methods. Since the meta data is stored as a simple CSV, this should be simple. Nevertheless we will explicitly specify a schema, since we do not want to rely on the automatic type inference from Spark.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572766_-351185235","id":"20170109-022408_398981917","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:30:21+0000","dateFinished":"2018-12-15T08:30:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6604"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.FloatType\nimport org.apache.spark.sql.types.DateType\n\ndef extractFloat = udf((v:String) => if (v != null) v.toFloat else None, FloatType)\n\nval isdSchema = StructType(\n        StructField(\"usaf\", StringType) ::\n        StructField(\"wban\", StringType) ::\n        StructField(\"name\", StringType) ::\n        StructField(\"country\", StringType) ::\n        StructField(\"state\", StringType) ::\n        StructField(\"icao\", StringType) ::\n        StructField(\"latitude\", StringType) ::\n        StructField(\"longitude\", StringType) ::\n        StructField(\"elevation\", StringType) ::\n        StructField(\"date_begin\", DateType) ::\n        StructField(\"date_end\", DateType) ::\n        Nil\n    )\nval isd = sqlContext.read\n    .option(\"header\",\"true\")\n    .option(\"dateFormat\",\"yyyyMMdd\")\n    .schema(isdSchema)\n    .csv(\"s3://dimajix-training/data/weather/isd-history\")\n    .withColumn(\"latitude\", extractFloat($\"latitude\"))\n    .withColumn(\"longitude\", extractFloat($\"longitude\"))\n    .withColumn(\"elevation\", extractFloat($\"elevation\"))\n    \n\nz.show(isd.limit(10))","dateUpdated":"2018-12-15T08:29:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572767_-351569984","id":"20170109-022420_197453754","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6605"},{"text":"isd.printSchema()","dateUpdated":"2018-12-15T08:29:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572768_-365805693","id":"20170109-022449_34670710","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6606"},{"text":"%md\n# 2. Connect to data source\n\nFirst you need to run the netcat program, for example via\n\n    spark-training/utils/pynetcat.py -I1 -B10 -P9977 < weather_sample.txt\n\nThen we connect to the raw data socket as the datasource by using the `DataStreamReader` API via `spark.readStream`. We need to specify the options `host`, `port` and we need to use the format `socket` for connecting to the data source. The socket will stream weather data samples in raw format, i.e. one record per line.","user":"anonymous","dateUpdated":"2018-12-15T08:35:04+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>2. Connect to data source</h1>\n<p>First you need to run the netcat program, for example via</p>\n<pre><code>spark-training/utils/pynetcat.py -I1 -B10 -P9977 &lt; weather_sample.txt\n</code></pre>\n<p>Then we connect to the raw data socket as the datasource by using the <code>DataStreamReader</code> API via <code>spark.readStream</code>. We need to specify the options <code>host</code>, <code>port</code> and we need to use the format <code>socket</code> for connecting to the data source. The socket will stream weather data samples in raw format, i.e. one record per line.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572770_-365036195","id":"20170109-022532_52613486","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:35:04+0000","dateFinished":"2018-12-15T08:35:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6607"},{"text":"// Fill in the correct AWS VPC address of your master host\nval master = \"kku.training.dimajix-aws.net\"\n\n// Connect to raw text stream socket using the DataStreamReader API via spark.readStream. You need to specify the options `host`, `port` and you need to use the format `socket`\nval lines = ... // YOUR CODE HERE\n","dateUpdated":"2018-12-15T08:29:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572771_-365420944","id":"20170109-022559_819187627","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6608"},{"text":"%md\n# 3. Extract Weather Data\n\nWe need to extract the weather data from the raw string. This can be done using SparkSQL methods. Since this is a rather time-consuming data-fiddling task, you can simply use the code as-is.","user":"anonymous","dateUpdated":"2018-12-15T08:34:59+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>3. Extract Weather Data</h1>\n<p>We need to extract the weather data from the raw string. This can be done using SparkSQL methods. Since this is a rather time-consuming data-fiddling task, you can simply use the code as-is.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572772_-367344689","id":"20170109-023945_1454623182","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:34:59+0000","dateFinished":"2018-12-15T08:34:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6609"},{"text":"import org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.types.FloatType\n\nval weather = lines\n    .withColumn(\"date\", lines(\"value\").substr(16,8))\n    .withColumn(\"time\", lines(\"value\").substr(24,4))\n    .withColumn(\"usaf\", lines(\"value\").substr(5,6))\n    .withColumn(\"wban\", lines(\"value\").substr(11,5))\n    .withColumn(\"air_temperature_quality\", lines(\"value\").substr(93,1).cast(IntegerType))\n    .withColumn(\"air_temperature\", lines(\"value\").substr(88,5).cast(FloatType)/10.0)\n    .withColumn(\"wind_speed_quality\", lines(\"value\").substr(70,1).cast(IntegerType))\n    .withColumn(\"wind_speed\", lines(\"value\").substr(66,4).cast(FloatType)/10.0)\n","dateUpdated":"2018-12-15T08:29:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572773_-367729438","id":"20170109-022624_1297431457","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6610"},{"text":"%md\n# 4. Peek inside Stream\n\nOne nice thing about structured streaming is that it is super easy to peek inside a stream. You need to perform the following steps:\n1. Create a `DataStreamWriter` object using the `writeStream` method of your DataFrame.\n2. Set the format to `console`\n3. Set the output mode to `append`\n4. Specify a `checkPointLocation` on HDFS (ok, this is not trivial, so it is in the code below)\n5. Start the continuous query via `start`","user":"anonymous","dateUpdated":"2018-12-15T08:34:53+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>4. Peek inside Stream</h1>\n<p>One nice thing about structured streaming is that it is super easy to peek inside a stream. You need to perform the following steps:<br/>1. Create a <code>DataStreamWriter</code> object using the <code>writeStream</code> method of your DataFrame.<br/>2. Set the format to <code>console</code><br/>3. Set the output mode to <code>append</code><br/>4. Specify a <code>checkPointLocation</code> on HDFS (ok, this is not trivial, so it is in the code below)<br/>5. Start the continuous query via <code>start</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572774_-366575191","id":"20170109-024025_443642059","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:34:53+0000","dateFinished":"2018-12-15T08:34:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6611"},{"text":"val query = ... // YOUR CODE HERE\n    ...\n    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-\" + System.currentTimeMillis())\n    ...\n    \nThread.sleep(30000)    ","dateUpdated":"2018-12-15T08:42:47+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572774_-366575191","id":"20170109-023319_1896648636","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6612"},{"text":"%md \n## 4.1 Stop Query\n\nIf nothing is done, the query will continue to run in the background. But we can stop it explicitly.","user":"anonymous","dateUpdated":"2018-12-15T08:34:18+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4.1 Stop Query</h2>\n<p>If nothing is done, the query will continue to run in the background. But we can stop it explicitly.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862818949_371173985","id":"20181215-083338_231028690","dateCreated":"2018-12-15T08:33:38+0000","dateStarted":"2018-12-15T08:34:18+0000","dateFinished":"2018-12-15T08:34:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6613"},{"text":"// YOUR CODE HERE","user":"anonymous","dateUpdated":"2018-12-15T08:34:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862860841_305118418","id":"20181215-083420_938881377","dateCreated":"2018-12-15T08:34:20+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6614"},{"text":"%md\n## 5. Perform Calculation\n\nAgain we perform a grouped aggregation of some metrics. We are inetersted in the following metrics, grouped by country and year:\n\n* Minimum air temperature\n* Maximum air temperature\n* Minimum wind speed\n* Maximum wind speed\n\nAgain we need to evaulate the \"quality\" fields of the incoming data to decide if the correspong wind speed or air temeprature is valid.","user":"anonymous","dateUpdated":"2018-12-15T08:34:37+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>5. Perform Calculation</h2>\n<p>Again we perform a grouped aggregation of some metrics. We are inetersted in the following metrics, grouped by country and year:</p>\n<ul>\n  <li>Minimum air temperature</li>\n  <li>Maximum air temperature</li>\n  <li>Minimum wind speed</li>\n  <li>Maximum wind speed</li>\n</ul>\n<p>Again we need to evaulate the &ldquo;quality&rdquo; fields of the incoming data to decide if the correspong wind speed or air temeprature is valid.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572777_-369268433","id":"20170109-023458_107574560","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:34:37+0000","dateFinished":"2018-12-15T08:34:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6615"},{"text":"// Create a broadcast version of the station list. This seems to make sense specifically for streaming\nval stations = broadcast(isd)\n\nval aggregated_weather = ...\n    // 1. Join weather data with station data on columns 'usaf' and 'wban'\n\n    // 2. Extract year from date column (first four letters), store it in a new column called 'year'\n\n    // 4. Group by country (from isd) and year (from above)\n\n    // 5. Perform aggregations of min/max of temperature and wind speed. Again pay attention to quality flags!\n","dateUpdated":"2018-12-15T08:29:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nstations: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [usaf: string, wban: string ... 9 more fields]\n\naggregated_weather: org.apache.spark.sql.DataFrame = [country: string, year: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1544862572778_-368114187","id":"20170109-024107_164598352","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6616"},{"text":"%md\n## 5.1 Peek Inside\n\nThis is not what we really want at the end, but let's try to peek inside the infinite streaming table. This is achieved by the same steps as above:\n\n1. Create a DataStreamWriter object using the writeStream method of your DataFrame `aggregated_weather`.\n2. Set the format to `console`\n3. Set the output mode to `append` (aggregations do not support `append`)\n4. Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)\n5. Start the continuous query via `start`","user":"anonymous","dateUpdated":"2018-12-15T08:35:11+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>5.1 Peek Inside</h2>\n<p>This is not what we really want at the end, but let&rsquo;s try to peek inside the infinite streaming table. This is achieved by the same steps as above:</p>\n<ol>\n  <li>Create a DataStreamWriter object using the writeStream method of your DataFrame <code>aggregated_weather</code>.</li>\n  <li>Set the format to <code>console</code></li>\n  <li>Set the output mode to <code>append</code> (aggregations do not support <code>append</code>)</li>\n  <li>Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)</li>\n  <li>Start the continuous query via <code>start</code></li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572779_-368498936","id":"20170218-153633_1699313591","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:35:11+0000","dateFinished":"2018-12-15T08:35:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6617"},{"text":"val query = // YOUR CODE HERE\n    ...\n    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-\" + System.currentTimeMillis())\n    ...\n    \nThread.sleep(30000)\nquery.stop()","dateUpdated":"2018-12-15T08:42:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572780_-370422680","id":"20170109-024151_139400986","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6618"},{"text":"%md\n# 6. Create live Table\n\nWe can also use a \"memory\" output, which is a queryable live table. In order to do so, we again create a new table, but this time with format `memory` and an explicit query name `aggregated_weather`. Using a `memory` output will create a dynamic table in memory (only `complete` output supported right now), which can be queried using SQL.\n\n1. Create a DataStreamWriter object using the writeStream method of your DataFrame `aggregated_weather`.\n2. Set the format to `memory`\n3. Set the output mode to `complete`\n4. Set the query name to `aggregated_weather`\n5. Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)\n6. Start the continuous query via `start`","user":"anonymous","dateUpdated":"2018-12-15T08:36:05+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>6. Create live Table</h1>\n<p>We can also use a &ldquo;memory&rdquo; output, which is a queryable live table. In order to do so, we again create a new table, but this time with format <code>memory</code> and an explicit query name <code>aggregated_weather</code>. Using a <code>memory</code> output will create a dynamic table in memory (only <code>complete</code> output supported right now), which can be queried using SQL.</p>\n<ol>\n  <li>Create a DataStreamWriter object using the writeStream method of your DataFrame <code>aggregated_weather</code>.</li>\n  <li>Set the format to <code>memory</code></li>\n  <li>Set the output mode to <code>complete</code></li>\n  <li>Set the query name to <code>aggregated_weather</code></li>\n  <li>Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)</li>\n  <li>Start the continuous query via <code>start</code></li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572781_-370807429","id":"20170109-043321_2106212152","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:36:05+0000","dateFinished":"2018-12-15T08:36:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6619"},{"text":"val query = ... // YOUR CODE HERE\n    ...\n    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-\" + System.currentTimeMillis())\n    ...\n    .start()","dateUpdated":"2018-12-15T08:42:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572782_-369653182","id":"20170109-024536_1643682762","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6620"},{"text":"%sql\nselect * from aggregated_weather","dateUpdated":"2018-12-15T08:29:32+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"country","index":0,"aggr":"sum"}],"values":[{"name":"year","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"country","index":0,"aggr":"sum"},"yAxis":{"name":"year","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572783_-370037931","id":"20170109-043226_2033527472","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6621"},{"text":"// Stop the query again\nquery.stop()","dateUpdated":"2018-12-15T08:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1544862572784_-359649711","id":"20170109-043235_704854873","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6622"},{"text":"%md\n# 7. SQL Output\n\nWe also like to persist the aggregated data into a SQL Database (MySQL in this case). The database will have the following schema:\n\n* country - String\n* year - String\n* temp_min - Float\n* temp_axn - Float\n* wind_min - Float\n* wind_max - Float\n\nPrimary key is `country` and `year`. \n\nIn order to perform this task, we will implement a `JDBCSink` for writing streaming results into a MySQL Database.","user":"anonymous","dateUpdated":"2018-12-15T08:36:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>7. SQL Output</h1>\n<p>We also like to persist the aggregated data into a SQL Database (MySQL in this case). The database will have the following schema:</p>\n<ul>\n  <li>country - String</li>\n  <li>year - String</li>\n  <li>temp_min - Float</li>\n  <li>temp_axn - Float</li>\n  <li>wind_min - Float</li>\n  <li>wind_max - Float</li>\n</ul>\n<p>Primary key is <code>country</code> and <code>year</code>. </p>\n<p>In order to perform this task, we will implement a <code>JDBCSink</code> for writing streaming results into a MySQL Database.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572785_-360034460","id":"20170109-043303_576621974","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:36:12+0000","dateFinished":"2018-12-15T08:36:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6623"},{"text":"%md\n## 7.1 Implement JDBCSink\n\nFirst we implement the JDBCSink. The sink will receive one record at a time and has to write it into the MySQL databae. You have to adjust the `process` method, such that the required fields are extracted correctly from your SQL Row.\n","user":"anonymous","dateUpdated":"2018-12-15T08:36:18+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>7.1 Implement JDBCSink</h2>\n<p>First we implement the JDBCSink. The sink will receive one record at a time and has to write it into the MySQL databae. You have to adjust the <code>process</code> method, such that the required fields are extracted correctly from your SQL Row.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572786_-358880213","id":"20180411-060738_1018959265","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:36:18+0000","dateFinished":"2018-12-15T08:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6624"},{"text":"class JDBCSink(url: String, user:String, pwd:String) extends org.apache.spark.sql.ForeachWriter[org.apache.spark.sql.Row] {\n    val driver = \"com.mysql.cj.jdbc.Driver\"\n\n    @transient var connection:java.sql.Connection = _\n    @transient var statement:java.sql.Statement = _\n\n    def open(partitionId: Long, version: Long):Boolean = {\n        Class.forName(driver)\n        connection = java.sql.DriverManager.getConnection(url, user, pwd)\n        statement = connection.createStatement\n        true\n    }\n\n    def process(value: org.apache.spark.sql.Row): Unit = { \n        try {\n            val country = // YOUR CODE HERE\n            val year = // YOUR CODE HERE\n            val min_temp = // YOUR CODE HERE\n            val max_temp = // YOUR CODE HERE\n            val min_wind = // YOUR CODE HERE\n            val max_wind = // YOUR CODE HERE\n            statement.executeUpdate(s\"\"\"\n                INSERT INTO weather_minmax(county, year, min_temp, max_temp, min_wind, max_wind) \n                VALUES ('$country','$year',$min_temp, $max_temp, $min_wind, $max_wind) \n                ON DUPLICATE KEY UPDATE min_temp=$min_temp, max_temp=$max_temp,min_wind=$min_wind,max_wind=$max_wind;\"\"\"\n            )\n        }\n        catch {\n            case t:Throwable =>\n        }\n    }\n\n    def close(errorOrNull:Throwable):Unit = {\n        connection.close\n        connection = null\n        statement = null\n    }\n}\n","dateUpdated":"2018-12-15T08:29:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572786_-358880213","id":"20180411-060754_1414799259","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6625"},{"text":"val url = \"jdbc:mysql://YOUR-CLUSTER-NODE-HERE/training\"\nval user = \"user\"\nval pwd = \"user\"\n\nval sink = new JDBCSink(url, user, pwd)","dateUpdated":"2018-12-15T08:29:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572787_-359264962","id":"20180411-060849_871559019","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6626"},{"text":"%md\n## 7.2 Create Target Table\n\nIn order to store the results in MySQL, we need to create an appropriate table in MySQL. We use shell commands for that.","user":"anonymous","dateUpdated":"2018-12-15T08:36:25+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>7.2 Create Target Table</h2>\n<p>In order to store the results in MySQL, we need to create an appropriate table in MySQL. We use shell commands for that.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572788_-361188706","id":"20180411-060906_1178671131","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:36:25+0000","dateFinished":"2018-12-15T08:36:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6627"},{"text":"%sh\nmysql --user=user --password=user -e \"CREATE TABLE IF NOT EXISTS training.weather_minmax (country VARCHAR(8) NOT NULL, year VARCHAR(4) NOT NULL, min_temp FLOAT, max_temp FLOAT, min_wind FLOAT, max_wind FLOAT, PRIMARY KEY (country, year))\"","dateUpdated":"2018-12-15T08:29:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572789_-361573455","id":"20180411-060919_943106383","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6628"},{"text":"%md\n## 7.3 Run Streaming Query\n\nFinally we need to set up an appropriate Streaming query and write its results into the database. We will reuse the query `aggreagated_weather` from above, but configure a different sink.\n","user":"anonymous","dateUpdated":"2018-12-15T08:36:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>7.3 Run Streaming Query</h2>\n<p>Finally we need to set up an appropriate Streaming query and write its results into the database. We will reuse the query <code>aggreagated_weather</code> from above, but configure a different sink.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544862572790_-360419209","id":"20180411-060930_1346959511","dateCreated":"2018-12-15T08:29:32+0000","dateStarted":"2018-12-15T08:36:31+0000","dateFinished":"2018-12-15T08:36:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6629"},{"text":"import org.apache.spark.sql.streaming.ProcessingTime\n\nval query = // YOUR CODE HERE\n","dateUpdated":"2018-12-15T08:29:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572791_-360803958","id":"20180411-060942_832647410","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6630"},{"text":"query.stop()","dateUpdated":"2018-12-15T08:29:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544862572792_-362727702","id":"20180411-061010_1325052378","dateCreated":"2018-12-15T08:29:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6631"}],"name":"Weather Structured Streaming - Exercise","id":"2DX1DDX2E","angularObjects":{"2D98FMAH1:shared_process":[],"2D7JMBHMW:shared_process":[],"2D97KQQMQ:shared_process":[],"2D75J9G8A:shared_process":[],"2D7EN88U2:shared_process":[],"2D9JW72F7:shared_process":[],"2D833H41P:shared_process":[],"2D9GNB6ZN:shared_process":[],"2D6C3QSGG:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}