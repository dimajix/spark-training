{"paragraphs":[{"text":"%md\n# Weather Data Analytics\nThis notebook performs some basic weather data analytics using the Spark DataFrame interface.","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Weather Data Analytics</h1>\n<p>This notebook performs some basic weather data analytics using the Spark DataFrame interface.</p>\n"}]},"apps":[],"jobName":"paragraph_1523368049103_1617271123","id":"20160612-182259_265217764","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4280"},{"text":"%md\n# Hive Preparations\n\nIn order to ease working with the data, we register all data as Hive tables.","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Hive Preparations</h1>\n<p>In order to ease working with the data, we register all data as Hive tables.</p>\n"}]},"apps":[],"jobName":"paragraph_1523368049103_1617271123","id":"20160612-182259_40944534","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4281"},{"text":"// Common data location - adjust if not correct\nval storageLocation = \"s3://dimajix-training/data/weather\"","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523368049103_1617271123","id":"20160612-182259_960015898","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4282"},{"text":"%sql\ncreate database if not exists training","user":"anonymous","dateUpdated":"2018-04-10T13:55:39+0000","config":{"editorMode":"ace/mode/sql","colWidth":12,"tableHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":""}]},"apps":[],"jobName":"paragraph_1523368049104_1627659343","id":"20160612-182259_942557775","dateCreated":"2018-04-10T13:47:29+0000","dateStarted":"2018-04-10T13:55:39+0000","dateFinished":"2018-04-10T13:55:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4283"},{"text":"%md\n## Creating Hive tables for Weather Data\nSince the weather data itself is not a simple CSV format, but something more complex, we chose a two step approach:\n\n1. Create a simple table, where the data files are presented as simple text records. Each line is a full record\n2. Create a VIEW on top of the table, which will extract the desired parameters from the raw data.\n\nSicne we have multiple years, we also partition the raw data table by year.","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Creating Hive tables for Weather Data</h2>\n<p>Since the weather data itself is not a simple CSV format, but something more complex, we chose a two step approach:</p>\n<ol>\n<li>Create a simple table, where the data files are presented as simple text records. Each line is a full record</li>\n<li>Create a VIEW on top of the table, which will extract the desired parameters from the raw data.</li>\n</ol>\n<p>Sicne we have multiple years, we also partition the raw data table by year.</p>\n"}]},"apps":[],"jobName":"paragraph_1523368049104_1627659343","id":"20160612-182259_1729144153","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4284"},{"text":"%sql\nCREATE EXTERNAL TABLE training.weather_raw (\n    data string\n)\nPARTITIONED BY(year STRING) \nSTORED AS TEXTFILE\nLOCATION 's3://dimajix-training/data/weather'\n","user":"anonymous","dateUpdated":"2018-04-10T13:58:31+0000","config":{"editorMode":"ace/mode/sql","colWidth":12,"tableHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523368049104_1627659343","id":"20160612-182259_1977068267","dateCreated":"2018-04-10T13:47:29+0000","dateStarted":"2018-04-10T13:56:40+0000","dateFinished":"2018-04-10T13:56:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4285"},{"text":"%sql\nCREATE VIEW training.weather AS\nSELECT \n    year,\n    SUBSTR(`data`,5,6) AS `usaf`,\n    SUBSTR(`data`,11,5) AS `wban`, \n    SUBSTR(`data`,16,8) AS `date`, \n    SUBSTR(`data`,24,4) AS `time`,\n    SUBSTR(`data`,42,5) AS report_type,\n    SUBSTR(`data`,61,3) AS wind_direction, \n    SUBSTR(`data`,64,1) AS wind_direction_qual, \n    SUBSTR(`data`,65,1) AS wind_observation, \n    CAST(SUBSTR(`data`,66,4) AS FLOAT)/10 AS wind_speed,\n    SUBSTR(`data`,70,1) AS wind_speed_qual,\n    CAST(SUBSTR(`data`,88,5) AS FLOAT)/10 AS air_temperature, \n    SUBSTR(`data`,93,1) AS air_temperature_qual \nFROM training.weather_raw\n","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/sql","colWidth":12,"tableHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"result","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"result","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"result\n"}]},"apps":[],"jobName":"paragraph_1523368049104_1627659343","id":"20160612-182259_571893438","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4286"},{"text":"%md\n## Adding Data Partitions\n\nNow we need to add all data as partitions. Since this would be somewhat tedious to do it over and over again for every partition, we remember that we know how to program and let Scala do the work for us.","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Adding Data Partitions</h2>\n<p>Now we need to add all data as partitions. Since this would be somewhat tedious to do it over and over again for every partition, we remember that we know how to program and let Scala do the work for us.</p>\n"}]},"apps":[],"jobName":"paragraph_1523368049104_1627659343","id":"20160612-182259_1823692141","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4287"},{"text":"// Add all partitions\n(2003 to 2014) foreach {i => spark.sql(s\"alter table training.weather_raw add partition(year='${i}') location '${storageLocation}/${i}'\") }","dateUpdated":"2018-04-10T13:51:25+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523368049105_1627274594","id":"20160612-182259_1907203803","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4288"},{"text":"%sql\nselect * from training.weather limit 10","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/sql","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523368049105_1627274594","id":"20160612-182259_1033401899","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4289"},{"text":"%md\n## Creating Table for Station Metadata\n\nFortunately station metadata is stored as CSV, so we can directly create a table.","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Creating Table for Station Metadata</h2>\n<p>Fortunately station metadata is stored as CSV, so we can directly create a table.</p>\n"}]},"apps":[],"jobName":"paragraph_1523368049105_1627274594","id":"20160612-182259_1685463204","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4290"},{"text":"%sql\nCREATE EXTERNAL TABLE training.stations(\n    usaf STRING,\n    wban STRING,\n    name STRING,\n    country STRING,\n    state STRING,\n    icao STRING,\n    latitude FLOAT,\n    longitude FLOAT,\n    elevation FLOAT,\n    date_begin STRING,\n    date_end STRING) \nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n   \"separatorChar\" = \",\",\n   \"quoteChar\"     = \"\\\"\",\n   \"escapeChar\"    = \"\\\\\"\n)\nSTORED AS TEXTFILE\nLOCATION 's3://dimajix-training/data/weather/isd-history'","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/sql","colWidth":12,"tableHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"result","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"result","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"result\n"}]},"apps":[],"jobName":"paragraph_1523368049105_1627274594","id":"20160612-182259_123282051","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4291"},{"text":"%sql\nselect * from training.stations limit 10","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/sql","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523368049105_1627274594","id":"20160612-182259_1827195449","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4292"},{"text":"%md\n# Analytics\n\nNow that we have everything in Hive, we can do the analysis easily.\n\n1. Load Hive tables as DataFrames\n2. Join both DataFrames and station code (wban and usaf)\n3. Extract year from date\n4. Group by country and year\n5. Aggregate minimum/maximum values for wind and temperature, pay attention to quality!\n6. Print the results, such that Zeppelin can make nice graphics :)","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Analytics</h1>\n<p>Now that we have everything in Hive, we can do the analysis easily.</p>\n<ol>\n<li>Load Hive tables as DataFrames</li>\n<li>Join both DataFrames and station code (wban and usaf)</li>\n<li>Extract year from date</li>\n<li>Group by country and year</li>\n<li>Aggregate minimum/maximum values for wind and temperature, pay attention to quality!</li>\n<li>Print the results, such that Zeppelin can make nice graphics :)</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1523368049105_1627274594","id":"20160612-182259_657759243","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4293"},{"text":"// YOUR CODE HERE\nval result = ...\n  \nz.show(result)","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"multiBarChart","height":164,"optionOpen":false,"keys":[{"name":"country","index":0,"aggr":"sum"}],"values":[{"name":"min_wind_speed","index":2,"aggr":"min"},{"name":"max_wind_speed","index":3,"aggr":"max"},{"name":"min_temperature","index":4,"aggr":"min"},{"name":"max_temperature","index":5,"aggr":"max"}],"groups":[{"name":"year","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"country","index":0,"aggr":"sum"},"yAxis":{"name":"year","index":1,"aggr":"sum"}}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523368049106_1628428841","id":"20160612-182259_1861265992","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4294"},{"text":"","dateUpdated":"2018-04-10T13:47:29+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523368049106_1628428841","id":"20160612-182259_2050725342","dateCreated":"2018-04-10T13:47:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4295"}],"name":"Weather DataFrame Analysis Hive Exercise","id":"2DB6DHYAV","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}