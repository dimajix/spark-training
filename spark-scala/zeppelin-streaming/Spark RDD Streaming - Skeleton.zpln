{"paragraphs":[{"text":"%md\n# Create a Streaming Context\n\nStreaming is performed using a special StreamingContext, which can be constructed from an existing SparkContext. Additionally a window size for micro batches is required.","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498298_1876307996","id":"20170108-233042_1449790585","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Create a Streaming Context</h1>\n<p>Streaming is performed using a special StreamingContext, which can be constructed from an existing SparkContext. Additionally a window size for micro batches is required.</p>\n"},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153"},{"text":"import org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.Seconds\n\nval ssc = // YOUR CODE HERE","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498300_1873999503","id":"20170108-232906_1300339760","dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:154"},{"text":"%md\n# Connect to a Socket Stream\n\nAs an input stream, we simply read text data line by line from a socket. In order to create the sending stream, you can use the `pynetcat` program as follows\n\n    pynetcat.py -T -I1 -B10 -P9977 < alice-in-wonderland.txt\n\nAlternatively you can also stream from S3 using\n\n    s3netcat.py -T -I1 -B10 -P9977 s3://dimajix-training/data/alice/alice-in-wonderland.txt","dateUpdated":"2017-02-28T23:34:24-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498300_1873999503","id":"20170108-233037_315077291","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Connect to a Socket Stream</h1>\n<p>As an input stream, we simply read text data line by line from a socket. In order to create the sending stream, you can use the <code>pynetcat</code> program as follows</p>\n<pre><code>pynetcat.py -T -I1 -B10 -P9977 &lt; alice-in-wonderland.txt\n</code></pre>\n<p>Alternatively you can also stream from S3 using</p>\n<pre><code>s3netcat.py -T -I1 -B10 -P9977 s3://dimajix-training/data/alice/alice-in-wonderland.txt\n</code></pre>\n"},"dateCreated":"2017-02-28T23:31:38-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:155","dateFinished":"2017-02-28T23:34:21-0800","dateStarted":"2017-02-28T23:34:21-0800","focus":true},{"text":"val master = // YOUR CODE HERE\nval input = // YOUR CODE HERE","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498300_1873999503","id":"20170108-233230_1519381867","dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:156"},{"text":"%md\n## Execute Print Action\nFor every RDD received from the stream, we want to print it on the driver side. This can be achieved by attaching a `print` action to the raw stream.","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498300_1873999503","id":"20170218-143246_852432697","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Execute Print Action</h2>\n<p>For every RDD received from the stream, we want to print it on the driver side. This can be achieved by attaching a <code>print</code> action to the raw stream.</p>\n"},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:157"},{"text":"// YOUR CODE HERE","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498301_1873614754","id":"20170108-233431_2053188651","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:158"},{"text":"%md\n## Start Execution of SparkStreamingContext\nThis will trigger the action above (input.print) every second.","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498301_1873614754","id":"20170108-234433_1599037616","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Start Execution of SparkStreamingContext</h2>\n<p>This will trigger the action above (input.print) every second.</p>\n"},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:159"},{"text":"// YOUR CODE HERE","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498301_1873614754","id":"20170108-233454_1512277211","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:160"},{"text":"%md\n## Stop Execution.\nIn order to stop the output, we have to stop the StreamingContext. This will actually render the streaming context invalid. This means that we need to create a new SparkStreamingContext for the next steps.\n\nYou also can specify if the underlying SparkContext should also be destroyed. Since we do not want this to happen, we explicitly pass `false` as an argument to prevent the SparkContext itself from being stopped.","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498301_1873614754","id":"20170108-234511_818359632","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Stop Execution.</h2>\n<p>In order to stop the output, we have to stop the StreamingContext. This will actually render the streaming context invalid. This means that we need to create a new SparkStreamingContext for the next steps.</p>\n<p>You also can specify if the underlying SparkContext should also be destroyed. Since we do not want this to happen, we explicitly pass <code>false</code> as an argument to prevent the SparkContext itself from being stopped.</p>\n"},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:161"},{"text":"// YOUR CODE HERE","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498302_1874769000","id":"20170108-233556_1618251830","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:162"},{"text":"%md\n# Streaming WordCount\n\nNow let's start over again with a simple streaming word count example. We need to recreatre a SparkStreamingContext, because the stopped one cannot be reused any more. \n\nTo make things a little bit more interesting, we will also use a sliding window which is 10 seconds large and slides in 3 seconds intervals.","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498302_1874769000","id":"20170108-233616_1495710021","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Streaming WordCount</h1>\n<p>Now let's start over again with a simple streaming word count example. We need to recreatre a SparkStreamingContext, because the stopped one cannot be reused any more.</p>\n<p>To make things a little bit more interesting, we will also use a sliding window which is 10 seconds large and slides in 3 seconds intervals.</p>\n"},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:163"},{"text":"val ssc = // YOUR CODE HERE\nval input = // YOUR CODE HERE\n\n// YOUR CODE HERE\n","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498302_1874769000","id":"20170108-234623_1986387111","result":{"code":"SUCCESS","type":"TEXT","msg":"\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@6976def6\n\ninput: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@4afabded\n"},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:164"},{"text":"ssc.start()","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498302_1874769000","id":"20170108-234704_1086216784","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:165"},{"text":"ssc.stop(false)","dateUpdated":"2017-02-28T23:31:38-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353498303_1874384252","id":"20170108-234733_765755999","dateCreated":"2017-02-28T23:31:38-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:166"}],"name":"Spark RDD Streaming - Skeleton","id":"2C9KKSBND","angularObjects":{"2C73P7NJN:shared_process":[],"2C7KU6EWG:shared_process":[],"2C8H7AG7Q:shared_process":[],"2CANY5QMM:shared_process":[],"2C7YM9SBT:shared_process":[],"2CAHZM5EW:shared_process":[],"2CA194TC4:shared_process":[],"2C85Z5A3J:shared_process":[],"2C77BBC6M:shared_process":[],"2C9T8R64M:shared_process":[],"2C82H3SUX:shared_process":[],"2C7W1UTSM:shared_process":[],"2C99CAHNC:shared_process":[],"2C7VKNJZ3:shared_process":[],"2C7MNAP62:shared_process":[],"2C8SJ4SC1:shared_process":[],"2C8273BS9:shared_process":[],"2CA9V89Q3:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}