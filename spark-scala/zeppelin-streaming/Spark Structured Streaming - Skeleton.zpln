{"paragraphs":[{"text":"%md\n# 0. Add Dependencies\n\nBefore we start working with Spark and Kafka, we need to add the Spark Kafka SQL dependency to Zeppelin:\n```\norg.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4 exclude: net.jpountz.lz4:lz4\n```","dateUpdated":"2020-01-30T07:45:25+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>0. Add Dependencies</h1>\n<p>Before we start working with Spark and Kafka, we need to add the Spark Kafka SQL dependency to Zeppelin:</p>\n<pre><code>org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4 exclude: net.jpountz.lz4:lz4\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1580370138315_1839108603","id":"20180605-064303_775078356","dateCreated":"2020-01-30T07:42:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:26","user":"anonymous","dateFinished":"2020-01-30T07:45:26+0000","dateStarted":"2020-01-30T07:45:25+0000"},{"text":"%md\n# 1. Create Kafka Producer\n\nWe want to fill a Kafka topic with some data, for example we can fill Alice in Wonderland to a topic\n\n    spark-training/utils/s3cat.py -T -I1 -B10 s3://dimajix-training/data/alice/alice-in-wonderland.txt | /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic alice\n","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>1. Create Kafka Producer</h1>\n<p>We want to fill a Kafka topic with some data, for example we can fill Alice in Wonderland to a topic</p>\n<pre><code>spark-training/utils/s3cat.py -T -I1 -B10 s3://dimajix-training/data/alice/alice-in-wonderland.txt | /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic alice\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1580370138342_1828720383","id":"20170218-160028_195174762","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27"},{"text":"%md\n# 2. Connect to Data Source\n\nNow that we have a Kafka producer up and running, we can connect to Kafka with Spark. We connect to the Kafka topic as the datasource by using the `DataStreamReader` API via `spark.readStream`. We need to specify the options `kafka.bootstrap.servers` and `subscribe` and we need to use the format `kafka` for connecting to the data source. The topic will stream data samples in raw format, i.e. one record per line.","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>2. Connect to Data Source</h1>\n<p>Now that we have a Kafka producer up and running, we can connect to Kafka with Spark. We connect to the Kafka topic as the datasource by using the <code>DataStreamReader</code> API via <code>spark.readStream</code>. We need to specify the options <code>kafka.bootstrap.servers</code> and <code>subscribe</code> and we need to use the format <code>kafka</code> for connecting to the data source. The topic will stream data samples in raw format, i.e. one record per line.</p>\n"}]},"apps":[],"jobName":"paragraph_1580370138351_1825257643","id":"20180605-064423_760896823","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28"},{"text":"// Fill in the correct AWS VPC address of your master host\nval master = \"kku.training.dimajix-aws.net\"\n\n// Connect to raw text stream socket using the DataStreamReader API via spark.readStream. You need to specify the options `kafka.bootstrap.servers`, `subscribe` and you need to use the format `kafka`\nval lines = ...","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138360_1832567872","id":"20170218-160002_129671727","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:29"},{"text":"%md\n## 2.1 Inspect Schema\n\nThe result of the load method is a `DataFrame` again, but a streaming one. This `DataFrame` again has a schema, which we can inspect with the usual method:","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>2.1 Inspect Schema</h2>\n<p>The result of the load method is a <code>DataFrame</code> again, but a streaming one. This <code>DataFrame</code> again has a schema, which we can inspect with the usual method:</p>\n"}]},"apps":[],"jobName":"paragraph_1580370138366_1831798374","id":"20170218-161504_128762750","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:30"},{"text":"lines.printSchema()","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138374_1914904137","id":"20170218-160206_920542952","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:31"},{"text":"%md\n## 2.2 Extract Timestamp\n\nThis time a timestamp is attached to every line. We want to extract the timestamp and handle it separately.","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>2.2 Extract Timestamp</h2>\n<p>This time a timestamp is attached to every line. We want to extract the timestamp and handle it separately.</p>\n"}]},"apps":[],"jobName":"paragraph_1580370138383_1911441397","id":"20170218-165528_130307776","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32"},{"text":"import org.apache.spark.sql.types.TimestampType\n\nval ts_lines = ...","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138391_1920675370","id":"20170218-165525_1370327040","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:33"},{"text":"ts_lines.printSchema","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138431_1905285414","id":"20170218-165602_1908375523","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:34"},{"text":"%md\n# 3. Inspect Data\n\nOf course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke `show`, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.\n\nIn order to create a continuous query, we need to perform the following steps\n\n1. Create a `DataStreamWriter` by using the `writeStream` method of a DataFrame\n2. Specify the output format. We use `console` in our case\n3. Specify a checkpoint location on HDFS. This is required for restarting\n4. Optionally specify a processing period\n5. Start the query\n6. For Zeppelin only: Sleep a little bit, or we miss the output","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>3. Inspect Data</h1>\n<p>Of course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke <code>show</code>, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.</p>\n<p>In order to create a continuous query, we need to perform the following steps</p>\n<ol>\n<li>Create a <code>DataStreamWriter</code> by using the <code>writeStream</code> method of a DataFrame</li>\n<li>Specify the output format. We use <code>console</code> in our case</li>\n<li>Specify a checkpoint location on HDFS. This is required for restarting</li>\n<li>Optionally specify a processing period</li>\n<li>Start the query</li>\n<li>For Zeppelin only: Sleep a little bit, or we miss the output</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1580370138440_1887971714","id":"20170218-161603_528321172","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35"},{"text":"import org.apache.spark.sql.streaming.ProcessingTime\n\nval query = ...","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138449_1896820938","id":"20170218-160132_1015574950","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36"},{"text":"%md\n## 3.1 Stop Query\n\nIn contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the `stop` method on the query object. This makes working with streams much easier.","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>3.1 Stop Query</h2>\n<p>In contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the <code>stop</code> method on the query object. This makes working with streams much easier.</p>\n"}]},"apps":[],"jobName":"paragraph_1580370138456_1894127696","id":"20170218-161746_736484246","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37"},{"text":"","dateUpdated":"2020-01-30T07:42:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1580370138462_1893358198","id":"20170218-160155_661067655","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:38"},{"text":"%md\n# 4. Counting Words\n\nSo we now want to create a streaming word count. We perform the same actions as in the traditional DataFrame word count example, i.e. we split every line into words, group the words and count the sizes of the groups.","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>4. Counting Words</h1>\n<p>So we now want to create a streaming word count. We perform the same actions as in the traditional DataFrame word count example, i.e. we split every line into words, group the words and count the sizes of the groups.</p>\n"}]},"apps":[],"jobName":"paragraph_1580370138468_1877198745","id":"20170218-161837_1521722724","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:39"},{"text":"val words = ...","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138474_1876429247","id":"20170218-160238_1475524112","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:40"},{"text":"val counts = ...","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138479_1874505502","id":"20170218-160526_2008806660","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41"},{"text":"","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138484_1883354727","id":"20170218-162308_1835168589","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:42"},{"text":"%md\n## 4.1 Print Results onto Console\n\nAgain we want to print the results onto the console.","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>4.1 Print Results onto Console</h2>\n<p>Again we want to print the results onto the console.</p>\n"}]},"apps":[],"jobName":"paragraph_1580370138489_1881430982","id":"20170218-161041_735302634","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:43"},{"text":"val query = ...","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138493_1879891987","id":"20170218-160613_988660270","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:44"},{"text":"","dateUpdated":"2020-01-30T07:42:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1580370138498_1965690992","id":"20170218-160716_194527039","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:45"},{"text":"%md\n# 5. Time-Windowed Aggregation\n\nAnother interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the `groupBy` clause. In addition we also define a so called *watermark* which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example).","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>5. Time-Windowed Aggregation</h1>\n<p>Another interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the <code>groupBy</code> clause. In addition we also define a so called <em>watermark</em> which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example).</p>\n"}]},"apps":[],"jobName":"paragraph_1580370138502_1964151996","id":"20170218-164639_1471935006","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:46"},{"text":"val windowedCounts = ...","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138507_1962228251","id":"20170218-164703_2039341008","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:47"},{"text":"","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138511_1960689256","id":"20170218-165357_1483843188","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:48"},{"text":"%md\n## 5.1 Create Dynamic Table\n\nWe can also use a \"memory\" output, which is a queryable live table. In order to do so, we again create a new table, but this time with format `memory` and an explicit query name `aggregated_weather`. Using a `memory` output will create a dynamic table in memory (only `complete` output supported right now), which can be queried using SQL.\n\n1. Create a DataStreamWriter object using the writeStream method of your DataFrame `windowedCounts`.\n2. Set the format to `memory`\n3. Set the output mode to `append` (this is supported for time windowed aggregations)\n4. Set the query name to `alice_counts`\n5. Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)\n6. Start the continuous query via `start`","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>5.1 Create Dynamic Table</h2>\n<p>We can also use a &ldquo;memory&rdquo; output, which is a queryable live table. In order to do so, we again create a new table, but this time with format <code>memory</code> and an explicit query name <code>aggregated_weather</code>. Using a <code>memory</code> output will create a dynamic table in memory (only <code>complete</code> output supported right now), which can be queried using SQL.</p>\n<ol>\n<li>Create a DataStreamWriter object using the writeStream method of your DataFrame <code>windowedCounts</code>.</li>\n<li>Set the format to <code>memory</code></li>\n<li>Set the output mode to <code>append</code> (this is supported for time windowed aggregations)</li>\n<li>Set the query name to <code>alice_counts</code></li>\n<li>Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)</li>\n<li>Start the continuous query via <code>start</code></li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1580370138516_1969538481","id":"20170218-160934_1425298948","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49"},{"text":"val tableQuery = ...","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138521_1967614736","id":"20170218-161145_208429016","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:50"},{"text":"%md\n## 5.2 Perform Query\n\nNow that we have a dynamic table, we can perform SQL queries against this table as if it was a normal static table.","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>5.2 Perform Query</h2>\n<p>Now that we have a dynamic table, we can perform SQL queries against this table as if it was a normal static table.</p>\n"}]},"apps":[],"jobName":"paragraph_1580370138525_1966075740","id":"20170218-162010_1110788275","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:51"},{"text":"%sql\n","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"window","index":0,"aggr":"sum"}],"values":[{"name":"word","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"window","index":0,"aggr":"sum"},"yAxis":{"name":"word","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138530_1953379027","id":"20170218-161337_1103351127","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:52"},{"text":"%md\n## 5.3 Stop Query\n\nIn order to clean everything up, we stop the query again.","dateUpdated":"2020-01-30T07:42:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>5.3 Stop Query</h2>\n<p>In order to clean everything up, we stop the query again.</p>\n"}]},"apps":[],"jobName":"paragraph_1580370138535_1951455282","id":"20170218-162048_842521010","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"text":"","dateUpdated":"2020-01-30T07:42:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1580370138539_1949916287","id":"20170218-161329_1844676678","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"text":"","dateUpdated":"2020-01-30T07:42:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580370138543_1948377291","id":"20170218-162247_324928954","dateCreated":"2020-01-30T07:42:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"}],"name":"Spark Structured Streaming - Skeleton","id":"2F1VP2HT1","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}