{"paragraphs":[{"text":"%md\n# Create a Streaming Context\n\nStreaming is performed using a special StreamingContext, which can be constructed from an existing SparkContext. Additionally a window size for micro batches is required.","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527993_1758620805","id":"20170108-233042_1449790585","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Create a Streaming Context</h1>\n<p>Streaming is performed using a special StreamingContext, which can be constructed from an existing SparkContext. Additionally a window size for micro batches is required.</p>\n"},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4676"},{"text":"import org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.Seconds\n\nval ssc = new StreamingContext(sc, Seconds(1))","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527994_1759775052","id":"20170108-232906_1300339760","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.streaming.StreamingContext\n\nimport org.apache.spark.streaming.Seconds\n\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@4e6c8bd9\n"},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4677"},{"text":"%md\n# Connect to a Socket Stream\n\nAs an input stream, we simply read text data line by line from a socket. In order to create the sending stream, you can use the `pynetcat` program as follows\n\n    pynetcat.py -T -I1 -B10 -P9977 < alice-in-wonderland.txt\n\nAlternatively you can also stream from S3 using\n\n    s3netcat.py -T -I1 -B10 -P9977 s3://dimajix-training/data/alice/alice-in-wonderland.txt","dateUpdated":"2017-02-28T23:34:12-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527994_1759775052","id":"20170108-233037_315077291","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Connect to a Socket Stream</h1>\n<p>As an input stream, we simply read text data line by line from a socket. In order to create the sending stream, you can use the <code>pynetcat</code> program as follows</p>\n<pre><code>pynetcat.py -T -I1 -B10 -P9977 &lt; alice-in-wonderland.txt\n</code></pre>\n<p>Alternatively you can also stream from S3 using</p>\n<pre><code>s3netcat.py -T -I1 -B10 -P9977 s3://dimajix-training/data/alice/alice-in-wonderland.txt\n</code></pre>\n"},"dateCreated":"2017-02-28T23:32:07-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4678","dateFinished":"2017-02-28T23:34:09-0800","dateStarted":"2017-02-28T23:34:09-0800","focus":true},{"text":"val master = \"10.200.101.118\"\nval input = ssc.socketTextStream(master, 9977)","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527994_1759775052","id":"20170108-233230_1519381867","result":{"code":"SUCCESS","type":"TEXT","msg":"\ninput: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@686df68\n"},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4679"},{"text":"%md\n## Execute Print Action\nFor every RDD received from the stream, we want to print it on the driver side. This can be achieved by attaching a `print` action to the raw stream.","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527995_1759390303","id":"20170218-143142_684022571","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Execute Print Action</h2>\n<p>For every RDD received from the stream, we want to print it on the driver side. This can be achieved by attaching a <code>print</code> action to the raw stream.</p>\n"},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4680"},{"text":"input.print()","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527995_1759390303","id":"20170108-233431_2053188651","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4681"},{"text":"%md\n## Start Execution of SparkStreamingContext\nThis will trigger the action above (input.print) every second.","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527995_1759390303","id":"20170108-234433_1599037616","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Start Execution of SparkStreamingContext</h2>\n<p>This will trigger the action above (input.print) every second.</p>\n"},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4682"},{"text":"ssc.start()","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527996_1757466558","id":"20170108-233454_1512277211","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4683"},{"text":"%md\n## Stop Execution.\nIn order to stop the output, we have to stop the StreamingContext. This will actually render the streaming context invalid. This means that we need to create a new SparkStreamingContext for the next steps.\n\nYou also can specify if the underlying SparkContext should also be destroyed. Since we do not want this to happen, we explicitly pass `false` as an argument to prevent the SparkContext itself from being stopped.","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527996_1757466558","id":"20170108-234511_818359632","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Stop Execution.</h2>\n<p>In order to stop the output, we have to stop the StreamingContext. This will actually render the streaming context invalid. This means that we need to create a new SparkStreamingContext for the next steps.</p>\n<p>You also can specify if the underlying SparkContext should also be destroyed. Since we do not want this to happen, we explicitly pass <code>false</code> as an argument to prevent the SparkContext itself from being stopped.</p>\n"},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4684"},{"text":"ssc.stop(false)","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527996_1757466558","id":"20170108-233556_1618251830","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4685"},{"text":"%md\n# Streaming WordCount\n\nNow let's start over again with a simple streaming word count example. We need to recreatre a SparkStreamingContext, because the stopped one cannot be reused any more. \n\nTo make things a little bit more interesting, we will also use a sliding window which is 10 seconds large and slides in 3 seconds intervals.","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527997_1757081810","id":"20170108-233616_1495710021","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Streaming WordCount</h1>\n<p>Now let's start over again with a simple streaming word count example. We need to recreatre a SparkStreamingContext, because the stopped one cannot be reused any more.</p>\n<p>To make things a little bit more interesting, we will also use a sliding window which is 10 seconds large and slides in 3 seconds intervals.</p>\n"},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4686"},{"text":"val ssc = new StreamingContext(sc, Seconds(1))\nval input = ssc.socketTextStream(master, 9977)\n\ninput.window(Seconds(10), Seconds(3))\n    .flatMap(_.split(\" \"))\n    .filter(_ != \"\")\n    .map(x => (x,1))\n    .reduceByKey(_ + _)\n    .transform(_.sortBy(_._2, ascending = false))\n    .print(20)\n","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527997_1757081810","id":"20170108-234623_1986387111","result":{"code":"SUCCESS","type":"TEXT","msg":"\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@6976def6\n\ninput: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@4afabded\n"},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4687"},{"text":"ssc.start()","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527997_1757081810","id":"20170108-234704_1086216784","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4688"},{"text":"ssc.stop(false)","dateUpdated":"2017-02-28T23:32:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1488353527998_1758236056","id":"20170108-234733_765755999","dateCreated":"2017-02-28T23:32:07-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4689"}],"name":"Spark RDD Streaming - Full","id":"2CABPV5FK","angularObjects":{"2C73P7NJN:shared_process":[],"2C7KU6EWG:shared_process":[],"2C8H7AG7Q:shared_process":[],"2CANY5QMM:shared_process":[],"2C7YM9SBT:shared_process":[],"2CAHZM5EW:shared_process":[],"2CA194TC4:shared_process":[],"2C85Z5A3J:shared_process":[],"2C77BBC6M:shared_process":[],"2C9T8R64M:shared_process":[],"2C82H3SUX:shared_process":[],"2C7W1UTSM:shared_process":[],"2C99CAHNC:shared_process":[],"2C7VKNJZ3:shared_process":[],"2C7MNAP62:shared_process":[],"2C8SJ4SC1:shared_process":[],"2C8273BS9:shared_process":[],"2CA9V89Q3:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}