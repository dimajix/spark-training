{"paragraphs":[{"text":"%md\n# 0. Add Dependencies\n\nBefore we start working with Spark and Kafka, we need to add the Spark Kafka SQL dependency to Zeppelin:\n```\norg.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0 exclude: net.jpountz.lz4:lz4\n```","dateUpdated":"2018-06-20T11:13:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>0. Add Dependencies</h1>\n<p>Before we start working with Spark and Kafka, we need to add the Spark Kafka SQL dependency to Zeppelin:</p>\n<pre><code>org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0 exclude: net.jpountz.lz4:lz4\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1529493207660_63311668","id":"20180605-062815_639825862","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:101"},{"text":"%md\n# 1. Create Kafka Producer\n\nWe want to fill a Kafka topic with some data, for example we can fill Alice in Wonderland to a topic\n\n    spark-training/utils/s3cat.py -T -I1 -B10 s3://dimajix-training/data/alice/alice-in-wonderland.txt | /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic alice\n","user":"anonymous","dateUpdated":"2018-06-20T13:23:09+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>1. Create Kafka Producer</h1>\n<p>We want to fill a Kafka topic with some data, for example we can fill Alice in Wonderland to a topic</p>\n<pre><code>spark-training/utils/s3cat.py -T -I1 -B10 s3://dimajix-training/data/alice/alice-in-wonderland.txt | /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic alice\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1529493207686_153342911","id":"20170218-160028_195174762","dateCreated":"2018-06-20T11:13:27+0000","dateStarted":"2018-06-20T13:23:09+0000","dateFinished":"2018-06-20T13:23:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"text":"%md\n# 2. Connect to Data Source\n\nNow that we have a Kafka producer up and running, we can connect to Kafka with Spark. We connect to the Kafka topic as the datasource by using the `DataStreamReader` API via `spark.readStream`. We need to specify the options `kafka.bootstrap.servers` and `subscribe` and we need to use the format `kafka` for connecting to the data source. The topic will stream data samples in raw format, i.e. one record per line.","dateUpdated":"2018-06-20T11:13:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>2. Connect to Data Source</h1>\n<p>Now that we have a Kafka producer up and running, we can connect to Kafka with Spark. We connect to the Kafka topic as the datasource by using the <code>DataStreamReader</code> API via <code>spark.readStream</code>. We need to specify the options <code>kafka.bootstrap.servers</code> and <code>subscribe</code> and we need to use the format <code>kafka</code> for connecting to the data source. The topic will stream data samples in raw format, i.e. one record per line.</p>\n"}]},"apps":[],"jobName":"paragraph_1529493207690_151803915","id":"20180605-062713_1171805611","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"text":"// Fill in the correct AWS VPC address of your master host\nval master = \"kku.training.dimajix-aws.net\"\n\n// Connect to raw text stream socket using the DataStreamReader API via spark.readStream. You need to specify the options `host`, `port` and you need to use the format `socket`\nval lines = spark.readStream\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", master + \":9092\")\n  .option(\"subscribe\", \"alice\")\n  .option(\"startingOffsets\", \"earliest\")\n  .load()","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"master: String = kku.training.dimajix-aws.net\nlines: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1529493207692_149495422","id":"20170218-160002_129671727","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"text":"%md\n## 2.1 Inspect Schema\n\nThe result of the load method is a `DataFrame` again, but a streaming one. This `DataFrame` again has a schema, which we can inspect with the usual method:","dateUpdated":"2018-06-20T11:13:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>2.1 Inspect Schema</h2>\n<p>The result of the load method is a <code>DataFrame</code> again, but a streaming one. This <code>DataFrame</code> again has a schema, which we can inspect with the usual method:</p>\n"}]},"apps":[],"jobName":"paragraph_1529493207696_160268391","id":"20170218-161504_128762750","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:105"},{"text":"lines.printSchema()","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- key: binary (nullable = true)\n |-- value: binary (nullable = true)\n |-- topic: string (nullable = true)\n |-- partition: integer (nullable = true)\n |-- offset: long (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- timestampType: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1529493207700_158729395","id":"20170218-160206_920542952","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:106"},{"text":"%md\n## 2.2 Extract Timestamp\n\nThis time a timestamp is attached to every line. We want to extract the timestamp and handle it separately.","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>2.2 Extract Timestamp</h2>\n<p>This time a timestamp is attached to every line. We want to extract the timestamp and handle it separately.</p>\n"}]},"apps":[],"jobName":"paragraph_1529493207704_157190400","id":"20170218-165528_130307776","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:107"},{"text":"import org.apache.spark.sql.types.TimestampType\n\nval ts_lines = lines.withColumn(\"ts_line\", split(lines(\"value\"), \"\\t\"))\n    .withColumn(\"ts\", col(\"ts_line\")(0).cast(TimestampType))\n    .withColumn(\"line\", col(\"ts_line\")(1))\n    .drop(\"ts_line\")\n    .drop(\"value\")","dateUpdated":"2018-06-20T13:49:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types.TimestampType\nts_lines: org.apache.spark.sql.DataFrame = [ts: timestamp, line: string]\n"}]},"apps":[],"jobName":"paragraph_1529493207708_155651404","id":"20170218-165525_1370327040","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"text":"ts_lines.printSchema","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- ts: timestamp (nullable = true)\n |-- line: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1529493207711_156036153","id":"20170218-165602_1908375523","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"text":"%md\n# 3. Inspect Data\n\nOf course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke `show`, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.\n\nIn order to create a continuous query, we need to perform the following steps\n\n1. Create a `DataStreamWriter` by using the `writeStream` method of a DataFrame\n2. Specify the output format. We use `console` in our case\n3. Specify a checkpoint location on HDFS. This is required for restarting\n4. Optionally specify a processing period\n5. Start the query\n6. For Zeppelin only: Sleep a little bit, or we miss the output","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>3. Inspect Data</h1>\n<p>Of course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke <code>show</code>, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.</p>\n<p>In order to create a continuous query, we need to perform the following steps</p>\n<ol>\n<li>Create a <code>DataStreamWriter</code> by using the <code>writeStream</code> method of a DataFrame</li>\n<li>Specify the output format. We use <code>console</code> in our case</li>\n<li>Specify a checkpoint location on HDFS. This is required for restarting</li>\n<li>Optionally specify a processing period</li>\n<li>Start the query</li>\n<li>For Zeppelin only: Sleep a little bit, or we miss the output</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1529493207714_142569941","id":"20170218-161603_528321172","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"import org.apache.spark.sql.streaming.ProcessingTime\n\nval query = ts_lines.writeStream\n    .format(\"console\")\n    .outputMode(\"append\")\n    .trigger(ProcessingTime(\"2 seconds\"))\n    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-alice-print-\" + System.currentTimeMillis())\n    .start()\n    \nThread.sleep(20000)","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":213.15,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.streaming.ProcessingTime\nwarning: there was one deprecation warning; re-run with -deprecation for details\nquery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@321bd59\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+--------------------+--------------------+\n|                  ts|                line|\n+--------------------+--------------------+\n|2018-06-05 06:35:...|[Last updated: De...|\n|2018-06-05 06:35:...|ALICE'S ADVENTURE...|\n|2018-06-05 06:35:...|                    |\n|2018-06-05 06:35:...|after it, and for...|\n|2018-06-05 06:35:...|cupboards and boo...|\n|2018-06-05 06:35:...|things of this so...|\n|2018-06-05 06:35:...|ask: perhaps I sh...|\n|2018-06-05 06:35:...|                    |\n|2018-06-05 06:35:...|Suddenly she came...|\n|2018-06-05 06:35:...|shut up like a te...|\n|2018-06-05 06:35:...|little histories ...|\n|2018-06-05 06:35:...|                    |\n|2018-06-05 06:35:...|After a while, fi...|\n|2018-06-05 06:35:...|'But it's no use ...|\n|2018-06-05 06:35:...|  in the common way.|\n|2018-06-05 06:35:...|for the moment sh...|\n|2018-06-05 06:35:...|         NEAR THE...|\n|2018-06-05 06:35:...|tears, until ther...|\n|2018-06-05 06:35:...|queer everything ...|\n|2018-06-05 06:35:...|London is the cap...|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+--------------------+--------------------+\n|                  ts|                line|\n+--------------------+--------------------+\n|2018-06-05 06:36:...|Alice,) 'Well, I ...|\n|2018-06-05 06:36:...|window, and some ...|\n|2018-06-05 06:36:...|appeared; but she...|\n|2018-06-05 06:36:...|said Alice, in a ...|\n|2018-06-05 06:36:...|all the while, ti...|\n|2018-06-05 06:36:...|the flowers and t...|\n|2018-06-05 06:36:...|                    |\n|2018-06-05 06:36:...|like a Jack-in-th...|\n|2018-06-05 06:36:...|which produced an...|\n|2018-06-05 06:36:...|                    |\n|2018-06-05 06:36:...|hungry, in which ...|\n|2018-06-05 06:36:...|                    |\n|2018-06-05 06:36:...|There was a large...|\n|2018-06-05 06:36:...|at last the Cater...|\n|2018-06-05 06:36:...|now--Don't choke ...|\n|2018-06-05 06:36:...|Alice heard the R...|\n|2018-06-05 06:36:...|the door, she ran...|\n|2018-06-05 06:36:...|        great hurry.|\n|2018-06-05 06:36:...|very like having ...|\n|2018-06-05 06:36:...|suppose I ought t...|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+--------------------+--------------------+\n|                  ts|                line|\n+--------------------+--------------------+\n|2018-06-05 06:36:...|   myself, you see.'|\n|2018-06-05 06:36:...|                    |\n|2018-06-05 06:36:...|                    |\n|2018-06-05 06:36:...|arms, took the ho...|\n|2018-06-05 06:36:...|'I don't see,' sa...|\n|2018-06-05 06:36:...|is, it would feel...|\n|2018-06-05 06:36:...|            to say!'|\n|2018-06-05 06:37:...|                    |\n|2018-06-05 06:36:...|'What do you mean...|\n|2018-06-05 06:36:...|after that into a...|\n|2018-06-05 06:36:...|                    |\n|2018-06-05 06:36:...|                    |\n|2018-06-05 06:36:...|                    |\n|2018-06-05 06:36:...|have to turn into...|\n|2018-06-05 06:36:...|'Why?' said the C...|\n|2018-06-05 06:36:...|'No,' said the Ca...|\n|2018-06-05 06:36:...|who I WAS when I ...|\n|2018-06-05 06:36:...|                    |\n|2018-06-05 06:36:...|you ought to tell...|\n|2018-06-05 06:36:...|              could.|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1529493207716_140261448","id":"20170218-160132_1015574950","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"text":"%md\n## 3.1 Stop Query\n\nIn contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the `stop` method on the query object. This makes working with streams much easier.","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>3.1 Stop Query</h2>\n<p>In contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the <code>stop</code> method on the query object. This makes working with streams much easier.</p>\n"}]},"apps":[],"jobName":"paragraph_1529493207717_139876699","id":"20170218-161746_736484246","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"text":"query.stop","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1529493207719_140646197","id":"20170218-160155_661067655","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"text":"%md\n# 4. Counting Words\n\nSo we now want to create a streaming word count. We perform the same actions as in the traditional DataFrame word count example, i.e. we split every line into words, group the words and count the sizes of the groups.","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>4. Counting Words</h1>\n<p>So we now want to create a streaming word count. We perform the same actions as in the traditional DataFrame word count example, i.e. we split every line into words, group the words and count the sizes of the groups.</p>\n"}]},"apps":[],"jobName":"paragraph_1529493207720_138722452","id":"20170218-161837_1521722724","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"text":"val words = ts_lines.select(\n    ts_lines(\"ts\"), \n    explode(split(ts_lines(\"line\"),\" \")).as(\"word\")\n)","dateUpdated":"2018-06-20T14:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"words: org.apache.spark.sql.DataFrame = [ts: timestamp, word: string]\n"}]},"apps":[],"jobName":"paragraph_1529493207722_139491950","id":"20170218-160238_1475524112","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"text":"val counts = words\n    .filter($\"word\" =!= lit(\"\"))\n    .groupBy(\"word\")\n    .agg(sum(lit(1)).as(\"count\"))","dateUpdated":"2018-06-20T14:11:11+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\ncounts: org.apache.spark.sql.DataFrame = [word: string, count: bigint]\n"}]},"apps":[],"jobName":"paragraph_1529493207723_137183457","id":"20170218-160526_2008806660","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116"},{"text":"counts.printSchema","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- word: string (nullable = true)\n |-- count: long (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1529493207725_136798708","id":"20170218-162308_1835168589","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:117"},{"text":"%md\n## 4.1 Print Results onto Console\n\nAgain we want to print the results onto the console.","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>4.1 Print Results onto Console</h2>\n<p>Again we want to print the results onto the console.</p>\n"}]},"apps":[],"jobName":"paragraph_1529493207727_137568206","id":"20170218-161041_735302634","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:118"},{"text":"val query = counts.writeStream\n    .format(\"console\")\n    .outputMode(\"complete\")\n    .trigger(ProcessingTime(\"1 seconds\"))\n    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-alice-count-\" + System.currentTimeMillis())\n    .start()\n    \nThread.sleep(20000)","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nquery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@40f157b4\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+--------+-----+\n|    word|count|\n+--------+-----+\n| enough;|    2|\n|    'And|    8|\n|  dear!'|    2|\n|  waters|    1|\n|   high:|    2|\n|  youth,|    3|\n|more--As|    1|\n|   still|    6|\n|      By|    2|\n|   ('the|    1|\n|   those|    6|\n| flashed|    1|\n|  LITTLE|    2|\n|replied;|    1|\n|  mouse:|    1|\n|chatte?'|    1|\n|   'Fury|    1|\n|    hope|    3|\n|    way!|    1|\n|  hoped)|    1|\n+--------+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1529493207728_147956426","id":"20170218-160613_988660270","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:119"},{"text":"query.stop()","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1529493207730_148725924","id":"20170218-160716_194527039","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:120"},{"text":"%md\n# 5. Time-Windowed Aggregation\n\nAnother interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the `groupBy` clause. In addition we also define a so called *watermark* which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example).","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>5. Time-Windowed Aggregation</h1>\n<p>Another interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the <code>groupBy</code> clause. In addition we also define a so called <em>watermark</em> which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example).</p>\n"}]},"apps":[],"jobName":"paragraph_1529493207731_148341175","id":"20170218-164639_1471935006","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:121"},{"text":"val windowedCounts = words\n    .withWatermark(\"ts\", \"10 seconds\")\n    .filter($\"word\" !== lit(\"\"))\n    .groupBy(window($\"ts\", \"5 seconds\", \"1 seconds\"), $\"word\")\n    .agg(sum(lit(1)).as(\"count\"))","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nwindowedCounts: org.apache.spark.sql.DataFrame = [window: struct<start: timestamp, end: timestamp>, word: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1529493207735_146802179","id":"20170218-164703_2039341008","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:122"},{"text":"windowedCounts.printSchema()","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- window: struct (nullable = true)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- word: string (nullable = true)\n |-- count: long (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1529493207740_143339439","id":"20170218-165357_1483843188","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:123"},{"text":"%md\n## 5.1 Create Dynamic Table\n\nWe can also use a \"memory\" output, which is a queryable live table. In order to do so, we again create a new table, but this time with format `memory` and an explicit query name `aggregated_weather`. Using a `memory` output will create a dynamic table in memory (only `complete` output supported right now), which can be queried using SQL.\n\n1. Create a DataStreamWriter object using the writeStream method of your DataFrame `windowedCounts`.\n2. Set the format to `memory`\n3. Set the output mode to `append` (this is supported for time windowed aggregations)\n4. Set the query name to `alice_counts`\n5. Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)\n6. Start the continuous query via `start`","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>5.1 Create Dynamic Table</h2>\n<p>We can also use a &ldquo;memory&rdquo; output, which is a queryable live table. In order to do so, we again create a new table, but this time with format <code>memory</code> and an explicit query name <code>aggregated_weather</code>. Using a <code>memory</code> output will create a dynamic table in memory (only <code>complete</code> output supported right now), which can be queried using SQL.</p>\n<ol>\n<li>Create a DataStreamWriter object using the writeStream method of your DataFrame <code>windowedCounts</code>.</li>\n<li>Set the format to <code>memory</code></li>\n<li>Set the output mode to <code>append</code> (this is supported for time windowed aggregations)</li>\n<li>Set the query name to <code>alice_counts</code></li>\n<li>Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)</li>\n<li>Start the continuous query via <code>start</code></li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1529493207744_129488479","id":"20170218-160934_1425298948","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:124"},{"text":"val tableQuery = windowedCounts.writeStream\n    .outputMode(\"append\")\n    .trigger(ProcessingTime(\"1 seconds\"))\n    .format(\"memory\")\n    .queryName(\"alice_counts\")\n    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-alice-table-\" + System.currentTimeMillis())\n    .start()","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\ntableQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@75babd08\n"}]},"apps":[],"jobName":"paragraph_1529493207749_127564734","id":"20170218-161145_208429016","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:125"},{"text":"%md\n## 5.2 Perform Query\n\nNow that we have a dynamic table, we can perform SQL queries against this table as if it was a normal static table.","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>5.2 Perform Query</h2>\n<p>Now that we have a dynamic table, we can perform SQL queries against this table as if it was a normal static table.</p>\n"}]},"apps":[],"jobName":"paragraph_1529493207753_126025739","id":"20170218-162010_1110788275","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:126"},{"text":"%sql\nselect \n    * \nfrom alice_counts\norder by count desc\nlimit 50","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"window","index":0,"aggr":"sum"}],"values":[{"name":"word","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"window","index":0,"aggr":"sum"},"yAxis":{"name":"word","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"window\tword\tcount\n[2018-06-05 06:35:52.0,2018-06-05 06:35:57.0]\tthe\t37\n[2018-06-05 06:37:21.0,2018-06-05 06:37:26.0]\tthe\t34\n[2018-06-05 06:35:51.0,2018-06-05 06:35:56.0]\tthe\t33\n[2018-06-05 06:35:50.0,2018-06-05 06:35:55.0]\tthe\t33\n[2018-06-05 06:37:22.0,2018-06-05 06:37:27.0]\tthe\t33\n[2018-06-05 06:36:48.0,2018-06-05 06:36:53.0]\tthe\t32\n[2018-06-05 06:36:49.0,2018-06-05 06:36:54.0]\tthe\t32\n[2018-06-05 06:35:53.0,2018-06-05 06:35:58.0]\tthe\t32\n[2018-06-05 06:36:10.0,2018-06-05 06:36:15.0]\tthe\t31\n[2018-06-05 06:38:41.0,2018-06-05 06:38:46.0]\tthe\t31\n[2018-06-05 06:36:31.0,2018-06-05 06:36:36.0]\tthe\t31\n[2018-06-05 06:36:50.0,2018-06-05 06:36:55.0]\tthe\t30\n[2018-06-05 06:38:40.0,2018-06-05 06:38:45.0]\tthe\t30\n[2018-06-05 06:37:23.0,2018-06-05 06:37:28.0]\tthe\t30\n[2018-06-05 06:37:53.0,2018-06-05 06:37:58.0]\tthe\t30\n[2018-06-05 06:36:47.0,2018-06-05 06:36:52.0]\tthe\t30\n[2018-06-05 06:37:54.0,2018-06-05 06:37:59.0]\tthe\t30\n[2018-06-05 06:38:35.0,2018-06-05 06:38:40.0]\tthe\t29\n[2018-06-05 06:35:58.0,2018-06-05 06:36:03.0]\tshe\t29\n[2018-06-05 06:38:42.0,2018-06-05 06:38:47.0]\tthe\t29\n[2018-06-05 06:38:34.0,2018-06-05 06:38:39.0]\tthe\t29\n[2018-06-05 06:36:46.0,2018-06-05 06:36:51.0]\tthe\t29\n[2018-06-05 06:38:14.0,2018-06-05 06:38:19.0]\tthe\t29\n[2018-06-05 06:36:09.0,2018-06-05 06:36:14.0]\tthe\t29\n[2018-06-05 06:38:39.0,2018-06-05 06:38:44.0]\tthe\t29\n[2018-06-05 06:35:36.0,2018-06-05 06:35:41.0]\tthe\t29\n[2018-06-05 06:38:23.0,2018-06-05 06:38:28.0]\tthe\t29\n[2018-06-05 06:37:20.0,2018-06-05 06:37:25.0]\tthe\t29\n[2018-06-05 06:35:55.0,2018-06-05 06:36:00.0]\tand\t28\n[2018-06-05 06:36:11.0,2018-06-05 06:36:16.0]\tthe\t28\n[2018-06-05 06:38:24.0,2018-06-05 06:38:29.0]\tthe\t28\n[2018-06-05 06:38:36.0,2018-06-05 06:38:41.0]\tthe\t28\n[2018-06-05 06:37:47.0,2018-06-05 06:37:52.0]\tthe\t28\n[2018-06-05 06:36:12.0,2018-06-05 06:36:17.0]\tthe\t28\n[2018-06-05 06:35:38.0,2018-06-05 06:35:43.0]\tthe\t28\n[2018-06-05 06:37:19.0,2018-06-05 06:37:24.0]\tthe\t28\n[2018-06-05 06:36:06.0,2018-06-05 06:36:11.0]\tand\t28\n[2018-06-05 06:37:24.0,2018-06-05 06:37:29.0]\tthe\t28\n[2018-06-05 06:37:46.0,2018-06-05 06:37:51.0]\tthe\t28\n[2018-06-05 06:37:55.0,2018-06-05 06:38:00.0]\tthe\t27\n[2018-06-05 06:38:17.0,2018-06-05 06:38:22.0]\tthe\t27\n[2018-06-05 06:38:16.0,2018-06-05 06:38:21.0]\tthe\t27\n[2018-06-05 06:36:07.0,2018-06-05 06:36:12.0]\tthe\t27\n[2018-06-05 06:35:54.0,2018-06-05 06:35:59.0]\tthe\t27\n[2018-06-05 06:36:13.0,2018-06-05 06:36:18.0]\tthe\t27\n[2018-06-05 06:37:30.0,2018-06-05 06:37:35.0]\tthe\t27\n[2018-06-05 06:38:32.0,2018-06-05 06:38:37.0]\tthe\t27\n[2018-06-05 06:36:32.0,2018-06-05 06:36:37.0]\tthe\t27\n[2018-06-05 06:37:45.0,2018-06-05 06:37:50.0]\tthe\t27\n[2018-06-05 06:38:11.0,2018-06-05 06:38:16.0]\tthe\t27\n"}]},"apps":[],"jobName":"paragraph_1529493207756_124871492","id":"20170218-161337_1103351127","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:127"},{"text":"%md\n## 5.3 Stop Query\n\nIn order to clean everything up, we stop the query again.","dateUpdated":"2018-06-20T11:13:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>5.3 Stop Query</h2>\n<p>In order to clean everything up, we stop the query again.</p>\n"}]},"apps":[],"jobName":"paragraph_1529493207760_135644461","id":"20170218-162048_842521010","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128"},{"text":"tableQuery.stop()","dateUpdated":"2018-06-20T11:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1529493207765_133720717","id":"20170218-161329_1844676678","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:129"},{"text":"","dateUpdated":"2018-06-20T11:13:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529493207769_132181721","id":"20170218-162247_324928954","dateCreated":"2018-06-20T11:13:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:130"}],"name":"Spark Structured Streaming - Full","id":"2DFRXCTN3","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}