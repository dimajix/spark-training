{"paragraphs":[{"text":"%md\n## Load Additional Libraries\nFirst we need to load some more Java/Scala libraries for stemming words. Either you can execute the following pararaph, or you need to manually add the following dependencies to the Zeppelin Spark Interpreter:\n* org.apache.opennlp:opennlp-tools:1.8.4","dateUpdated":"2018-03-21T06:09:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Load Additional Libraries</h2>\n<p>First we need to load some more Java/Scala libraries for stemming words. Either you can execute the following pararaph, or you need to manually add the following dependencies to the Zeppelin Spark Interpreter:</p>\n<ul>\n<li>org.apache.opennlp:opennlp-tools:1.8.4</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1521612562099_2117643734","id":"20180310-134706_381140984","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2520"},{"text":"%spark.dep\nz.load(\"org.apache.opennlp:opennlp-tools:1.8.4\")","dateUpdated":"2018-03-21T06:09:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562106_2114950492","id":"20180306-162025_898941184","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2521"},{"text":"%md\n# Load Data\n\nFirst we load data from HDFS. It is stored as a trivial CSV file with three columns\n\n    product name\n    review text\n    rating (1 - 5)\n\n","dateUpdated":"2018-03-21T06:09:22+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Load Data</h1>\n<p>First we load data from HDFS. It is stored as a trivial CSV file with three columns</p>\n<pre><code>product name\nreview text\nrating (1 - 5)\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1521612562107_2114565743","id":"20170129-045139_1503434840","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2522"},{"text":"val basedir = \"s3://dimajix-training/data/\"","dateUpdated":"2018-03-21T06:09:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562108_2112641998","id":"20170129-091247_907732707","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2523"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.functions.col\n\n// Explicitly build the schema as we expect it in the CSV file. Note that we also assume \"rating\" to be string type. This saves\n// us from errors due to wrong entries. Casting a String to something else later only produces SQL NULL values, but no errors.\nval schema = StructType(\n        StructField(\"name\", StringType) ::\n        StructField(\"review\", StringType) ::\n        StructField(\"rating\", StringType) ::\n        Nil\n    )\n// Read in CSV data and cast rating column to integer    \nval data = sqlContext.read\n    .schema(schema)\n    .csv(basedir + \"amazon_baby\")\n    .withColumn(\"rating\", col(\"rating\").cast(IntegerType))\n    .filter(col(\"rating\").isNotNull)\nz.show(data.limit(10))","dateUpdated":"2018-03-21T06:09:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562109_2112257249","id":"20170129-045155_986374275","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2524"},{"text":"%md\n# Implement Transformer\n\nWe need a custom Transformer to build the pipeline. The transformer should remove all punctuations from a given column containing text.\n","dateUpdated":"2018-03-21T06:09:22+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Implement Transformer</h1>\n<p>We need a custom Transformer to build the pipeline. The transformer should remove all punctuations from a given column containing text.</p>\n"}]},"apps":[],"jobName":"paragraph_1521612562110_2113411496","id":"20170129-045225_1360438852","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2525"},{"text":"import org.apache.spark.ml.Transformer\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions.col\n\nclass PunctuationCleanupTransformer(override val uid: String) extends org.apache.spark.ml.Transformer {\n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val inputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"inputCol\", \"input column name\")\n    setDefault(inputCol, \"input\")\n    def getInputCol: String = $(inputCol)\n    def setInputCol(value:String): this.type = set(inputCol, value)\n\n    val outputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"outputCol\", \"output column name\")\n    setDefault(outputCol, \"output\")\n    def getOutputCol: String = $(outputCol)\n    def setOutputCol(value:String): this.type = set(outputCol, value)\n    \n    override def transform(dataset:org.apache.spark.sql.Dataset[_]) = {\n        val removePunctuation = org.apache.spark.sql.functions.udf { text:String => if (text != null) text.replaceAll(\"\\\\p{Punct}\", \" \") else \"\" }\n        dataset.withColumn(getOutputCol, removePunctuation(dataset(getInputCol)))\n    }\n    \n    override def transformSchema(schema:org.apache.spark.sql.types.StructType) = {\n        val outputColName = $(outputCol)\n        val outCol = org.apache.spark.sql.types.StructField(outputColName, org.apache.spark.sql.types.StringType)\n        \n        if (schema.fieldNames.contains(outputColName)) {\n          throw new IllegalArgumentException(s\"Output column $outputColName already exists.\")\n        }\n        org.apache.spark.sql.types.StructType(schema.fields :+ outCol)\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): PunctuationCleanupTransformer = {\n        val that = new PunctuationCleanupTransformer(uid)\n        copyValues(that, extra)\n        that\n    }\n}","dateUpdated":"2018-03-21T06:09:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562111_2113026747","id":"20170129-045233_1450874507","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2526"},{"text":"import opennlp.tools.stemmer.PorterStemmer\n\nclass PorterStemmerTransformer(override val uid: String) extends org.apache.spark.ml.Transformer {\n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val inputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"inputCol\", \"input column name\")\n    setDefault(inputCol, \"input\")\n    def getInputCol: String = $(inputCol)\n    def setInputCol(value:String): this.type = set(inputCol, value)\n\n    val outputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"outputCol\", \"output column name\")\n    setDefault(outputCol, \"output\")\n    def getOutputCol: String = $(outputCol)\n    def setOutputCol(value:String): this.type = set(outputCol, value)\n    \n    override def transform(dataset:org.apache.spark.sql.Dataset[_]) = {\n        val stemWord = org.apache.spark.sql.functions.udf { words:Seq[String] => \n            if (words != null) {\n                val stemmer = new opennlp.tools.stemmer.PorterStemmer()\n                words.map(stemmer.stem)\n            }\n            else {\n                Seq()\n            }\n        }\n        dataset.withColumn(getOutputCol, stemWord(dataset(getInputCol)))\n    }\n    \n    override def transformSchema(schema:org.apache.spark.sql.types.StructType) = {\n        val outputColName = $(outputCol)\n        val outCol = org.apache.spark.sql.types.StructField(outputColName, org.apache.spark.sql.types.ArrayType(org.apache.spark.sql.types.StringType))\n\n        if (schema.fieldNames.contains(outputColName)) {\n          throw new IllegalArgumentException(s\"Output column $outputColName already exists.\")\n        }\n        org.apache.spark.sql.types.StructType(schema.fields :+ outCol)\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): PorterStemmerTransformer = {\n        val that = new PorterStemmerTransformer(uid)\n        copyValues(that, extra)\n        that\n    }\n}\n","dateUpdated":"2018-03-21T06:09:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562112_2098791038","id":"20180306-172331_1459660287","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2527"},{"text":"%md\n# Model Evaluation\n\nAs in the original exercise, we want to use a custom metric for assessing the performance.\n","dateUpdated":"2018-03-21T06:09:22+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Model Evaluation</h1>\n<p>As in the original exercise, we want to use a custom metric for assessing the performance.</p>\n"}]},"apps":[],"jobName":"paragraph_1521612562112_2098791038","id":"20170129-045421_315570748","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2528"},{"text":"import org.apache.spark.sql.functions.col\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.ml.param.Param\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.evaluation.Evaluator\n\nclass AccuracyClassificationEvaluator(override val uid: String) extends org.apache.spark.ml.evaluation.Evaluator {\n    \n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val labelCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"labelCol\", \"label column name\")\n    setDefault(labelCol, \"label\")\n    def getLabelCol: String = $(labelCol)\n    def setLabelCol(value:String): this.type = set(labelCol, value)\n\n    val predictionCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"predictionCol\", \"prediction column name\")\n    setDefault(predictionCol, \"prediction\")\n    def getPredictionCol: String = $(predictionCol)\n    def setPredictionCol(value:String): this.type = set(predictionCol, value)\n\n    override def evaluate(dataset: org.apache.spark.sql.Dataset[_]) : Double = {\n        val num_total = dataset.count()\n        val num_correct = dataset.filter(dataset(getLabelCol) === dataset(getPredictionCol)).count()\n        num_correct.toDouble / num_total\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): AccuracyClassificationEvaluator = null // defaultCopy(extra)\n\n}","dateUpdated":"2018-03-21T06:09:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562113_2098406289","id":"20170129-045431_175924070","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2529"},{"text":"%md\n# Create ML Pipeline\n\nNow we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before\n\n* `PunctuationCleanupTransformer` - remove any punctuation\n* `SQLTransformer` - map rating to 0.0 or 1.0\n* `Tokenizer` - for splitting reviews into words\n* `StopWordRemover` - for removing stop words\n* `PorterStemmer` - stem words\n* `NGram` - for creating n-grams (tuples of consecutive words)\n* `CountVectorizer` - for creating bag-of-word features from the words\n* `IDF` - for creating a TF-IDF model from the raw counts\n* `LogisticRegression` - for creating the real model\n\nYou also need to perform the following transforms:\n* The incoming rating (1-5) needs to be mapped to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more `SQLTransformer` instances. You may want to look that up in the Spark documentation (org.apache.spark.ml.feature.SQLTransformer).\n* The Punctuations need to be removed. This can be done using the `PunctuationRemovalTransfomer`.\n\nThe logical order of feature transformation should be\n1. Remove punctuations\n2. Create sentiment\n3. Tokenize text data\n4. Remove stop words\n5. Stem words\n6. Create Bi-Grams (N-Grams with two words each)\n7. Count words per review\n8. Create TF-IDF model\n","dateUpdated":"2018-03-21T06:09:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Create ML Pipeline</h1>\n<p>Now we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before</p>\n<ul>\n  <li><code>PunctuationCleanupTransformer</code> - remove any punctuation</li>\n  <li><code>SQLTransformer</code> - map rating to 0.0 or 1.0</li>\n  <li><code>Tokenizer</code> - for splitting reviews into words</li>\n  <li><code>StopWordRemover</code> - for removing stop words</li>\n  <li><code>PorterStemmer</code> - stem words</li>\n  <li><code>NGram</code> - for creating n-grams (tuples of consecutive words)</li>\n  <li><code>CountVectorizer</code> - for creating bag-of-word features from the words</li>\n  <li><code>IDF</code> - for creating a TF-IDF model from the raw counts</li>\n  <li><code>LogisticRegression</code> - for creating the real model</li>\n</ul>\n<p>You also need to perform the following transforms:<br/>* The incoming rating (1-5) needs to be mapped to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more <code>SQLTransformer</code> instances. You may want to look that up in the Spark documentation (org.apache.spark.ml.feature.SQLTransformer).<br/>* The Punctuations need to be removed. This can be done using the <code>PunctuationRemovalTransfomer</code>.</p>\n<p>The logical order of feature transformation should be<br/>1. Remove punctuations<br/>2. Create sentiment<br/>3. Tokenize text data<br/>4. Remove stop words<br/>5. Stem words<br/>6. Create Bi-Grams (N-Grams with two words each)<br/>7. Count words per review<br/>8. Create TF-IDF model</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521612562114_2099560536","id":"20170129-045307_120063111","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2530"},{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification._\n\n// First let's load a list of stop words\nval stopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n\n// Now let's define an array of pipeline stages. Each stage (except the last one) is a transformer, and the last one if the LogisticRegression.\nval stages = Array(\n    new PunctuationCleanupTransformer()\n        .setInputCol(\"review\")\n        .setOutputCol(\"clean_review\"),\n    new SQLTransformer()\n        .setStatement(\"SELECT *,CASE WHEN rating < 3 THEN 0.0 ELSE 1.0 END AS sentiment FROM __THIS__ WHERE rating <> 3\"),\n    new Tokenizer()\n        .setInputCol(\"clean_review\")\n        .setOutputCol(\"words\"),\n    new StopWordsRemover()\n        .setInputCol(\"words\")\n        .setOutputCol(\"vwords\")\n        .setStopWords(stopWords),\n    new PorterStemmerTransformer()\n        .setInputCol(\"vwords\")\n        .setOutputCol(\"stems\"),\n    new NGram()\n        .setInputCol(\"stems\")\n        .setOutputCol(\"ngrams\")\n        .setN(2),\n    new CountVectorizer()\n        .setInputCol(\"ngrams\")\n        .setOutputCol(\"tf\")\n        .setMinDF(2.0),\n    new IDF()\n        .setInputCol(\"tf\")\n        .setOutputCol(\"features\"),\n    new LogisticRegression()\n        .setFeaturesCol(\"features\")\n        setLabelCol(\"sentiment\")\n)\nval pipe = new Pipeline().setStages(stages)","dateUpdated":"2018-03-21T06:09:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562114_2099560536","id":"20170129-045329_2143290843","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2531"},{"text":"%md\n# Hyper Parameter Tuning\n\nThe whole pipeline has some parameters which have an influence on the result, i.e. the accuracy. For example the size of the n-grams will probably have a big impact and also the minDF parameter of the CountVecttorizer will probably have some impact. These settings are called \"hyper parameters\", because they are also model parameters, but not learnt directly during the training phase. But which parameters work best?\n\nWe will use a CrossValidation to select the best set of hyperparameters.","dateUpdated":"2018-03-21T06:09:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Hyper Parameter Tuning</h1>\n<p>The whole pipeline has some parameters which have an influence on the result, i.e. the accuracy. For example the size of the n-grams will probably have a big impact and also the minDF parameter of the CountVecttorizer will probably have some impact. These settings are called &ldquo;hyper parameters&rdquo;, because they are also model parameters, but not learnt directly during the training phase. But which parameters work best?</p>\n<p>We will use a CrossValidation to select the best set of hyperparameters.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521612562115_2099175787","id":"20170129-045504_1499849390","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2532"},{"text":"// Inspect parameters of stage 5 (NGram)\n// YOUR CODE HERE","dateUpdated":"2018-03-21T06:11:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562116_2097252042","id":"20170129-045513_83457101","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2533"},{"text":"// Inspect parameters of stage 6 (CountVectorizer)\n// YOUR CODE HERE","dateUpdated":"2018-03-21T06:11:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562116_2097252042","id":"20180310-134959_107650993","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2534"},{"text":"%md\n## Create ParamGrid\n\nNow we create a param grid that should be used for using different sets of parameters. We want to tweak two parameters again:\n\n* NGRam sizes should take values in [2,3,5]\n* CounterVectoriter minimum document frequency (minDF) should take values in [1,2,3,5])\n\nIn order to create this grid, we first need to retrieve the corresponding stages from the pipeline, so we can access its parameters.","dateUpdated":"2018-03-21T06:09:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Create ParamGrid</h2>\n<p>Now we create a param grid that should be used for using different sets of parameters. We want to tweak two parameters again:</p>\n<ul>\n  <li>NGRam sizes should take values in [2,3,5]</li>\n  <li>CounterVectoriter minimum document frequency (minDF) should take values in [1,2,3,5])</li>\n</ul>\n<p>In order to create this grid, we first need to retrieve the corresponding stages from the pipeline, so we can access its parameters.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521612562117_2096867293","id":"20170129-045529_389742601","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2535"},{"text":"import org.apache.spark.ml.tuning.ParamGridBuilder\n\nval ngram = stages(5)\nval count = stages(6)\n\nval param_grid = // YOUR CODE HERE","dateUpdated":"2018-03-21T06:11:48+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562117_2096867293","id":"20170129-045537_1963157387","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2536"},{"text":"%md\n### Split Train Data / Test Data\n\nNow let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. The sampling can be done via the DataFrame method `randomSplit`.\n","dateUpdated":"2018-03-21T06:09:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Split Train Data / Test Data</h3>\n<p>Now let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. The sampling can be done via the DataFrame method <code>randomSplit</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1521612562118_2098021540","id":"20170129-045205_1227820739","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2537"},{"text":"val Array(trainingData, validationData) = data.randomSplit(Array(0.8,0.2), seed=1)\n\nprintln(s\"trainingData: ${trainingData.count()}\")\nprintln(s\"validationData: ${validationData.count()}\")","dateUpdated":"2018-03-21T06:09:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562119_2097636791","id":"20170129-045214_331567309","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2538"},{"text":"%md\n## Perform Hyper Parameter tuning using CrossValidator\n\nNow we can wrap the previous pipeline inside a CrossValidator which trains the model over and over again for all entries in the ParameterGrid. The CrossValidator works as a wrapper of the regression algorithm and will return a pipeline model. In order to evaluate the goodness of a fit, the CrossValidator also needs an Evaluator - we'll use our AccuracyClassificationEvaluator again.\n","dateUpdated":"2018-03-21T06:09:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Perform Hyper Parameter tuning using CrossValidator</h2>\n<p>Now we can wrap the previous pipeline inside a CrossValidator which trains the model over and over again for all entries in the ParameterGrid. The CrossValidator works as a wrapper of the regression algorithm and will return a pipeline model. In order to evaluate the goodness of a fit, the CrossValidator also needs an Evaluator - we&rsquo;ll use our AccuracyClassificationEvaluator again.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521612562119_2097636791","id":"20180305-194053_783571599","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2539"},{"text":"import org.apache.spark.ml.tuning.CrossValidator\n\n// Create instance of AccuracyClassificationEvaluator with labelCol set to the real sentiment column\nval evaluator = // YOUR CODE HERE\n\n// Create a CrossValidator instance\nval validator = // YOUR CODE HERE\n    \n// Fit model to pipeline\nval model = // YOUR CODE HERE","dateUpdated":"2018-03-21T06:12:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"msg":[{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1521612562120_2095713047","id":"20180305-194048_596318854","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2540"},{"text":"%md\n## Assess model performance\nAgain we want to evaluate the model that was trained using the pipeline and compare it the same simple baseline model which always predicts 'positive'","dateUpdated":"2018-03-21T06:09:22+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Assess model performance</h2>\n<p>Again we want to evaluate the model that was trained using the pipeline and compare it the same simple baseline model which always predicts 'positive'</p>\n"}]},"apps":[],"jobName":"paragraph_1521612562120_2095713047","id":"20170131-103049_1557536721","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2541"},{"text":"// Predict sentiment for test data\nval pred = // YOUR CODE HERE\nval always_positive = // YOUR CODE HERE\n\nval acc = // YOUR CODE HERE\nval baseline = // YOUR CODE HERE\nprintln(s\"Model Accuracy = ${acc}\")\nprintln(s\"Baseline Accuracy = ${baseline}\")\n","dateUpdated":"2018-03-21T06:12:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562121_2095328298","id":"20170129-045619_1930087860","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2542"},{"text":"","dateUpdated":"2018-03-21T06:09:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521612562122_2096482544","id":"20170129-101018_1552288438","dateCreated":"2018-03-21T06:09:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2543"}],"name":"Amazon Baby Products Hyper Parameters Exercise","id":"2D8KRMWYZ","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}