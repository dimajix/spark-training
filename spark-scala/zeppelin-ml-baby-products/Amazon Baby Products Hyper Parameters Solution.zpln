{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520689626486_903092841","id":"20180310-134706_381140984","dateCreated":"2018-03-10T13:47:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:38075","text":"%md\n## Load Additional Libraries\nFirst we need to load some more Java/Scala libraries for stemming words. Either you can execute the following pararaph, or you need to manually add the following dependencies to the Zeppelin Spark Interpreter:\n* org.apache.opennlp:opennlp-tools:1.8.4","dateUpdated":"2018-03-10T13:47:08+0000","dateFinished":"2018-03-10T13:47:08+0000","dateStarted":"2018-03-10T13:47:08+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Load Additional Libraries</h2>\n<p>First we need to load some more Java/Scala libraries for stemming words. Either you can execute the following pararaph, or you need to manually add the following dependencies to the Zeppelin Spark Interpreter:</p>\n<ul>\n<li>org.apache.opennlp:opennlp-tools:1.8.4</li>\n</ul>\n"}]}},{"text":"%spark.dep\nz.load(\"org.apache.opennlp:opennlp-tools:1.8.4\")","dateUpdated":"2018-03-10T13:46:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@2b02cccb\n"}]},"apps":[],"jobName":"paragraph_1520689580980_-713499444","id":"20180306-162025_898941184","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:36596"},{"text":"%md\n# Load Data\n\nFirst we load data from HDFS. It is stored as a trivial CSV file with three columns\n\n    product name\n    review text\n    rating (1 - 5)\n\n","dateUpdated":"2018-03-10T13:46:20+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Load Data</h1>\n<p>First we load data from HDFS. It is stored as a trivial CSV file with three columns</p>\n<pre><code>product name\nreview text\nrating (1 - 5)\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1520689580980_-713499444","id":"20170129-045139_1503434840","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36598"},{"text":"val basedir = \"s3://dimajix-training/data/\"","dateUpdated":"2018-03-10T13:47:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"basedir: String = s3://dimajix-training/data/\n"}]},"apps":[],"jobName":"paragraph_1520689580980_-713499444","id":"20170129-091247_907732707","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36599","user":"anonymous","dateFinished":"2018-03-10T13:47:21+0000","dateStarted":"2018-03-10T13:47:21+0000"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.functions.col\n\n\nval schema = StructType(\n        StructField(\"name\", StringType) ::\n        StructField(\"review\", StringType) ::\n        StructField(\"rating\", StringType) ::\n        Nil\n    )\nval data = sqlContext.read\n    .schema(schema)\n    .csv(basedir + \"amazon_baby\")\n    .withColumn(\"rating\", col(\"rating\").cast(IntegerType))\n    .filter(col(\"rating\").isNotNull)\nz.show(data.limit(10))","dateUpdated":"2018-03-10T13:47:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.functions.col\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(review,StringType,true), StructField(rating,StringType,true))\ndata: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, review: string ... 1 more field]\n"},{"type":"TABLE","data":"name\treview\trating\nPlanetwise Flannel Wipes\tThese flannel wipes are OK, but in my opinion not worth keeping.  I also ordered someImse Vimse Cloth Wipes-Ocean Blue-12 countwhich are larger, had a nicer, softer texture and just seemed higher quality.  I use cloth wipes for hands and faces and have been usingThirsties 6 Pack Fab Wipes, Boyfor about 8 months now and need to replace them because they are starting to get rough and have had stink issues for a while that stripping no longer handles.\t3\nPlanetwise Wipe Pouch\tit came early and was not disappointed. i love planet wise bags and now my wipe holder. it keps my osocozy wipes moist and does not leak. highly recommend it.\t5\nAnnas Dream Full Quilt with 2 Shams\tVery soft and comfortable and warmer than it looks...fit the full size bed perfectly...would recommend to anyone looking for this type of quilt\t5\nStop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\tThis is a product well worth the purchase.  I have not found anything else like this, and it is a positive, ingenious approach to losing the binky.  What I love most about this product is how much ownership my daughter has in getting rid of the binky.  She is so proud of herself, and loves her little fairy.  I love the artwork, the chart in the back, and the clever approach of this tool.\t5\nStop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\tAll of my kids have cried non-stop when I tried to ween them off their pacifier, until I found Thumbuddy To Love's Binky Fairy Puppet.  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from it.This is a must buy book, and a great gift for expecting parents!!  You will save them soo many headaches.Thanks for this book!  You all rock!!\t5\nStop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\tWhen the Binky Fairy came to our house, we didn't have any special gift and book to help explain to her about how important it is to stop using a pacifier. This book does a great job to help prepare your child for the loss of their favorite item. The doll is adorable and we made lots of cute movies with the Binky Fairy telling our daughter about what happens when the Binky Fairy comes. I would highly recommend this product for any parent trying to break the pacifier or thumb sucking habit.\t5\nA Tale of Baby's Days with Peter Rabbit\tLovely book, it's bound tightly so you may not be able to add alot of photos/cards aside from the designated spaces in the book. Shop around before you purchase, as it is currently listed at Barnes & Noble for 29.95!\t4\nBaby Tracker&reg; - Daily Childcare Journal, Schedule Log\tPerfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!\t5\nBaby Tracker&reg; - Daily Childcare Journal, Schedule Log\tA friend of mine pinned this product on Pinterest so I decided to give it a whirl! It is fantastic! If you are a new parent, this will help you keep track of feedings, diaper changes and the like!\t5\nBaby Tracker&reg; - Daily Childcare Journal, Schedule Log\tThis has been an easy way for my nanny to record all the key events that happen with my baby when I'm not at home.  Would highly recommend it to someone who wants to stay informed of what your baby is up to while you're not home.The only reason this isn't a 5 is because I think there could have been some more standarad pre-printed options.I plan on ordering another one when we run out of pages in this journal.\t4\n"}]},"apps":[],"jobName":"paragraph_1520689580980_-713499444","id":"20170129-045155_986374275","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36600","user":"anonymous","dateFinished":"2018-03-10T13:47:38+0000","dateStarted":"2018-03-10T13:47:25+0000"},{"text":"%md\n# Implement Transformer\n\nWe need a custom Transformer to build the pipeline. The transformer should remove all punctuations from a given column containing text.\n","dateUpdated":"2018-03-10T13:46:20+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Implement Transformer</h1>\n<p>We need a custom Transformer to build the pipeline. The transformer should remove all punctuations from a given column containing text.</p>\n"}]},"apps":[],"jobName":"paragraph_1520689580981_-713884193","id":"20170129-045225_1360438852","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36603"},{"text":"import org.apache.spark.ml.Transformer\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions.col\n\nclass PunctuationCleanupTransformer(override val uid: String) extends org.apache.spark.ml.Transformer {\n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val inputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"inputCol\", \"input column name\")\n    setDefault(inputCol, \"input\")\n    def getInputCol: String = $(inputCol)\n    def setInputCol(value:String): this.type = set(inputCol, value)\n\n    val outputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"outputCol\", \"output column name\")\n    setDefault(outputCol, \"output\")\n    def getOutputCol: String = $(outputCol)\n    def setOutputCol(value:String): this.type = set(outputCol, value)\n    \n    override def transform(dataset:org.apache.spark.sql.Dataset[_]) = {\n        val removePunctuation = org.apache.spark.sql.functions.udf { text:String => if (text != null) text.replaceAll(\"\\\\p{Punct}\", \" \") else \"\" }\n        dataset.withColumn(getOutputCol, removePunctuation(dataset(getInputCol)))\n    }\n    \n    override def transformSchema(schema:org.apache.spark.sql.types.StructType) = {\n        val outputColName = $(outputCol)\n        val outCol = org.apache.spark.sql.types.StructField(outputColName, org.apache.spark.sql.types.StringType)\n        \n        if (schema.fieldNames.contains(outputColName)) {\n          throw new IllegalArgumentException(s\"Output column $outputColName already exists.\")\n        }\n        org.apache.spark.sql.types.StructType(schema.fields :+ outCol)\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): PunctuationCleanupTransformer = {\n        val that = new PunctuationCleanupTransformer(uid)\n        copyValues(that, extra)\n        that\n    }\n}","dateUpdated":"2018-03-10T14:30:10+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.Transformer\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions.col\ndefined class PunctuationCleanupTransformer\nwarning: previously defined object PunctuationCleanupTransformer is not a companion to class PunctuationCleanupTransformer.\nCompanions must be defined together; you may wish to use :paste mode for this.\n"}]},"apps":[],"jobName":"paragraph_1520689580981_-713884193","id":"20170129-045233_1450874507","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36604","user":"anonymous","dateFinished":"2018-03-10T14:30:14+0000","dateStarted":"2018-03-10T14:30:10+0000"},{"text":"import opennlp.tools.stemmer.PorterStemmer\n\nclass PorterStemmerTransformer(override val uid: String) extends org.apache.spark.ml.Transformer {\n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val inputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"inputCol\", \"input column name\")\n    setDefault(inputCol, \"input\")\n    def getInputCol: String = $(inputCol)\n    def setInputCol(value:String): this.type = set(inputCol, value)\n\n    val outputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"outputCol\", \"output column name\")\n    setDefault(outputCol, \"output\")\n    def getOutputCol: String = $(outputCol)\n    def setOutputCol(value:String): this.type = set(outputCol, value)\n    \n    override def transform(dataset:org.apache.spark.sql.Dataset[_]) = {\n        val stemWord = org.apache.spark.sql.functions.udf { words:Seq[String] => \n            if (words != null) {\n                val stemmer = new opennlp.tools.stemmer.PorterStemmer()\n                words.map(stemmer.stem)\n            }\n            else {\n                Seq()\n            }\n        }\n        dataset.withColumn(getOutputCol, stemWord(dataset(getInputCol)))\n    }\n    \n    override def transformSchema(schema:org.apache.spark.sql.types.StructType) = {\n        val outputColName = $(outputCol)\n        val outCol = org.apache.spark.sql.types.StructField(outputColName, org.apache.spark.sql.types.ArrayType(org.apache.spark.sql.types.StringType))\n\n        if (schema.fieldNames.contains(outputColName)) {\n          throw new IllegalArgumentException(s\"Output column $outputColName already exists.\")\n        }\n        org.apache.spark.sql.types.StructType(schema.fields :+ outCol)\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): PorterStemmerTransformer = {\n        val that = new PorterStemmerTransformer(uid)\n        copyValues(that, extra)\n        that\n    }\n}\n","dateUpdated":"2018-03-10T14:30:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import opennlp.tools.stemmer.PorterStemmer\ndefined class PorterStemmerTransformer\n"}]},"apps":[],"jobName":"paragraph_1520689580981_-713884193","id":"20180306-172331_1459660287","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36605","user":"anonymous","dateFinished":"2018-03-10T14:30:16+0000","dateStarted":"2018-03-10T14:30:15+0000"},{"text":"%md\n# Model Evaluation\n\nAs in the original exercise, we want to use a custom metric for assessing the performance.\n","dateUpdated":"2018-03-10T13:46:20+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Model Evaluation</h1>\n<p>As in the original exercise, we want to use a custom metric for assessing the performance.</p>\n"}]},"apps":[],"jobName":"paragraph_1520689580981_-713884193","id":"20170129-045421_315570748","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36606"},{"text":"import org.apache.spark.sql.functions.col\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.ml.param.Param\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.evaluation.Evaluator\n\nclass AccuracyClassificationEvaluator(override val uid: String) extends org.apache.spark.ml.evaluation.Evaluator {\n    \n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val labelCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"labelCol\", \"label column name\")\n    setDefault(labelCol, \"label\")\n    def getLabelCol: String = $(labelCol)\n    def setLabelCol(value:String): this.type = set(labelCol, value)\n\n    val predictionCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"predictionCol\", \"prediction column name\")\n    setDefault(predictionCol, \"prediction\")\n    def getPredictionCol: String = $(predictionCol)\n    def setPredictionCol(value:String): this.type = set(predictionCol, value)\n\n    override def evaluate(dataset: org.apache.spark.sql.Dataset[_]) : Double = {\n        val num_total = dataset.count()\n        val num_correct = dataset.filter(dataset(getLabelCol) === dataset(getPredictionCol)).count()\n        num_correct.toDouble / num_total\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): AccuracyClassificationEvaluator = null // defaultCopy(extra)\n\n}","dateUpdated":"2018-03-10T13:48:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520689580981_-713884193","id":"20170129-045431_175924070","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36607","user":"anonymous","dateFinished":"2018-03-10T13:48:57+0000","dateStarted":"2018-03-10T13:48:56+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions.col\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.ml.param.Param\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.evaluation.Evaluator\ndefined class AccuracyClassificationEvaluator\n"}]}},{"text":"%md\n# Create ML Pipeline\n\nNow we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before\n\n* `PunctuationCleanupTransformer` - remove any punctuation\n* `SQLTransformer` - map rating to 0.0 or 1.0\n* `Tokenizer` - for splitting reviews into words\n* `StopWordRemover` - for removing stop words\n* `PorterStemmer` - stem words\n* `NGram` - for creating n-grams (tuples of consecutive words)\n* `CountVectorizer` - for creating bag-of-word features from the words\n* `IDF` - for creating a TF-IDF model from the raw counts\n* `LogisticRegression` - for creating the real model\n\nYou also need to perform the following transforms:\n* The incoming rating (1-5) needs to be mapped to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more `SQLTransformer` instances. You may want to look that up in the Spark documentation (org.apache.spark.ml.feature.SQLTransformer).\n* The Punctuations need to be removed. This can be done using the `PunctuationRemovalTransfomer`.\n\nThe logical order of feature transformation should be\n1. Remove punctuations\n2. Create sentiment\n3. Tokenize text data\n4. Remove stop words\n5. Stem words\n6. Create Bi-Grams (N-Grams with two words each)\n7. Count words per review\n8. Create TF-IDF model\n","dateUpdated":"2018-03-10T13:46:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Create ML Pipeline</h1>\n<p>Now we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before</p>\n<ul>\n  <li><code>PunctuationCleanupTransformer</code> - remove any punctuation</li>\n  <li><code>SQLTransformer</code> - map rating to 0.0 or 1.0</li>\n  <li><code>Tokenizer</code> - for splitting reviews into words</li>\n  <li><code>StopWordRemover</code> - for removing stop words</li>\n  <li><code>PorterStemmer</code> - stem words</li>\n  <li><code>NGram</code> - for creating n-grams (tuples of consecutive words)</li>\n  <li><code>CountVectorizer</code> - for creating bag-of-word features from the words</li>\n  <li><code>IDF</code> - for creating a TF-IDF model from the raw counts</li>\n  <li><code>LogisticRegression</code> - for creating the real model</li>\n</ul>\n<p>You also need to perform the following transforms:<br/>* The incoming rating (1-5) needs to be mapped to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more <code>SQLTransformer</code> instances. You may want to look that up in the Spark documentation (org.apache.spark.ml.feature.SQLTransformer).<br/>* The Punctuations need to be removed. This can be done using the <code>PunctuationRemovalTransfomer</code>.</p>\n<p>The logical order of feature transformation should be<br/>1. Remove punctuations<br/>2. Create sentiment<br/>3. Tokenize text data<br/>4. Remove stop words<br/>5. Stem words<br/>6. Create Bi-Grams (N-Grams with two words each)<br/>7. Count words per review<br/>8. Create TF-IDF model</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520689580982_-712729946","id":"20170129-045307_120063111","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36608"},{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification._\n\n// First let's load a list of stop words\nval stopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n\n// Now let's define an array of pipeline stages. Each stage (except the last one) is a transformer, and the last one if the LogisticRegression.\nval stages = Array(\n    new PunctuationCleanupTransformer()\n        .setInputCol(\"review\")\n        .setOutputCol(\"clean_review\"),\n    new SQLTransformer()\n        .setStatement(\"SELECT *,CASE WHEN rating < 3 THEN 0.0 ELSE 1.0 END AS sentiment FROM __THIS__ WHERE rating <> 3\"),\n    new Tokenizer()\n        .setInputCol(\"clean_review\")\n        .setOutputCol(\"words\"),\n    new StopWordsRemover()\n        .setInputCol(\"words\")\n        .setOutputCol(\"vwords\")\n        .setStopWords(stopWords),\n    new PorterStemmerTransformer()\n        .setInputCol(\"vwords\")\n        .setOutputCol(\"stems\"),\n    new NGram()\n        .setInputCol(\"stems\")\n        .setOutputCol(\"ngrams\")\n        .setN(2),\n    new CountVectorizer()\n        .setInputCol(\"ngrams\")\n        .setOutputCol(\"tf\")\n        .setMinDF(2.0),\n    new IDF()\n        .setInputCol(\"tf\")\n        .setOutputCol(\"features\"),\n    new LogisticRegression()\n        .setFeaturesCol(\"features\")\n        setLabelCol(\"sentiment\")\n)\nval pipe = new Pipeline().setStages(stages)","dateUpdated":"2018-03-10T14:30:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification._\nstopWords: Array[String] = Array(i, me, my, myself, we, our, ours, ourselves, you, your, yours, yourself, yourselves, he, him, his, himself, she, her, hers, herself, it, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, should, now, i'll, you'll, h...stages: Array[org.apache.spark.ml.PipelineStage] = Array(accuEval_3a86cfedcc0a, sql_0d275eaa76ba, tok_5475f6cf04eb, stopWords_82ec415f787f, accuEval_a6ef5874634a, ngram_e2b0b481e278, cntVec_229419e9d63d, idf_e66bbd0315c7, logreg_e65daef9256c)\npipe: org.apache.spark.ml.Pipeline = pipeline_db9aef317381\n"}]},"apps":[],"jobName":"paragraph_1520689580982_-712729946","id":"20170129-045329_2143290843","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36609","user":"anonymous","dateFinished":"2018-03-10T14:30:27+0000","dateStarted":"2018-03-10T14:30:24+0000"},{"text":"%md\n# Hyper Parameter Tuning\n\nThe whole pipeline has some parameters which have an influence on the result, i.e. the accuracy. For example the size of the n-grams will probably have a big impact and also the minDF parameter of the CountVecttorizer will probably have some impact. These settings are called \"hyper parameters\", because they are also model parameters, but not learnt directly during the training phase. But which parameters work best?\n\nWe will use a CrossValidation to select the best set of hyperparameters.","dateUpdated":"2018-03-10T13:46:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Hyper Parameter Tuning</h1>\n<p>The whole pipeline has some parameters which have an influence on the result, i.e. the accuracy. For example the size of the n-grams will probably have a big impact and also the minDF parameter of the CountVecttorizer will probably have some impact. These settings are called &ldquo;hyper parameters&rdquo;, because they are also model parameters, but not learnt directly during the training phase. But which parameters work best?</p>\n<p>We will use a CrossValidation to select the best set of hyperparameters.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520689580982_-712729946","id":"20170129-045504_1499849390","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36610"},{"text":"println(stages(5).explainParams())","dateUpdated":"2018-03-10T13:50:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"inputCol: input column name (current: stems)\nn: number elements per n-gram (>=1) (default: 2, current: 2)\noutputCol: output column name (default: ngram_9ab7b715bded__output, current: ngrams)\n"}]},"apps":[],"jobName":"paragraph_1520689580982_-712729946","id":"20170129-045513_83457101","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36611","user":"anonymous","dateFinished":"2018-03-10T13:50:26+0000","dateStarted":"2018-03-10T13:50:26+0000"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520689799725_250017427","id":"20180310-134959_107650993","dateCreated":"2018-03-10T13:49:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:38408","text":"println(stages(6).explainParams())","dateUpdated":"2018-03-10T13:50:30+0000","dateFinished":"2018-03-10T13:50:30+0000","dateStarted":"2018-03-10T13:50:30+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"binary: If True, all non zero counts are set to 1. (default: false)\ninputCol: input column name (current: ngrams)\nminDF: Specifies the minimum number of different documents a term must appear in to be included in the vocabulary. If this is an integer >= 1, this specifies the number of documents the term must appear in; if this is a double in [0,1), then this specifies the fraction of documents. (default: 1.0, current: 2.0)\nminTF: Filter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer >= 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document's token count). Note that the parameter is only used in transform of CountVectorizerModel and does not affect fitting. (default: 1.0)\noutputCol: output column name (default: cntVec_64a9df692b38__output, current: tf)\nvocabSize: max size of the vocabulary (default: 262144)\n"}]}},{"text":"%md\n## Create ParamGrid\n\nNow we create a param grid that should be used for using different sets of parameters. We want to tweak two parameters again:\n\n* NGRam sizes should take values in [2,3,5]\n* CounterVectoriter minimum document frequency (minDF) should take values in [1,2,3,5])\n\nIn order to create this grid, we first need to retrieve the corresponding stages from the pipeline, so we can access its parameters.","dateUpdated":"2018-03-10T13:46:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Create ParamGrid</h2>\n<p>Now we create a param grid that should be used for using different sets of parameters. We want to tweak two parameters again:</p>\n<ul>\n  <li>NGRam sizes should take values in [2,3,5]</li>\n  <li>CounterVectoriter minimum document frequency (minDF) should take values in [1,2,3,5])</li>\n</ul>\n<p>In order to create this grid, we first need to retrieve the corresponding stages from the pipeline, so we can access its parameters.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520689580982_-712729946","id":"20170129-045529_389742601","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36612"},{"text":"import org.apache.spark.ml.tuning.ParamGridBuilder\n\nval ngram = stages(5)\nval count = stages(6)\n\nval param_grid = new ParamGridBuilder()\n    .addGrid(ngram.getParam(\"n\"), Seq(2, 3, 5))\n    .addGrid(count.getParam(\"minDF\"), Seq(1.0, 2.0, 3.0, 5.0))\n    .build()","dateUpdated":"2018-03-10T14:31:44+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520689580982_-712729946","id":"20170129-045537_1963157387","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36613","user":"anonymous","dateFinished":"2018-03-10T14:31:47+0000","dateStarted":"2018-03-10T14:31:44+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.tuning.ParamGridBuilder\nngram: org.apache.spark.ml.PipelineStage = ngram_e2b0b481e278\ncount: org.apache.spark.ml.PipelineStage = cntVec_229419e9d63d\nparam_grid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\tcntVec_229419e9d63d-minDF: 1.0,\n\tngram_e2b0b481e278-n: 2\n}, {\n\tcntVec_229419e9d63d-minDF: 1.0,\n\tngram_e2b0b481e278-n: 3\n}, {\n\tcntVec_229419e9d63d-minDF: 1.0,\n\tngram_e2b0b481e278-n: 5\n}, {\n\tcntVec_229419e9d63d-minDF: 2.0,\n\tngram_e2b0b481e278-n: 2\n}, {\n\tcntVec_229419e9d63d-minDF: 2.0,\n\tngram_e2b0b481e278-n: 3\n}, {\n\tcntVec_229419e9d63d-minDF: 2.0,\n\tngram_e2b0b481e278-n: 5\n}, {\n\tcntVec_229419e9d63d-minDF: 3.0,\n\tngram_e2b0b481e278-n: 2\n}, {\n\tcntVec_229419e9d63d-minDF: 3.0,\n\tngram_e2b0b481e278-n: 3\n}, {\n\tcntVec_229419e9d63d-minDF: 3.0,\n\tngram_e2b0b481e278-n: 5\n}, {\n\tcntVec_229419e9d63d-minDF: 5.0,\n\tngram_e2b0b481e278-n: 2\n}, {\n\tcntVec_229419e9d63d-minDF: 5.0,\n\tngram_e2b0b481e278-n: 3\n}, {\n\tcntVec_229419e9d63d-minDF: 5.0,\n\tngram_..."}]}},{"text":"%md\n### Split Train Data / Test Data\n\nNow let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. The sampling can be done via the DataFrame method `randomSplit`.\n","dateUpdated":"2018-03-10T13:56:02+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Split Train Data / Test Data</h3>\n<p>Now let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. The sampling can be done via the DataFrame method <code>randomSplit</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1520689580980_-713499444","id":"20170129-045205_1227820739","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36601","focus":true,"user":"anonymous","dateFinished":"2018-03-10T13:56:02+0000","dateStarted":"2018-03-10T13:56:02+0000"},{"text":"val Array(trainingData, validationData) = data.randomSplit(Array(0.8,0.2), seed=1)\n\nprintln(s\"trainingData: ${trainingData.count()}\")\nprintln(s\"validationData: ${validationData.count()}\")","dateUpdated":"2018-03-10T13:47:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, review: string ... 1 more field]\nvalidationData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, review: string ... 1 more field]\ntrainingData: 140033\nvalidationData: 35118\n"}]},"apps":[],"jobName":"paragraph_1520689580981_-713884193","id":"20170129-045214_331567309","dateCreated":"2018-03-10T13:46:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36602","user":"anonymous","dateFinished":"2018-03-10T13:47:47+0000","dateStarted":"2018-03-10T13:47:30+0000","focus":true},{"text":"%md\n## Perform Hyper Parameter tuning using CrossValidator\n\nNow we can wrap the previous pipeline inside a CrossValidator which trains the model over and over again for all entries in the ParameterGrid. The CrossValidator works as a wrapper of the regression algorithm and will return a pipeline model. In order to evaluate the goodness of a fit, the CrossValidator also needs an Evaluator - we'll use our AccuracyClassificationEvaluator again.\n","dateUpdated":"2018-03-10T13:55:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Perform Hyper Parameter tuning using CrossValidator</h2>\n<p>Now we can wrap the previous pipeline inside a CrossValidator which trains the model over and over again for all entries in the ParameterGrid. The CrossValidator works as a wrapper of the regression algorithm and will return a pipeline model. In order to evaluate the goodness of a fit, the CrossValidator also needs an Evaluator - we&rsquo;ll use our AccuracyClassificationEvaluator again.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520689580982_-712729946","id":"20180305-194053_783571599","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36614"},{"text":"import org.apache.spark.ml.tuning.CrossValidator\n\n// Create instance of AccuracyClassificationEvaluator with labelCol set to the real sentiment column\nval evaluator = new AccuracyClassificationEvaluator()\n    .setLabelCol(\"sentiment\")\n\n// Create a CrossValidator instance\nval validator = new CrossValidator()\n    .setEstimator(pipe)\n    .setEstimatorParamMaps(param_grid)\n    .setEvaluator(evaluator)\n    .setNumFolds(3)\n    \n// Fit model to pipeline\nval model = validator.fit(trainingData)    ","dateUpdated":"2018-03-10T14:31:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520689580982_-712729946","id":"20180305-194048_596318854","dateCreated":"2018-03-10T13:46:20+0000","status":"RUNNING","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36615","user":"anonymous","dateFinished":"2018-03-10T14:30:38+0000","dateStarted":"2018-03-10T14:31:49+0000","results":{"msg":[{"data":"","type":"TEXT"}]}},{"text":"%md\n## Assess model performance\nAgain we want to evaluate the model that was trained using the pipeline and compare it the same simple baseline model which always predicts 'positive'","dateUpdated":"2018-03-10T13:56:06+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Assess model performance</h2>\n<p>Again we want to evaluate the model that was trained using the pipeline and compare it the same simple baseline model which always predicts 'positive'</p>\n"}]},"apps":[],"jobName":"paragraph_1520689580983_-713114695","id":"20170131-103049_1557536721","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36616"},{"text":"// Predict sentiment for test data\nval pred = model.transform(validationData)\nval always_positive = validationData.withColumn(\"prediction\", lit(1.0))\n\nval acc = evaluator.evaluate(pred)\nval baseline = evaluator.evaluate(always_positive)\nprintln(s\"Model Accuracy = ${acc}\")\nprintln(s\"Baseline Accuracy = ${baseline}\")\n","dateUpdated":"2018-03-10T13:46:20+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520689580983_-713114695","id":"20170129-045619_1930087860","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36617"},{"text":"","dateUpdated":"2018-03-10T13:46:20+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520689580983_-713114695","id":"20170129-101018_1552288438","dateCreated":"2018-03-10T13:46:20+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36618"}],"name":"Amazon Baby Products Hyper Parameters Solution","id":"2DA56FTP7","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}