{"paragraphs":[{"text":"%md\n## Load Additional Libraries\nFirst we need to load some more Java/Scala libraries for stemming words. Either you can execute the following pararaph, or you need to manually add the following dependencies to the Zeppelin Spark Interpreter:\n* org.apache.opennlp:opennlp-tools:1.8.4","user":"anonymous","dateUpdated":"2018-03-20T19:49:53+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Load Additional Libraries</h2>\n<p>First we need to load some more Java/Scala libraries for stemming words. Either you can execute the following pararaph, or you need to manually add the following dependencies to the Zeppelin Spark Interpreter:<br/>* org.apache.opennlp:opennlp-tools:1.8.4</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521575378421_-2074761930","id":"20180320-194938_1127259857","dateCreated":"2018-03-20T19:49:38+0000","dateStarted":"2018-03-20T19:49:50+0000","dateFinished":"2018-03-20T19:49:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4358"},{"text":"%spark.dep\nz.load(\"org.apache.opennlp:opennlp-tools:1.8.4\")","user":"anonymous","dateUpdated":"2018-03-20T19:50:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521575404320_-807906395","id":"20180320-195004_400141995","dateCreated":"2018-03-20T19:50:04+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4359"},{"text":"%md\n# Load Data\n\nFirst we load data from HDFS. It is stored as a trivial CSV file with three columns\n\n* product name\n* review text\n* rating (1 - 5)\n\nWe will explicitly specify a Schema, because the CSV might not contain a valid header. Moreover this keeps us in detailed control over the column types and saves us from one Spark run trying to guess the types.","user":"anonymous","dateUpdated":"2018-03-20T19:50:56+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Load Data</h1>\n<p>First we load data from HDFS. It is stored as a trivial CSV file with three columns</p>\n<ul>\n  <li>product name</li>\n  <li>review text</li>\n  <li>rating (1 - 5)</li>\n</ul>\n<p>We will explicitly specify a Schema, because the CSV might not contain a valid header. Moreover this keeps us in detailed control over the column types and saves us from one Spark run trying to guess the types.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521574923085_1815994615","id":"20170129-045139_1503434840","dateCreated":"2018-03-20T19:42:03+0000","dateStarted":"2018-03-20T19:50:56+0000","dateFinished":"2018-03-20T19:50:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4360"},{"text":"val basedir = \"s3://dimajix-training/data/\"","dateUpdated":"2018-03-21T17:34:49+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923085_1815994615","id":"20170129-091247_907732707","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4361"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.functions.col\n\n\n// Explicitly build the schema as we expect it in the CSV file. Note that we also assume \"rating\" to be string type. This saves\n// us from errors due to wrong entries. Casting a String to something else later only produces SQL NULL values, but no errors.\nval schema = StructType(\n        StructField(\"name\", StringType) ::\n        StructField(\"review\", StringType) ::\n        StructField(\"rating\", StringType) ::\n        Nil\n    )\n// Read in CSV data and cast rating column to integer    \nval data = spark.read\n    .schema(schema)\n    .csv(basedir + \"amazon_baby\")\n    .withColumn(\"rating\", col(\"rating\").cast(IntegerType))\n    .filter(col(\"rating\").isNotNull)\nz.show(data.limit(10))","dateUpdated":"2018-03-21T17:34:36+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923085_1815994615","id":"20170129-045155_986374275","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4362"},{"text":"%md\n## Implement PunctuationCleanupTransformer\n\nWe could simply use a udf in order to remove all punctuations, but in order to build a ML Pipeline, we implement a new `PunctuationCleanupTransformer`","user":"anonymous","dateUpdated":"2018-03-20T19:52:25+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Implement PunctuationCleanupTransformer</h2>\n<p>We could simply use a udf in order to remove all punctuations, but in order to build a ML Pipeline, we implement a new <code>PunctuationCleanupTransformer</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521574923085_1815994615","id":"20170129-045225_1360438852","dateCreated":"2018-03-20T19:42:03+0000","dateStarted":"2018-03-20T19:52:25+0000","dateFinished":"2018-03-20T19:52:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4363"},{"text":"import org.apache.spark.ml.Transformer\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions.col\n\n\nclass PunctuationCleanupTransformer(override val uid: String) extends org.apache.spark.ml.Transformer {\n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val inputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"inputCol\", \"input column name\")\n    setDefault(inputCol, \"input\")\n    def getInputCol: String = $(inputCol)\n    def setInputCol(value:String): this.type = set(inputCol, value)\n\n    val outputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"outputCol\", \"output column name\")\n    setDefault(outputCol, \"output\")\n    def getOutputCol: String = $(outputCol)\n    def setOutputCol(value:String): this.type = set(outputCol, value)\n    \n    override def transform(dataset:org.apache.spark.sql.Dataset[_]) = {\n        val removePunctuation = org.apache.spark.sql.functions.udf { text:String => if (text != null) text.replaceAll(\"\\\\p{Punct}\", \" \") else \"\" }\n        dataset.withColumn(getOutputCol, removePunctuation(dataset(getInputCol)))\n    }\n    \n    override def transformSchema(schema:org.apache.spark.sql.types.StructType) = {\n        val outputColName = $(outputCol)\n        val outCol = org.apache.spark.sql.types.StructField(outputColName, org.apache.spark.sql.types.StringType)\n        \n        if (schema.fieldNames.contains(outputColName)) {\n          throw new IllegalArgumentException(s\"Output column $outputColName already exists.\")\n        }\n        org.apache.spark.sql.types.StructType(schema.fields :+ outCol)\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): PunctuationCleanupTransformer = defaultCopy(extra)\n}\n","dateUpdated":"2018-03-20T19:42:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923086_1817148862","id":"20170129-045233_1450874507","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4364"},{"text":"%md\n## Test Transformer\n\nLets create an instance of the Transformer and test it. This involves the following steps\n1. Create an instance of the class PunctuationCleanupTransformer\n2. Configure input and output column names in transfomer. Use `clean_review` for the output column.\n3. Transform the data and store the result in a DataFrame `clean_data`\n \nOptionally you can also display the first couple of entries using `z.show()`.\n","user":"anonymous","dateUpdated":"2018-03-20T19:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Test Transformer</h2>\n<p>Lets create an instance of the Transformer and test it. This involves the following steps<br/>1. Create an instance of the class PunctuationCleanupTransformer<br/>2. Configure input and output column names in transfomer. Use <code>clean_review</code> for the output column.<br/>3. Transform the data and store the result in a DataFrame <code>clean_data</code></p>\n<p>Optionally you can also display the first couple of entries using <code>z.show()</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521574923086_1817148862","id":"20170129-045246_1519878179","dateCreated":"2018-03-20T19:42:03+0000","dateStarted":"2018-03-20T19:57:34+0000","dateFinished":"2018-03-20T19:57:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4365"},{"text":"// Create an instance of the PunctuationCleanupTransformer\nval cleaner = ... // YOUR CODE HERE\n\n// Apply the cleaner to the input data\nval clean_data = ... // YOUR CODE HERE\n\nz.show(clean_data.limit(6))","dateUpdated":"2018-03-20T19:53:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923086_1817148862","id":"20170129-045257_1112043389","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4366"},{"text":"%md\n## Implement PorterStemmerTransformer\n\nWe also want to perform stemming. Spark currently doesn't offer a solution out of the box, so again we need to implement a dedicated Transformer. We use the Porter stemming algorithm which is specifically designed to handle the english language. We don't implement the algorithm on our own, instead we use the implementation provided by the Apache OpenNLP project.","user":"anonymous","dateUpdated":"2018-03-20T19:54:22+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Implement PorterStemmerTransformer</h2>\n<p>We also want to perform stemming. Spark currently doesn&rsquo;t offer a solution out of the box, so again we need to implement a dedicated Transformer. We use the Porter stemming algorithm which is specifically designed to handle the english language. We don&rsquo;t implement the algorithm on our own, instead we use the implementation provided by the Apache OpenNLP project.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521575660499_1100031107","id":"20180320-195420_1773194349","dateCreated":"2018-03-20T19:54:20+0000","dateStarted":"2018-03-20T19:54:22+0000","dateFinished":"2018-03-20T19:54:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4367"},{"text":"import opennlp.tools.stemmer.PorterStemmer\n\nclass PorterStemmerTransformer(override val uid: String) extends org.apache.spark.ml.Transformer {\n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val inputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"inputCol\", \"input column name\")\n    setDefault(inputCol, \"input\")\n    def getInputCol: String = $(inputCol)\n    def setInputCol(value:String): this.type = set(inputCol, value)\n\n    val outputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"outputCol\", \"output column name\")\n    setDefault(outputCol, \"output\")\n    def getOutputCol: String = $(outputCol)\n    def setOutputCol(value:String): this.type = set(outputCol, value)\n    \n    override def transform(dataset:org.apache.spark.sql.Dataset[_]) = {\n        val stemWords = org.apache.spark.sql.functions.udf { words:Seq[String] => \n            if (words != null) {\n                val stemmer = new opennlp.tools.stemmer.PorterStemmer()\n                words.map(stemmer.stem)\n            }\n            else {\n                Seq()\n            }\n        }\n        dataset.withColumn(getOutputCol, stemWords(dataset(getInputCol)))\n    }\n    \n    override def transformSchema(schema:org.apache.spark.sql.types.StructType) = {\n        val outputColName = $(outputCol)\n        val outCol = org.apache.spark.sql.types.StructField(outputColName, org.apache.spark.sql.types.ArrayType(org.apache.spark.sql.types.StringType))\n        \n        if (schema.fieldNames.contains(outputColName)) {\n          throw new IllegalArgumentException(s\"Output column $outputColName already exists.\")\n        }\n        org.apache.spark.sql.types.StructType(schema.fields :+ outCol)\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): PorterStemmerTransformer = defaultCopy(extra)\n}","user":"anonymous","dateUpdated":"2018-03-20T19:54:39+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521575672425_-645959401","id":"20180320-195432_707574815","dateCreated":"2018-03-20T19:54:32+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4368"},{"text":"%md\n### Test Stemmer Transform\n\nAgain we want to do a quick test of the stemmer. This transform only works with individual words.","user":"anonymous","dateUpdated":"2018-03-20T19:54:52+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Test Stemmer Transform</h3>\n<p>Again we want to do a quick test of the stemmer. This transform only works with individual words.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521575690818_436000478","id":"20180320-195450_349373560","dateCreated":"2018-03-20T19:54:50+0000","dateStarted":"2018-03-20T19:54:52+0000","dateFinished":"2018-03-20T19:54:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4369"},{"text":"import org.apache.spark.ml.feature.Tokenizer\n\n// FIrst create a Tokenizer (because stemming only works with words). Set input column to \"clean_review\" and output column to \"words\"\nval tokenizer = // YOUR CODE HERE\n\n// Apply the tokeinzer to the clean_data\nval words = tokenizer.transform(clean_data)\n\n// Now create a PorterStemmerTransformer and configure it to work with the tokenized data. Set the output column to \"stems\"\nval stemmer = // YOUR CODE HERE\n\n// Apply stemmer to output of tokenizer    \nval stems = stemmer.transform(words)\n\nz.show(stems.select(\"stems\").limit(10))","user":"anonymous","dateUpdated":"2018-03-20T19:59:02+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521575701719_-2071408104","id":"20180320-195501_442662256","dateCreated":"2018-03-20T19:55:01+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4370"},{"text":"%md\n# Create ML Pipeline\n\nNow we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before\n\n* `Tokenizer` - for splitting reviews into words\n* `StopWordRemover` - for removing stop words\n* `NGram` - for creating n-grams (tuples of consecutive words)\n* `CountVectorizer` - for creating bag-of-word features from the words\n* `IDF` - for creating a TF-IDF model from the raw counts\n* `LogisticRegression` - for creating the real model\n\nYou also need to perform the following transforms:\n\n* The incoming rating (1-5) needs to be mapped to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more `SQLTransformer` instances. You may want to look that up in the Spark documentation (org.apache.spark.ml.feature.SQLTransformer).\n* The Punctuations need to be removed. This can be done using the `PunctuationRemovalTransfomer`.\n\nThe logical order of feature transformation should be\n1. Remove punctuations\n2. Create sentiment\n3. Tokenize text data\n4. Remove stop words\n5. Stem words\n6. Create Bi-Grams (N-Grams with two words each)\n7. Count words per review\n8. Create TF-IDF model\n\nAnd the very last step of the pipeline has to be a `LogisticRegression` working with the features from TF-IDF.","user":"anonymous","dateUpdated":"2018-03-20T20:03:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Create ML Pipeline</h1>\n<p>Now we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before</p>\n<ul>\n  <li><code>Tokenizer</code> - for splitting reviews into words</li>\n  <li><code>StopWordRemover</code> - for removing stop words</li>\n  <li><code>NGram</code> - for creating n-grams (tuples of consecutive words)</li>\n  <li><code>CountVectorizer</code> - for creating bag-of-word features from the words</li>\n  <li><code>IDF</code> - for creating a TF-IDF model from the raw counts</li>\n  <li><code>LogisticRegression</code> - for creating the real model</li>\n</ul>\n<p>You also need to perform the following transforms:</p>\n<ul>\n  <li>The incoming rating (1-5) needs to be mapped to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more <code>SQLTransformer</code> instances. You may want to look that up in the Spark documentation (org.apache.spark.ml.feature.SQLTransformer).</li>\n  <li>The Punctuations need to be removed. This can be done using the <code>PunctuationRemovalTransfomer</code>.</li>\n</ul>\n<p>The logical order of feature transformation should be<br/>1. Remove punctuations<br/>2. Create sentiment<br/>3. Tokenize text data<br/>4. Remove stop words<br/>5. Stem words<br/>6. Create Bi-Grams (N-Grams with two words each)<br/>7. Count words per review<br/>8. Create TF-IDF model</p>\n<p>And the very last step of the pipeline has to be a <code>LogisticRegression</code> working with the features from TF-IDF.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521574923086_1817148862","id":"20170129-045307_120063111","dateCreated":"2018-03-20T19:42:03+0000","dateStarted":"2018-03-20T20:03:48+0000","dateFinished":"2018-03-20T20:03:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4371"},{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification._\n\nval stopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n\n// Now let's define an array of pipeline stages. Each stage (except the last one) is a transformer, and the last one if the LogisticRegression.\nval stages = Array(\n    new PunctuationCleanupTransformer()\n        // YOUR CODE HERE\n        ,\n    new SQLTransformer()\n        // YOUR CODE HERE\n        ,\n    new Tokenizer()\n        // YOUR CODE HERE\n        ,\n    new StopWordsRemover()\n        // YOUR CODE HERE\n        ,\n    new PorterStemmerTransformer()\n        // YOUR CODE HERE\n        ,\n    new NGram()\n        // YOUR CODE HERE\n        ,\n    new CountVectorizer()\n        // YOUR CODE HERE\n        ,\n    new IDF()\n        // YOUR CODE HERE\n        ,\n    new LogisticRegression()\n        // YOUR CODE HERE\n)\nval pipe = new Pipeline().setStages(stages)","dateUpdated":"2018-03-20T20:00:46+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923086_1817148862","id":"20170129-045329_2143290843","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4372"},{"text":"%md\n## Split Train Data / Test Data\n\nNow let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. The sampling can be done via the DataFrame method `randomSplit`.\n","user":"anonymous","dateUpdated":"2018-03-20T19:59:40+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Split Train Data / Test Data</h2>\n<p>Now let&rsquo;s do the usual split of our data into a training data set and a validation data set. Let&rsquo;s use 80% of all reviews for training and 20% for validation. The sampling can be done via the DataFrame method <code>randomSplit</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521574923085_1815994615","id":"20170129-045205_1227820739","dateCreated":"2018-03-20T19:42:03+0000","dateStarted":"2018-03-20T19:59:40+0000","dateFinished":"2018-03-20T19:59:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4373"},{"text":"val Array(trainingData, validationData) = // YOUR CODE HERE\n\nprintln(s\"trainingData: ${trainingData.count()}\")\nprintln(s\"validationData: ${validationData.count()}\")","dateUpdated":"2018-03-20T19:42:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923085_1815994615","id":"20170129-045214_331567309","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4374"},{"text":"%md\n## Train Classifier\n\nNow that we have both a ML pipeline configuration and a training set, we can fit the whole pipeline to the data.","user":"anonymous","dateUpdated":"2018-03-20T20:04:18+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Train Classifier</h2>\n<p>Now that we have both a ML pipeline configuration and a training set, we can fit the whole pipeline to the data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521574923086_1817148862","id":"20170129-045341_907324707","dateCreated":"2018-03-20T19:42:03+0000","dateStarted":"2018-03-20T20:04:18+0000","dateFinished":"2018-03-20T20:04:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4375"},{"text":"val model = ... // YOUR CODE HERE","dateUpdated":"2018-03-20T19:42:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923086_1817148862","id":"20170129-045351_1240420989","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4376"},{"text":"%md\n## Predict Data\n\nLet us do some predictions of the test data using the model.\n","dateUpdated":"2018-03-20T19:42:03+0000","config":{"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Predict Data</h2>\n<p>Let us do some predictions of the test data using the model.</p>\n"}]},"apps":[],"jobName":"paragraph_1521574923086_1817148862","id":"20170129-045405_2130383484","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4377"},{"text":"val pred = ... // YOUR CODE HERE\n\nz.show(pred.limit(10))","dateUpdated":"2018-03-20T19:42:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923087_1816764113","id":"20170129-045410_1701329257","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4378"},{"text":"%md\n# Model Evaluation\n\nAs in the original exercise, we want to use a custom metric for assessing the performance.\n","dateUpdated":"2018-03-20T20:08:40+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Model Evaluation</h1>\n<p>As in the original exercise, we want to use a custom metric for assessing the performance.</p>\n"}]},"apps":[],"jobName":"paragraph_1521574923087_1816764113","id":"20170129-045421_315570748","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4379"},{"text":"import org.apache.spark.sql.functions.col\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.ml.param.Param\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.evaluation.Evaluator\n\nclass AccuracyClassificationEvaluator(override val uid: String) extends org.apache.spark.ml.evaluation.Evaluator {\n    \n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val labelCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"labelCol\", \"label column name\")\n    setDefault(labelCol, \"label\")\n    def getLabelCol: String = $(labelCol)\n    def setLabelCol(value:String): this.type = set(labelCol, value)\n\n    val predictionCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"predictionCol\", \"prediction column name\")\n    setDefault(predictionCol, \"prediction\")\n    def getPredictionCol: String = $(predictionCol)\n    def setPredictionCol(value:String): this.type = set(predictionCol, value)\n\n    override def evaluate(dataset: org.apache.spark.sql.Dataset[_]) : Double = {\n        val num_total = dataset.count()\n        val num_correct = dataset.filter(dataset(getLabelCol) === dataset(getPredictionCol)).count()\n        num_correct.toDouble / num_total\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): AccuracyClassificationEvaluator = null // defaultCopy(extra)\n\n}","dateUpdated":"2018-03-20T19:42:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923087_1816764113","id":"20170129-045431_175924070","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4380"},{"text":"%md\n## Assess Performance\n\nWith the evaluator we can assess the performance of the prediction and easily compare it to a simple model which always predicts 'positive'.\n\nFor evaluating a model, the following steps have to be performed\n1. Create instance of `AccuracyClassificationEvaluator`\n2. Set names of label column and prediction in evaluator\n3. Evaluate trained model\n4. Evaluate dummy model which always predicts \"positive\" as a baseline\n\nHow can we create a model that always predicts 'positive'? Actually we don't need a model itself, since evaluation is done by looking at the predicted values. So it is enough to create a DataFrame with a prediction column with constant value '1'.\n","dateUpdated":"2018-03-20T20:09:08+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Assess Performance</h2>\n<p>With the evaluator we can assess the performance of the prediction and easily compare it to a simple model which always predicts 'positive'.</p>\n<p>For evaluating a model, the following steps have to be performed</p>\n<ol>\n<li>Create instance of <code>AccuracyClassificationEvaluator</code></li>\n<li>Set names of label column and prediction in evaluator</li>\n<li>Evaluate trained model</li>\n<li>Evaluate dummy model which always predicts &ldquo;positive&rdquo; as a baseline</li>\n</ol>\n<p>How can we create a model that always predicts 'positive'? Actually we don't need a model itself, since evaluation is done by looking at the predicted values. So it is enough to create a DataFrame with a prediction column with constant value '1'.</p>\n"}]},"apps":[],"jobName":"paragraph_1521574923087_1816764113","id":"20170129-045442_862289758","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4381"},{"text":"val always_positive = pred.withColumn(\"prediction\",lit(1.0))\n\nval evaluator = ... // YOUR CODE HERE\n\nval accuracy = ... // YOUR CODE HERE\nval baseline = ... // YOUR CODE HERE\nprintln(s\"Model Accuracy = ${accuracy}\")\nprintln(s\"Baseline Accuracy = ${baseline}\")","dateUpdated":"2018-03-20T19:42:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923087_1816764113","id":"20170129-045451_732519493","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4382"},{"text":"","dateUpdated":"2018-03-20T19:42:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574923088_1827152334","id":"20170129-101018_1552288438","dateCreated":"2018-03-20T19:42:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4383"}],"name":"Amazon Baby Products Pipeline Exercise","id":"2D9AU4ZWS","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}