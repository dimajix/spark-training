{"paragraphs":[{"text":"%md\n# Word Count Revisited\n\nNow we have a simple word counting algorithm, we want to extend it by two simple features:\n* We want to ignore some words, which are not interesting to use.\n* We want to have a counter showing how many words have been ignored.\n\nThese two features will be implemented using *broadcast variables* and *accumulators*.\n\nBut first let's have a look at the original word-count solution as a starting point.","dateUpdated":"2017-02-18T12:19:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420021710_-963657955","id":"20170130-221734_229219199","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Word Count Revisited</h1>\n<p>Now we have a simple word counting algorithm, we want to extend it by two simple features:</p>\n<ul>\n<li>We want to ignore some words, which are not interesting to use.</li>\n<li>We want to have a counter showing how many words have been ignored.</li>\n</ul>\n<p>These two features will be implemented using <em>broadcast variables</em> and <em>accumulators</em>.</p>\n<p>But first let's have a look at the original word-count solution as a starting point.</p>\n"},"dateCreated":"2017-02-18T12:13:41+0000","dateStarted":"2017-02-18T12:19:49+0000","dateFinished":"2017-02-18T12:19:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1718"},{"text":"// The original word count solution\nval text = sc.textFile(\"s3://dimajix-training/data/alice\")\nval words = text.flatMap(_.split(\" \"))\n    .filter(_ != \"\")\n    .map(x => (x,1))\n    .reduceByKey(_ + _)\n    .sortBy(_._2, ascending=false)\n    .map({ case (k,v) => k + '\\t' + v.toString() })\n    \nwords.take(10).foreach(println)","dateUpdated":"2017-02-18T12:48:40+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420021713_-954039233","id":"20170130-221747_308746184","result":{"code":"SUCCESS","type":"TEXT","msg":"\ntext: org.apache.spark.rdd.RDD[String] = s3://dimajix-training/data/alice MapPartitionsRDD[30] at textFile at <console>:32\n\nwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[40] at map at <console>:39\nthe\t1664\nand\t780\nto\t773\na\t662\nof\t596\nshe\t484\nsaid\t416\nin\t401\nit\t356\nwas\t329\n"},"dateCreated":"2017-02-18T12:13:41+0000","dateStarted":"2017-02-18T12:20:10+0000","dateFinished":"2017-02-18T12:20:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1719"},{"text":"%md\n# 1. Ignore Stop Words\n\nNow let's try to implement the first feature: We want to ignore words coming from a pre-defined set of uninteresting words. Specifically we want to ignore the following words:\n\n* the, a, an, and, of, I, she, it, to, in, was, *empty word* (\"\")\n \nFor implementing the stop word filter, we will perform the following steps.\n\n1. Create a Scala set of all stop words. We should make them also lowercase for simpler comparison\n2. Use SparkContext.broadcast to explicitly send this set to all worker nodes\n3. Create a simple function isStopWord that checks if a given word is part of the set.\n4. Modify original word count solution to use this new filter","dateUpdated":"2017-02-18T12:36:35+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420432373_2034135264","id":"20170218-122032_1842586031","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>1. Ignore Stop Words</h1>\n<p>Now let's try to implement the first feature: We want to ignore words coming from a pre-defined set of uninteresting words. Specifically we want to ignore the following words:</p>\n<ul>\n<li>the, a, an, and, of, I, she, it, to, in, was, <em>empty word</em> (&ldquo;&ldquo;)</li>\n</ul>\n<p>For implementing the stop word filter, we will perform the following steps.</p>\n<ol>\n<li>Create a Scala set of all stop words. We should make them also lowercase for simpler comparison</li>\n<li>Use SparkContext.broadcast to explicitly send this set to all worker nodes</li>\n<li>Create a simple function isStopWord that checks if a given word is part of the set.</li>\n<li>Modify original word count solution to use this new filter</li>\n</ol>\n"},"dateCreated":"2017-02-18T12:20:32+0000","dateStarted":"2017-02-18T12:36:33+0000","dateFinished":"2017-02-18T12:36:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1720"},{"text":"// 1. Create a Set of the mentioned stopWords, transform each word to lower case using the String method 'toLowerCase'\nval stopWords = Set(\"the\", \"a\", \"an\", \"and\", \"of\", \"i\", \"she\", \"it\", \"to\", \"in\", \"was\", \"\").map(_.toLowerCase)\n\n// 2. Create a broadcast variable from the set of stop words. This can be achieved by using the method SparkContext.broadcast\nval stopWordsBc = sc.broadcast(stopWords)\n\n// 3. Write a function 'isStopWord' which takes a single string as argument and returns true if the argument is in the list of stop words. \n//    Inside the function (which will be executed in a distributed manner on all nodes), we need to access the broadcast variable. You can\n//    retrieve the original Scala set of words by using stopWordsBc.value. Do not forget to transform the incoming word to lower case!\ndef isStopWord(word:String) = stopWordsBc.value.contains(word.toLowerCase)","dateUpdated":"2017-02-18T12:52:26+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420515074_-1372108045","id":"20170218-122155_1057252807","result":{"code":"SUCCESS","type":"TEXT","msg":"\nstopWords: scala.collection.immutable.Set[String] = Set(\"\", in, it, a, she, i, to, was, an, of, and, the)\n\nstopWordsBc: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] = Broadcast(26)\n\nisStopWord: (word: String)Boolean\n"},"dateCreated":"2017-02-18T12:21:55+0000","dateStarted":"2017-02-18T12:28:39+0000","dateFinished":"2017-02-18T12:28:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1721"},{"text":"%md\nNow that we have everything in place, we only need to modify the previous solution by replacing the filter operation","dateUpdated":"2017-02-18T12:29:34+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420942611_-536624426","id":"20170218-122902_455043473","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now that we have everything in place, we only need to modify the previous solution by replacing the filter operation</p>\n"},"dateCreated":"2017-02-18T12:29:02+0000","dateStarted":"2017-02-18T12:29:31+0000","dateFinished":"2017-02-18T12:29:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1722"},{"text":"// 4. Modify the original program to use the new filter\nval text = sc.textFile(\"s3://dimajix-training/data/alice\")\nval words = text.flatMap(_.split(\" \"))\n    .filter(!isStopWord(_))\n    .map(x => (x,1))\n    .reduceByKey(_ + _)\n    .sortBy(_._2, ascending=false)\n    .map({ case (k,v) => k + '\\t' + v.toString() })\n\nwords.take(10).foreach(println)","dateUpdated":"2017-02-18T12:37:03+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":false,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420722337_1963956078","id":"20170218-122522_1465615836","result":{"code":"SUCCESS","type":"TEXT","msg":"\ntext: org.apache.spark.rdd.RDD[String] = s3://dimajix-training/data/alice MapPartitionsRDD[56] at textFile at <console>:32\n\nwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[66] at map at <console>:45\nsaid\t416\nyou\t301\nas\t246\nthat\t226\nAlice\t221\nwith\t214\nat\t211\nher\t203\nhad\t175\nall\t168\n"},"dateCreated":"2017-02-18T12:25:22+0000","dateStarted":"2017-02-18T12:28:47+0000","dateFinished":"2017-02-18T12:29:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1723"},{"text":"%md\n# 2. Count invocations of isStopWord inclusing results\nNow that we have a nice and efficient solution in place, we also might want to count how often a word was rejected or accepted, i.e. how often isStopWord returns true or false. Of course we could simply perform a `count` after the filter, but this might not work in more complex scenarios, or this might be very expensive. \n\nWhat we really want is two simple counters that are increased whenever a word is filtered or accepted. Unfortunately this is not trivial in a distributed environment, so we cannot use a simple Scala variable. But again, Spark comes to the resuce and offers us *Accumulators*, which are distributed counters.\n\n1. Create two Accumulators using SparkContext.longAccumulator(name)\n2. Create a new filter function `isStopWord` which also increments the appropriate accumulator\n3. Rerun word count with new isStopWord function\n4. Inspect the value of both accumulators\n","dateUpdated":"2017-02-18T12:48:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420021713_-954039233","id":"20170130-221810_265200548","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>2. Count invocations of isStopWord inclusing results</h1>\n<p>Now that we have a nice and efficient solution in place, we also might want to count how often a word was rejected or accepted, i.e. how often isStopWord returns true or false. Of course we could simply perform a <code>count</code> after the filter, but this might not work in more complex scenarios, or this might be very expensive.</p>\n<p>What we really want is two simple counters that are increased whenever a word is filtered or accepted. Unfortunately this is not trivial in a distributed environment, so we cannot use a simple Scala variable. But again, Spark comes to the resuce and offers us <em>Accumulators</em>, which are distributed counters.</p>\n<ol>\n<li>Create two Accumulators using SparkContext.longAccumulator(name)</li>\n<li>Create a new filter function <code>isStopWord</code> which also increments the appropriate accumulator</li>\n<li>Rerun word count with new isStopWord function</li>\n<li>Inspect the value of both accumulators</li>\n</ol>\n"},"dateCreated":"2017-02-18T12:13:41+0000","dateStarted":"2017-02-18T12:47:59+0000","dateFinished":"2017-02-18T12:47:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1724"},{"text":"// 1. Create two accumulators, one for the number of stop words, and another one for the number of all other words.\nval stopWordCounter = sc.longAccumulator(\"stop_words\")\nval otherWordCounter = sc.longAccumulator(\"other_words\")\n\n// 2. ReWrite a function 'isStopWord' to increment the appropriate accumulator using the method 'add' of each accumulator\ndef isStopWord(word:String) = {\n    val result = stopWordsBc.value.contains(word.toLowerCase)\n    if (result)\n        stopWordCounter.add(1)\n    else\n        otherWordCounter.add(1)\n    result\n}","dateUpdated":"2017-02-18T12:49:38+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420021713_-954039233","id":"20170130-221759_788343263","result":{"code":"SUCCESS","type":"TEXT","msg":"\nstopWordCounter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 2095, name: Some(stop_words), value: 0)\n\notherWordCounter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 2096, name: Some(other_words), value: 0)\n\nisStopWord: (word: String)Boolean\n"},"dateCreated":"2017-02-18T12:13:41+0000","dateStarted":"2017-02-18T12:44:20+0000","dateFinished":"2017-02-18T12:44:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1725"},{"text":"// 3. Rerun word count\nval text = sc.textFile(\"s3://dimajix-training/data/alice\")\nval words = text.flatMap(_.split(\" \"))\n    .filter(!isStopWord(_))\n    .map(x => (x,1))\n    .reduceByKey(_ + _)\n    .sortBy(_._2, ascending=false)\n    .map({ case (k,v) => k + '\\t' + v.toString() })\n\nwords.take(10).foreach(println)","dateUpdated":"2017-02-18T12:44:22+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420096626_-1548537580","id":"20170218-121456_1172349725","result":{"code":"SUCCESS","type":"TEXT","msg":"\ntext: org.apache.spark.rdd.RDD[String] = s3://dimajix-training/data/alice MapPartitionsRDD[70] at textFile at <console>:33\n\nwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[80] at map at <console>:49\nsaid\t416\nyou\t301\nas\t246\nthat\t226\nAlice\t221\nwith\t214\nat\t211\nher\t203\nhad\t175\nall\t168\n"},"dateCreated":"2017-02-18T12:14:56+0000","dateStarted":"2017-02-18T12:44:22+0000","dateFinished":"2017-02-18T12:44:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1726"},{"text":"// 4. Print accumulators. You can access the actual value with the 'value' member of each accumulator\nprintln(s\"Stop words: ${stopWordCounter.value}\")\nprintln(s\"Other words: ${otherWordCounter.value}\")","dateUpdated":"2017-02-18T12:47:47+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487421848460_569386334","id":"20170218-124408_2146008077","result":{"code":"SUCCESS","type":"TEXT","msg":"Stop words: 8868\nOther words: 22831\n"},"dateCreated":"2017-02-18T12:44:08+0000","dateStarted":"2017-02-18T12:47:10+0000","dateFinished":"2017-02-18T12:47:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1727"},{"text":"","dateUpdated":"2017-02-18T12:47:47+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487421982301_1973181829","id":"20170218-124622_2016533883","dateCreated":"2017-02-18T12:46:22+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1728"}],"name":"WordCount with StopWords - Solution","id":"2CBSJC6M1","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}