{"paragraphs":[{"text":"%md\n# About Spark Datasets\n\nInitially Spark only implemented the lower level RDD API, which already provides a relational algebra as an object oriented functional API heavily inspired by the Scala collections API. But the implementation is difficult to use and does only very limited optimizations. \nThen after some time a second API, the DataFrame API was implemented, which operates at a higher level of abstraction and which is heavily inspired by traditional SQL. This API offers more query optimizations and is easier to use.\n\nBut there are cases where some aspects of both APIs would be beneficial, therefore the Spark developers generalized the DataFrame API to build upon a new API called Datasets. Datasets basically work at the abstraction level of DataFrames, but include a mechanism to map a DataFrame schema to an Scala object model, thereby supporting a functional API similar to the Scala collection API. This can be very helpful from time to time, specifically when you want to work with a traditional Scala object model for transformations instead of using user defined functions.","dateUpdated":"2019-04-01T15:15:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About Spark Datasets</h1>\n<p>Initially Spark only implemented the lower level RDD API, which already provides a relational algebra as an object oriented functional API heavily inspired by the Scala collections API. But the implementation is difficult to use and does only very limited optimizations.<br/>Then after some time a second API, the DataFrame API was implemented, which operates at a higher level of abstraction and which is heavily inspired by traditional SQL. This API offers more query optimizations and is easier to use.</p>\n<p>But there are cases where some aspects of both APIs would be beneficial, therefore the Spark developers generalized the DataFrame API to build upon a new API called Datasets. Datasets basically work at the abstraction level of DataFrames, but include a mechanism to map a DataFrame schema to an Scala object model, thereby supporting a functional API similar to the Scala collection API. This can be very helpful from time to time, specifically when you want to work with a traditional Scala object model for transformations instead of using user defined functions.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131703857_-2066613264","id":"20190324-175857_136432681","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:101"},{"text":"%md\n# 1 Creating Datasets\n\nAgain we initially have a look at how a Dataset can be created from a Scala collection. Although this is not the normal use case, this fits very well to the general approach of Datasets, which provide some functionality comparable to a ORM (Object Relational Mapper).\n\nSince a Dataset requires a data type, we first define an appropriate case class.","dateUpdated":"2019-04-01T15:15:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>1 Creating Datasets</h1>\n<p>Again we initially have a look at how a Dataset can be created from a Scala collection. Although this is not the normal use case, this fits very well to the general approach of Datasets, which provide some functionality comparable to a ORM (Object Relational Mapper).</p>\n<p>Since a Dataset requires a data type, we first define an appropriate case class.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131703876_-2086235458","id":"20160617-201427_857258739","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"text":"case class Person(\n    name:String,\n    sex:String,\n    age:Int)\n{\n}","dateUpdated":"2019-04-01T15:15:03+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Person\n"}]},"apps":[],"jobName":"paragraph_1554131703882_-2087004956","id":"20160617-200218_1910498306","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"text":"%md\n## 1.1 Creating a Dataset from a Scala collection\n\nAs wrritten above, we first create a Spark Dataset from a Scala collection containing instances of the `Person` case class.","dateUpdated":"2019-04-01T15:15:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.1 Creating a Dataset from a Scala collection</h2>\n<p>As wrritten above, we first create a Spark Dataset from a Scala collection containing instances of the <code>Person</code> case class.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131703891_-2078155731","id":"20190324-181317_1113205546","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"text":"val persons = spark.createDataset(\n        Seq(\n            Person(\"Alice\", \"female\", 32), \n            Person(\"Bob\", \"male\", 24),\n            Person(\"Christine\", \"female\", 42), \n            Person(\"Dale\", \"male\", 19),\n            Person(\"Eve\", \"female\", 33)\n        )\n    )","dateUpdated":"2019-04-01T15:15:03+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"persons: org.apache.spark.sql.Dataset[Person] = [name: string, sex: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1554131703898_-2080848974","id":"20160617-200626_1254551091","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:105"},{"text":"%md\n### Notable observations\n\nThere are two important aspects of the result above:\n1. The result is a `Dataset[Person]`, which indicates that a Dataset is statically typed.\n2. Nevertheless the result seems to have a schema, which is indicated by the representation `[name: string, sex: string ... 1 more field]`\n\nSo first let us transfer the records of the Spark Dataset back to a Scala collection. This will give us a Scala `Array[Person]`, which implies that the original data type is preserved, like with the RDD API.","dateUpdated":"2019-04-01T15:15:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Notable observations</h3>\n<p>There are two important aspects of the result above:<br/>1. The result is a <code>Dataset[Person]</code>, which indicates that a Dataset is statically typed.<br/>2. Nevertheless the result seems to have a schema, which is indicated by the representation <code>[name: string, sex: string ... 1 more field]</code></p>\n<p>So first let us transfer the records of the Spark Dataset back to a Scala collection. This will give us a Scala <code>Array[Person]</code>, which implies that the original data type is preserved, like with the RDD API.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131703903_-2082772718","id":"20190329-200826_848383403","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:106"},{"text":"val array = persons.collect()\narray.foreach(println)","dateUpdated":"2019-04-01T15:15:03+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"array: Array[Person] = Array(Person(Alice,female,32), Person(Bob,male,24), Person(Christine,female,42), Person(Dale,male,19), Person(Eve,female,33))\nPerson(Alice,female,32)\nPerson(Bob,male,24)\nPerson(Christine,female,42)\nPerson(Dale,male,19)\nPerson(Eve,female,33)\n"}]},"apps":[],"jobName":"paragraph_1554131703908_-2098547423","id":"20170105-002313_1230126684","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:107"},{"text":"%md\n### Inspect Schema\n\nWe already saw a slight indication, that the Dataset also has a schema, like we know it from a DataFrame. This means that a Dataset provides an API with a two complementary characters: One API working with class instances (`Person` in our case) like RDDs and another API working with a schema and columns like a DataFrame.\n\nLet us inspect the schema now:","dateUpdated":"2019-04-01T15:15:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Inspect Schema</h3>\n<p>We already saw a slight indication, that the Dataset also has a schema, like we know it from a DataFrame. This means that a Dataset provides an API with a two complementary characters: One API working with class instances (<code>Person</code> in our case) like RDDs and another API working with a schema and columns like a DataFrame.</p>\n<p>Let us inspect the schema now:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131703917_-2102010163","id":"20160617-205347_1951340760","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"text":"persons.schema","dateUpdated":"2019-04-01T15:15:03+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres3: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(sex,StringType,true), StructField(age,IntegerType,false))\n"}]},"apps":[],"jobName":"paragraph_1554131703925_-2092776189","id":"20160617-201326_798645553","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"text":"persons.printSchema()","dateUpdated":"2019-04-01T15:15:03+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":124.60000610351562,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- name: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- age: integer (nullable = false)\n\n"}]},"apps":[],"jobName":"paragraph_1554131703934_-2094699934","id":"20170105-010005_1268070331","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"%md\n## 1.2 Loading Data\n\nSo far we only saw how we can create a Spark Dataset from a local Scala collection. Of course this is not suitable when working with large amounts of data. Therefore we want to load the data again from a distributed file system.\n\nIn the following example, we also see how a Spark DataFrame (which provides a flexible schema at runtime and therefore is not really statically typed) can be transformed into a Dataset. The full example is a two step operation:\n1. Load data from JSON as a Spark Dataframe\n2. Convert the DataFrame to a Spark Dataset of Person objects","dateUpdated":"2019-04-01T15:15:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.2 Loading Data</h2>\n<p>So far we only saw how we can create a Spark Dataset from a local Scala collection. Of course this is not suitable when working with large amounts of data. Therefore we want to load the data again from a distributed file system.</p>\n<p>In the following example, we also see how a Spark DataFrame (which provides a flexible schema at runtime and therefore is not really statically typed) can be transformed into a Dataset. The full example is a two step operation:<br/>1. Load data from JSON as a Spark Dataframe<br/>2. Convert the DataFrame to a Spark Dataset of Person objects</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131703941_-2012748418","id":"20170105-010041_2117356113","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"text":"import org.apache.spark.sql.types.IntegerType\n\nval personsDf = spark.read.json(\"s3://dimajix-training/data/persons.json\")\n    .withColumn(\"age\", $\"age\".cast(IntegerType))","dateUpdated":"2019-04-01T15:15:03+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1554131703950_-2014672163","id":"20170105-010057_986098899","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"text":"val persons = personsDf.as[Person]","dateUpdated":"2019-04-01T15:15:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1554131703958_-2005438189","id":"20190324-182634_83254903","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"text":"%md\n### Creating a Dataset\n\nThe last operation returned a simple Spark DataFrame. Using the `as[U]` method, we can now convert the DataFrame to a strongly typed Dataset.  The method used to map columns depend on the type of U:\n\n* When U is a class, fields for the class will be mapped to columns of the same name (case sensitivity is determined by `spark.sql.caseSensitive`).\n* When U is a tuple, the columns will be mapped by ordinal (i.e. the first column will be assigned to _1).\n* When U is a primitive type (i.e. String, Int, etc), then the first column of the DataFrame will be used.\n\nIf the schema of the Dataset does not match the desired U type, you can use select along with alias or as to rearrange or rename as required.\n\nNote that `as[]` only changes the view of the data that is passed into typed operations, such as map(), and does not eagerly project away any columns that are not present in the specified class. ","dateUpdated":"2019-04-01T15:15:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating a Dataset</h3>\n<p>The last operation returned a simple Spark DataFrame. Using the <code>as[U]</code> method, we can now convert the DataFrame to a strongly typed Dataset. The method used to map columns depend on the type of U:</p>\n<ul>\n  <li>When U is a class, fields for the class will be mapped to columns of the same name (case sensitivity is determined by <code>spark.sql.caseSensitive</code>).</li>\n  <li>When U is a tuple, the columns will be mapped by ordinal (i.e. the first column will be assigned to _1).\n</li>\n  <li>When U is a primitive type (i.e. String, Int, etc), then the first column of the DataFrame will be used.</li>\n</ul>\n<p>If the schema of the Dataset does not match the desired U type, you can use select along with alias or as to rearrange or rename as required.</p>\n<p>Note that <code>as[]</code> only changes the view of the data that is passed into typed operations, such as map(), and does not eagerly project away any columns that are not present in the specified class.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131703967_-2008900929","id":"20190324-182427_1088977353","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"text":"%md\n### Note Schema vs Class Definition\n\nAs is noted above, the `as[]` operation does not eagerly remove columns not present in the class. Therefore the schema of the Dataset is still unchanged and contains all columns.","dateUpdated":"2019-04-01T15:15:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Note Schema vs Class Definition</h3>\n<p>As is noted above, the <code>as[]</code> operation does not eagerly remove columns not present in the class. Therefore the schema of the Dataset is still unchanged and contains all columns.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131703975_-2024290885","id":"20170105-010721_139087314","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"text":"persons.printSchema()","dateUpdated":"2019-04-01T15:15:03+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- age: integer (nullable = true)\n |-- height: long (nullable = true)\n |-- name: string (nullable = true)\n |-- sex: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1554131703983_-2027368876","id":"20170105-010659_1991798171","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116"},{"text":"%md\n### Pretty Printing\n\nNow that we have a meaningful Dataset, we want to display it again nicely in Zeppelin. This can be done via `z.show(dataset)`","dateUpdated":"2019-04-01T15:15:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pretty Printing</h3>\n<p>Now that we have a meaningful Dataset, we want to display it again nicely in Zeppelin. This can be done via <code>z.show(dataset)</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131703992_-2020058647","id":"20160618-054348_706632489","dateCreated":"2019-04-01T15:15:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:117"},{"text":"z.show(persons)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":204,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\n14\t156\tAlice\tfemale\n21\t181\tBob\tmale\n27\t176\tCharlie\tmale\n24\t167\tEve\tfemale\n19\t172\tFrances\tfemale\n31\t191\tGeorge\tmale\n"}]},"apps":[],"jobName":"paragraph_1554131704001_-2035833352","id":"20160618-054337_2135335309","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:118"},{"text":"%md\n# 2 DataFrame vs Datasets\n\nWe already saw that Datasets seem to share some concepts with DataFrames. Actually it is the other way round, a DataFrame is simply a type alias for `Dataset[Row]`. This means that a Dataset provides all functionality of a DataFrame (since the DataFrame is only a special Dataset), but not all functions of a Dataset can be used with DataFrames (because the DataFrame `Row` object does not provide a static schema which is known at compile time).\n\nWe already saw how to convert a DataFrame to a Dataset with the `as[U]` method. A Dataset can be converted to a DataFrame with the `toDF` method.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>2 DataFrame vs Datasets</h1>\n<p>We already saw that Datasets seem to share some concepts with DataFrames. Actually it is the other way round, a DataFrame is simply a type alias for <code>Dataset[Row]</code>. This means that a Dataset provides all functionality of a DataFrame (since the DataFrame is only a special Dataset), but not all functions of a Dataset can be used with DataFrames (because the DataFrame <code>Row</code> object does not provide a static schema which is known at compile time).</p>\n<p>We already saw how to convert a DataFrame to a Dataset with the <code>as[U]</code> method. A Dataset can be converted to a DataFrame with the <code>toDF</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704009_-2038911343","id":"20190329-202334_558853676","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:119"},{"text":"%md\n# 3 Simple Transformations\n\nLike with DataFrames, the next step is to perform simple transformations. Datasets offer both RDD like and DataFrame like operations, we will have a look at both variants.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>3 Simple Transformations</h1>\n<p>Like with DataFrames, the next step is to perform simple transformations. Datasets offer both RDD like and DataFrame like operations, we will have a look at both variants.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704017_-2029677370","id":"20160617-205415_2045240993","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:120"},{"text":"%md\n## 3.1 `map` et al\n\nThe simplest RDD transformation was a `map`. Datasets also provide exactly this type of transformation.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.1 <code>map</code> et al</h2>\n<p>The simplest RDD transformation was a <code>map</code>. Datasets also provide exactly this type of transformation.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704024_-2032370612","id":"20181112-191819_759653816","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:121"},{"text":"val result = persons.map(p => p.name.toUpperCase)\nz.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.Dataset[String] = [value: string]\n"},{"type":"TABLE","data":"value\nALICE\nBOB\nCHRISTINE\nDALE\nEVE\n"}]},"apps":[],"jobName":"paragraph_1554131704029_-2034294357","id":"20181112-191841_1123152213","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:122"},{"text":"%md\nIf the result is a single primitive value of String, the column name of the associated schema is `value`. For complex objects (like case classes or tuples), the column names of the schema are inferred from the type. For example if we map each `Person` object to a tuple, the column names are `_1` and `_2`, since these are the member variables of a tuple.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>If the result is a single primitive value of String, the column name of the associated schema is <code>value</code>. For complex objects (like case classes or tuples), the column names of the schema are inferred from the type. For example if we map each <code>Person</code> object to a tuple, the column names are <code>_1</code> and <code>_2</code>, since these are the member variables of a tuple.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704036_-2049299564","id":"20190330-083931_1955268295","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:123"},{"text":"val result = persons.map(p => (p.name.toUpperCase, p.sex))\nz.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.Dataset[(String, String)] = [_1: string, _2: string]\n"},{"type":"TABLE","data":"_1\t_2\nALICE\tfemale\nBOB\tmale\nCHRISTINE\tfemale\nDALE\tmale\nEVE\tfemale\n"}]},"apps":[],"jobName":"paragraph_1554131704040_-2050838559","id":"20190330-083805_105763294","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:124"},{"text":"%md\n## 3.2 DataFrame `select` (untyped)\n\nSince a Dataset provides the full DataFrame API, we can also use a simple but powerful `select` method. Since the compiler cannot know the data types of the expressions, the result is again a DataFrame together with a schema which is determined at run time (as opposed to compile time)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.2 DataFrame <code>select</code> (untyped)</h2>\n<p>Since a Dataset provides the full DataFrame API, we can also use a simple but powerful <code>select</code> method. Since the compiler cannot know the data types of the expressions, the result is again a DataFrame together with a schema which is determined at run time (as opposed to compile time)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704043_-2050453810","id":"20181112-191810_1129285467","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:125"},{"text":"val result = persons.select($\"name\", $\"age\")","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.DataFrame = [name: string, age: int]\n"}]},"apps":[],"jobName":"paragraph_1554131704046_-2051608057","id":"20190329-203321_1419396404","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:126"},{"text":"z.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nAlice\t32\nBob\t24\nChristine\t42\nDale\t19\nEve\t33\n"}]},"apps":[],"jobName":"paragraph_1554131704049_-2041989335","id":"20190329-203819_89819805","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:127"},{"text":"%md\n## 3.3 Dataset `select` (typed)\n\nIn addition to the DataFrame `select`, Spark also provides a Dataset `select`, which uses statically typed columns. The additional explicit data types provides the compiler with enough information to return a statically typed Dataset again as opposed to a dynamically typed DataFrame. You can convert any untyped Spark column to a statically typed column via the `as` method as follows:","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.3 Dataset <code>select</code> (typed)</h2>\n<p>In addition to the DataFrame <code>select</code>, Spark also provides a Dataset <code>select</code>, which uses statically typed columns. The additional explicit data types provides the compiler with enough information to return a statically typed Dataset again as opposed to a dynamically typed DataFrame. You can convert any untyped Spark column to a statically typed column via the <code>as</code> method as follows:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704052_-2043143581","id":"20190329-203337_1582218243","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128"},{"text":"val result = persons.select($\"name\".as[String], $\"age\".as[String])","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.Dataset[(String, String)] = [name: string, age: int]\n"}]},"apps":[],"jobName":"paragraph_1554131704055_-2042758832","id":"20160618-055305_406692908","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:129"},{"text":"z.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nAlice\t32\nBob\t24\nChristine\t42\nDale\t19\nEve\t33\n"}]},"apps":[],"jobName":"paragraph_1554131704057_-2045067326","id":"20190329-203841_1370796177","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:130"},{"text":"%md\n# 4 Filtering\n\nFor most operations, the Dataset offers both a RDD-like typed variant and a DataFrame-like untyped variant. For example filtering can be performed via two complementary operations.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>4 Filtering</h1>\n<p>For most operations, the Dataset offers both a RDD-like typed variant and a DataFrame-like untyped variant. For example filtering can be performed via two complementary operations.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704060_-2046221573","id":"20190330-084239_1539668241","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:131"},{"text":"%md\n## 4.1 Typed Dataset filter","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4.1 Typed Dataset filter</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704063_-2045836824","id":"20190330-084416_973621737","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:132"},{"text":"val result = persons.filter(_.sex == \"male\")\nz.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.Dataset[Person] = [name: string, sex: string ... 1 more field]\n"},{"type":"TABLE","data":"name\tsex\tage\nBob\tmale\t24\nDale\tmale\t19\n"}]},"apps":[],"jobName":"paragraph_1554131704066_-1960807317","id":"20190330-084435_1832619107","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133"},{"text":"%md\n## 4.2 Untyped DataFrame filter","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4.2 Untyped DataFrame filter</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704068_-1963115810","id":"20190330-084508_1012939599","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:134"},{"text":"val result = persons.filter($\"sex\" === \"male\")\nz.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.Dataset[Person] = [name: string, sex: string ... 1 more field]\n"},{"type":"TABLE","data":"name\tsex\tage\nBob\tmale\t24\nDale\tmale\t19\n"}]},"apps":[],"jobName":"paragraph_1554131704070_-1962346312","id":"20190330-084528_1524419907","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:135"},{"text":"%md\n# 5 Joins\n\nJoin operations are another important area with two different implementations, which also differ significantly with regards to the result data type.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>5 Joins</h1>\n<p>Join operations are another important area with two different implementations, which also differ significantly with regards to the result data type.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704072_-1964654806","id":"20181112-191450_2081317570","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:136"},{"text":"%md\n## 5.1 Typed Dataset Joining\n\nThe Dataset join operation is typed and will return a new Dataset containing a tuple containing records from both incoming Datasets. Note that the join condition itself is formulated as *an untyped* column expression as opposed to a Scala expression.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>5.1 Typed Dataset Joining</h2>\n<p>The Dataset join operation is typed and will return a new Dataset containing a tuple containing records from both incoming Datasets. Note that the join condition itself is formulated as <em>an untyped</em> column expression as opposed to a Scala expression.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704073_-1965039555","id":"20160618-064756_323624134","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:137"},{"text":"case class PersonAddress (\n    name:String,\n    city:String\n) { }\n\nval addresses = spark.read.json(\"s3a://dimajix-training/data/addresses.json\")\n    .as[PersonAddress]","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1554131704074_-1963885308","id":"20160618-064806_552944470","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:138"},{"text":"val result = persons.as(\"person\").joinWith(addresses.as(\"address\"), $\"person.name\" === $\"address.name\")","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":124,"optionOpen":false,"keys":[{"name":"_1","index":0,"aggr":"sum"}],"values":[{"name":"_2","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"_1","index":0,"aggr":"sum"},"yAxis":{"name":"_2","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"_1\t_2\n[14,156,Alice,female]\t[Hamburg,Alice]\n[21,181,Bob,male]\t[Frankfurt,Bob]\n"}]},"apps":[],"jobName":"paragraph_1554131704075_-1964270057","id":"20160618-065315_531175389","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:139"},{"text":"result.printSchema","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _1: struct (nullable = false)\n |    |-- age: integer (nullable = true)\n |    |-- height: long (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- sex: string (nullable = true)\n |-- _2: struct (nullable = false)\n |    |-- city: string (nullable = true)\n |    |-- name: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1554131704076_-1966193801","id":"20170131-224714_1038353131","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:140"},{"text":"%md\n### Observations\nNote that the return type is a tuple of both incoming data types. Also note that the associated schema is now nested.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Observations</h3>\n<p>Note that the return type is a tuple of both incoming data types. Also note that the associated schema is now nested.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704077_-1966578550","id":"20190330-085927_2041027946","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:141"},{"text":"z.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres23: org.apache.spark.sql.Dataset[(Person, PersonAddress)] = [_1: struct<age: int, height: bigint ... 2 more fields>, _2: struct<city: string, name: string>]\n"}]},"apps":[],"jobName":"paragraph_1554131704078_-1965424304","id":"20170105-003325_556063070","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:142"},{"text":"%md\n## 5.2 DataFrame like Joining\n\nYou can also use traditional DataFrame join methods, these will return a DataFrame instead of a Dataset","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>5.2 DataFrame like Joining</h2>\n<p>You can also use traditional DataFrame join methods, these will return a DataFrame instead of a Dataset</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704079_-1965809052","id":"20170105-003438_1163697573","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:143"},{"text":"val result = persons.as(\"person\").join(addresses.as(\"address\"), $\"person.name\" === $\"address.name\")\nz.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":129,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\tcity\tname\n14\t156\tAlice\tfemale\tHamburg\tAlice\n21\t181\tBob\tmale\tFrankfurt\tBob\n"}]},"apps":[],"jobName":"paragraph_1554131704080_-1955420832","id":"20170105-003456_603691475","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:144"},{"text":"%md\n### Observations\nNote that when using the simpler untyped DataFrame `join` method, the return type now simply is a DataFrame and the schema is also not nested any more.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Observations</h3>\n<p>Note that when using the simpler untyped DataFrame <code>join</code> method, the return type now simply is a DataFrame and the schema is also not nested any more.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704081_-1955805581","id":"20190330-090108_1671243149","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:145"},{"text":"result.printSchema","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- age: integer (nullable = true)\n |-- height: long (nullable = true)\n |-- name: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- city: string (nullable = true)\n |-- name: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1554131704083_-1955036083","id":"20170105-003538_2088315656","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:146"},{"text":"%md\n# 6 Grouping & Aggrgeation","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>6 Grouping &amp; Aggrgeation</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704085_-1957344577","id":"20181112-191520_1613620509","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147"},{"text":"%md\n## 6.1 Typed Dataset Grouping\n\nAlso typed grouping operations are supported. The pattern is very similar to the DataFrame `groupBy` operations, except that we need to use the method `groupByKey` which returns a special  \"KeyValueGroupedDataset\". This return type forces us API-wise to specify the desired aggregation as the next operation.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>6.1 Typed Dataset Grouping</h2>\n<p>Also typed grouping operations are supported. The pattern is very similar to the DataFrame <code>groupBy</code> operations, except that we need to use the method <code>groupByKey</code> which returns a special &ldquo;KeyValueGroupedDataset&rdquo;. This return type forces us API-wise to specify the desired aggregation as the next operation.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704086_-1956190330","id":"20160618-060107_408877976","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148"},{"text":"persons.groupByKey(_.sex)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res4: org.apache.spark.sql.KeyValueGroupedDataset[String,Person] = org.apache.spark.sql.KeyValueGroupedDataset@2b07d1b7\n"}]},"apps":[],"jobName":"paragraph_1554131704088_-1958498823","id":"20170105-011441_1643155245","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:149"},{"text":"val result = persons.groupByKey(_.sex).agg(sum($\"age\").as(\"sum_age\").as[Long])","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":159,"optionOpen":false,"keys":[{"name":"value","index":0,"aggr":"sum"}],"values":[{"name":"sum_age","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"value","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, sum_age: bigint]\n"}]},"apps":[],"jobName":"paragraph_1554131704090_-1957729326","id":"20160617-205426_874417461","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:150"},{"text":"z.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"value\tsum_age\nfemale\t107\nmale\t43\n"}]},"apps":[],"jobName":"paragraph_1554131704091_-1958114074","id":"20170105-003813_1445330503","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:151"},{"text":"%md\n## 6.2 Use groupByKey with mapGroups\n\nAnother very flexible and powerful transformation is given by the `mapGroups` operation of a `KeyValueGroupedDataset` which allows you to specify any Scala transformation. This transformation will receive the grouping key and a list of all records to be aggregated.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>6.2 Use groupByKey with mapGroups</h2>\n<p>Another very flexible and powerful transformation is given by the <code>mapGroups</code> operation of a <code>KeyValueGroupedDataset</code> which allows you to specify any Scala transformation. This transformation will receive the grouping key and a list of all records to be aggregated.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704092_-1960037819","id":"20170105-004009_780928702","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:152"},{"text":"// Define a new case class for result type. We also could have used a tuple instead\ncase class AverageAge(\n    sex:String,\n    average:Double\n)\n\n// Define a simple Scala averaging method for persons age\ndef avg_age(persons:Iterator[Person]) = {\n    val (age,count) = persons\n        .map(p => (p.age,1))\n        .reduce((l,r) => (l._1 + r._1, l._2 + r._2))\n        \n    age.toFloat / count.toFloat\n}\n\n// Perform aggregation\nval result = persons\n    .groupByKey(_.sex)\n    .mapGroups { case (sex,values) => AverageAge(sex, avg_age(values)) }","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":154,"optionOpen":false,"keys":[{"name":"_1","index":0,"aggr":"sum"}],"values":[{"name":"_2","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"_1","index":0,"aggr":"sum"},"yAxis":{"name":"_2","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class AverageAge\navg_age: (persons: Iterator[Person])Float\nresult: org.apache.spark.sql.Dataset[AverageAge] = [sex: string, average: double]\n"}]},"apps":[],"jobName":"paragraph_1554131704093_-1960422568","id":"20160618-060120_1186022942","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153"},{"text":"z.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"sex\taverage\nfemale\t35.66666793823242\nmale\t21.5\n"}]},"apps":[],"jobName":"paragraph_1554131704094_-1959268321","id":"20190330-090519_2054911268","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:154"},{"text":"%md\n## 6.3 DataFrame groupBy\n\nFinally we can still use the DataFrame `groupBy` method, which returns a `RelationalGroupedDataset` (instead of a `KeyValueGroupedDataset`) which provides methods for untyped aggregations.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>6.3 DataFrame groupBy</h2>\n<p>Finally we can still use the DataFrame <code>groupBy</code> method, which returns a <code>RelationalGroupedDataset</code> (instead of a <code>KeyValueGroupedDataset</code>) which provides methods for untyped aggregations.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704095_-1959653070","id":"20170105-003951_1649173549","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:155"},{"text":"val result = persons.groupBy($\"sex\").agg(sum($\"age\").as(\"sum_age\"))","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":134,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.sql.DataFrame = [sex: string, sum_age: bigint]\n"}]},"apps":[],"jobName":"paragraph_1554131704097_-1974273528","id":"20170105-004005_1889169884","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:156"},{"text":"z.show(result)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"sex\tsum_age\nfemale\t107\nmale\t43\n"}]},"apps":[],"jobName":"paragraph_1554131704098_-1973119282","id":"20170131-225205_2110338895","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:157"},{"text":"%md\n# 7 Dataset <> RDD C7onversions\n\nWe already saw how to convert a DataFrame to a typed Dataset via the `as[U]` method and how to convert a Dataset back to a DataFrame using the `toDF` method. But of course it is also possible to convert a Dataset to an RDD and vice versa.\n\nBut note that you should come up with a very good reason to do so, because a Dataset offers almost the same functionality as a RDD, and it is much more powerful and flexible.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>7 Dataset &lt;&gt; RDD C7onversions</h1>\n<p>We already saw how to convert a DataFrame to a typed Dataset via the <code>as[U]</code> method and how to convert a Dataset back to a DataFrame using the <code>toDF</code> method. But of course it is also possible to convert a Dataset to an RDD and vice versa.</p>\n<p>But note that you should come up with a very good reason to do so, because a Dataset offers almost the same functionality as a RDD, and it is much more powerful and flexible.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704099_-1973504030","id":"20181112-191652_124673017","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:158"},{"text":"%md\n## 7.1 Convert to RDD\n\nYou can convert a Dataset to a simpler RDD. This will finally drop unused columns.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>7.1 Convert to RDD</h2>\n<p>You can convert a Dataset to a simpler RDD. This will finally drop unused columns.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704101_-1975812524","id":"20160617-201355_1310344176","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:159"},{"text":"val rdd = persons.rdd\n\nrdd.collect().foreach(println)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rdd: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[36] at rdd at <console>:30\nPerson(Alice,female,32)\nPerson(Bob,male,24)\nPerson(Christine,female,42)\nPerson(Dale,male,19)\nPerson(Eve,female,33)\n"}]},"apps":[],"jobName":"paragraph_1554131704102_-1974658277","id":"20160618-053912_1439030409","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:160"},{"text":"%md\n## 7.2 Convert back RDD to Dataset\n\nNote that the Dataset will not contain the unused column \"height\" any more.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>7.2 Convert back RDD to Dataset</h2>\n<p>Note that the Dataset will not contain the unused column &ldquo;height&rdquo; any more.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704104_-1976966771","id":"20160618-053953_1578828210","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:161"},{"text":"val ds = spark.createDataset(rdd)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ds: org.apache.spark.sql.Dataset[Person] = [name: string, sex: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1554131704106_-1976197273","id":"20160618-053926_647385200","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:162"},{"text":"z.show(ds)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tsex\tage\nAlice\tfemale\t32\nBob\tmale\t24\nChristine\tfemale\t42\nDale\tmale\t19\nEve\tfemale\t33\n"}]},"apps":[],"jobName":"paragraph_1554131704107_-1976582022","id":"20190330-100131_1600317691","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:163"},{"text":"%md\n# 8 I/O Operations\n\nWe already saw how to load data into a typed Dataset from an external storage layer like HDFS and S3. But of course you can also save a Dataset to an external file. Both operations are done using the `DataFrameReader` and `DataFrameWriter` classes, i.e. there is no special class for perform I/O operations with typed Datasets. This implies that I/O operations are always untyped. The good (and magical) thing about this approach is that you can load and store data contained in strongly typed classes using generic file formats like JSON, Parquet or event CSV.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>8 I/O Operations</h1>\n<p>We already saw how to load data into a typed Dataset from an external storage layer like HDFS and S3. But of course you can also save a Dataset to an external file. Both operations are done using the <code>DataFrameReader</code> and <code>DataFrameWriter</code> classes, i.e. there is no special class for perform I/O operations with typed Datasets. This implies that I/O operations are always untyped. The good (and magical) thing about this approach is that you can load and store data contained in strongly typed classes using generic file formats like JSON, Parquet or event CSV.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704109_-1978890515","id":"20181112-191613_1426194629","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:164"},{"text":"%md\n## 8.1 Write to Parquet\n\nSince we already have the `persons` Dataset at hand, let us first store it as Parquet files. This is done exactly the same way as with DataFrames (since this will use exactly the same operations and a `DataFrameWriter` object)\n\n1. Get DataFrame writer\n2. Write Dataset to Parquet\n\nNote that this will also write unused columns!","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>8.1 Write to Parquet</h2>\n<p>Since we already have the <code>persons</code> Dataset at hand, let us first store it as Parquet files. This is done exactly the same way as with DataFrames (since this will use exactly the same operations and a <code>DataFrameWriter</code> object)</p>\n<ol>\n  <li>Get DataFrame writer</li>\n  <li>Write Dataset to Parquet</li>\n</ol>\n<p>Note that this will also write unused columns!</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704111_-1978121017","id":"20160617-201512_1954263996","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:165"},{"text":"import org.apache.spark.sql.SaveMode\n\npersons.write.mode(SaveMode.Overwrite).parquet(\"persons.parquet\")","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.SaveMode\n"}]},"apps":[],"jobName":"paragraph_1554131704113_-1968117546","id":"20160617-201343_563402997","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:166"},{"text":"%md\n## 8.2 Read from Parquet\n\nNow that we have stored all persons as Parquet files, we want to read them back. As noted above, this is done by using the generic `DataFrameReader` object from the current Spark session and then cast it to a Dataset of the desired type (`Person` in our case).\n\n1. Read DataFrame from Parquet\n2. Convert DataFrame to DataSet\n\nSee the unused column \"height\" reappear aggain.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>8.2 Read from Parquet</h2>\n<p>Now that we have stored all persons as Parquet files, we want to read them back. As noted above, this is done by using the generic <code>DataFrameReader</code> object from the current Spark session and then cast it to a Dataset of the desired type (<code>Person</code> in our case).</p>\n<ol>\n  <li>Read DataFrame from Parquet</li>\n  <li>Convert DataFrame to DataSet</li>\n</ol>\n<p>See the unused column &ldquo;height&rdquo; reappear aggain.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704115_-1967348048","id":"20160617-204919_1431109393","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:167"},{"text":"val df = sqlContext.read.parquet(\"persons.parquet\")\nval ds = df.as[Person]","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":244,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [name: string, sex: string ... 1 more field]\nds: org.apache.spark.sql.Dataset[Person] = [name: string, sex: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1554131704127_-1971965035","id":"20160617-204932_794634446","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:168"},{"text":"z.show(ds)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tsex\tage\nChristine\tfemale\t42\nAlice\tfemale\t32\nEve\tfemale\t33\nDale\tmale\t19\nBob\tmale\t24\n"}]},"apps":[],"jobName":"paragraph_1554131704130_-1985431246","id":"20190330-100818_1857206773","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:169"},{"text":"z.show(df)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tsex\tage\nChristine\tfemale\t42\nAlice\tfemale\t32\nEve\tfemale\t33\nDale\tmale\t19\nBob\tmale\t24\n"}]},"apps":[],"jobName":"paragraph_1554131704132_-1987739740","id":"20190330-100826_898477002","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:170"},{"text":"%md\n# 9 Nested Classes\n\nAs we have already seen in the section about joins, Dataset can also represent nested object structures. In contrast to a traditional ORM, this will not be stored in a normalized model as separate tables, but as a single wide and nested table containing all sub entities.\n\nLet us make up a small example with persons and their addresses, stored in a single nested object.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>9 Nested Classes</h1>\n<p>As we have already seen in the section about joins, Dataset can also represent nested object structures. In contrast to a traditional ORM, this will not be stored in a normalized model as separate tables, but as a single wide and nested table containing all sub entities.</p>\n<p>Let us make up a small example with persons and their addresses, stored in a single nested object.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704133_-1988124489","id":"20160617-202215_225550959","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:171"},{"text":"// First the Address of a person\ncase class Address(\n    city:String,\n    street:String\n)\n    \n// Now the Person itself with an array of Addresses where they live    \ncase class Person(\n    name:String,\n    sex:String,\n    addresses:Array[Address]\n)\n","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Address\ndefined class Person\n"}]},"apps":[],"jobName":"paragraph_1554131704134_-1986970242","id":"20160618-093335_468111792","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:172"},{"text":"// Create an example Dataset\nval ds = spark.createDataset(\n    Seq(\n        Person(\"Alice\",\"female\",Array(Address(\"Berlin\",\"Hauptstraße 2\"),Address(\"Stuttgart\",\"Am Anger 13\"))),\n        Person(\"Bob\",\"male\",Array(Address(\"Mainz\",\"Kellergasse 23\")))\n    )\n)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ds: org.apache.spark.sql.Dataset[Person] = [name: string, sex: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1554131704134_-1986970242","id":"20160618-093452_1868521561","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:173"},{"text":"%md\nWhen we now inspect the schema of the Person, we see that it contains an array element with the addresses.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>When we now inspect the schema of the Person, we see that it contains an array element with the addresses.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704136_-1989278735","id":"20190330-101130_1464676885","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:174"},{"text":"ds.printSchema()","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- name: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- addresses: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- city: string (nullable = true)\n |    |    |-- street: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1554131704137_-1989663484","id":"20160618-093933_727106056","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:175"},{"text":"%md\n## 9.1 I/O with nested entities\n\nWhen you want to store complex structures with arrays or nested elements, you need to chose a file format which actually supports these types. Important examples are Parquet, JSON and ORC.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>9.1 I/O with nested entities</h2>\n<p>When you want to store complex structures with arrays or nested elements, you need to chose a file format which actually supports these types. Important examples are Parquet, JSON and ORC.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704138_-1988509238","id":"20190330-101220_2004287299","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:176"},{"text":"%md\n### Writing","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Writing</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704139_-1988893986","id":"20190330-101356_2057442010","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:177"},{"text":"import org.apache.spark.sql.SaveMode\n\nds.write.mode(SaveMode.Overwrite).parquet(\"persons2.parquet\")","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.SaveMode\n"}]},"apps":[],"jobName":"paragraph_1554131704140_-1990817731","id":"20160618-093622_1119250452","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:178"},{"text":"%md\n### Reading","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Reading</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704141_-1991202480","id":"20190330-101333_1784791362","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:179"},{"text":"val ds = sqlContext.read.parquet(\"persons2.parquet\").as[Person]","dateUpdated":"2019-04-01T15:15:04+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"sex","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"sex","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ds: org.apache.spark.sql.Dataset[Person] = [name: string, sex: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1554131704142_-1990048233","id":"20160618-093745_1829082972","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:180"},{"text":"z.show(ds)","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tsex\taddresses\nAlice\tfemale\tWrappedArray([Berlin,Hauptstraße 2], [Stuttgart,Am Anger 13])\nBob\tmale\tWrappedArray([Mainz,Kellergasse 23])\n"}]},"apps":[],"jobName":"paragraph_1554131704143_-1990432982","id":"20190330-101327_201392999","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:181"},{"text":"%md\n# 10 When to use what\n\nNow that we have three different Spark APIs (RDD, DataFrame and Dataset), the natural question to ask is \"when should I use which API\". Luckily a clear answer can be given from my perspective:\n\n1. Always try to avoid RDD API. It is the initial \"low level\" API of Spark, which is difficult to use and does not receive much development any more. Technically it is still important, since the RDD API is the execution vehicle for DataFrames and Datasets. But it does not offer much room for optimizations by Spark, since all transformations are given as Scala code and are a block box from Sparks point of view. Therefore Spark cannot simply concatenate operations, or even swap operations.\n2. Always try to use the DataFrame API. The main downside of the DataFrame API is that it does not provide you a statically typed code, and threfore a compiler cannot detect errors like missing columns. These types of errors are only detected during runtime. But since all transformations are specified using column placeholders, Spark can understand exactly what you try to achive. This is an important precondition for Spark to perform optimizations on the execution plan.\n3. If required due to complex transformation logic, you may want fall back to Dataset operations. But be aware of a slight performancy penalty, since Spark has to pack/unpack raw values into an object representation.\n\nThis expresses my experience, and I almsot never had to use the Dataset API and could express almsot everything using the DataFrame API so far.","dateUpdated":"2019-04-01T15:15:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>10 When to use what</h1>\n<p>Now that we have three different Spark APIs (RDD, DataFrame and Dataset), the natural question to ask is &ldquo;when should I use which API&rdquo;. Luckily a clear answer can be given from my perspective:</p>\n<ol>\n  <li>Always try to avoid RDD API. It is the initial &ldquo;low level&rdquo; API of Spark, which is difficult to use and does not receive much development any more. Technically it is still important, since the RDD API is the execution vehicle for DataFrames and Datasets. But it does not offer much room for optimizations by Spark, since all transformations are given as Scala code and are a block box from Sparks point of view. Therefore Spark cannot simply concatenate operations, or even swap operations.</li>\n  <li>Always try to use the DataFrame API. The main downside of the DataFrame API is that it does not provide you a statically typed code, and threfore a compiler cannot detect errors like missing columns. These types of errors are only detected during runtime. But since all transformations are specified using column placeholders, Spark can understand exactly what you try to achive. This is an important precondition for Spark to perform optimizations on the execution plan.</li>\n  <li>If required due to complex transformation logic, you may want fall back to Dataset operations. But be aware of a slight performancy penalty, since Spark has to pack/unpack raw values into an object representation.</li>\n</ol>\n<p>This expresses my experience, and I almsot never had to use the Dataset API and could express almsot everything using the DataFrame API so far.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1554131704144_-1980044762","id":"20160618-093840_324274912","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:182"},{"text":"","dateUpdated":"2019-04-01T15:15:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1554131704145_-1980429511","id":"20190330-091348_1402977558","dateCreated":"2019-04-01T15:15:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:183"}],"name":"Spark Dataset - Full","id":"2E9HH71ND","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}