{"paragraphs":[{"text":"%md\n# Working with Hive in Spark\nSpark can access the Hive metastore catalog, and therefore you can access all tables in Hive with Spark. You can even execute DDL statements from Spark.","user":"anonymous","dateUpdated":"2018-03-24T17:58:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521914238117_-682244161","id":"20180324-175718_871629966","dateCreated":"2018-03-24T17:57:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:36698","dateFinished":"2018-03-24T17:58:01+0000","dateStarted":"2018-03-24T17:58:01+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Working with Hive in Spark</h1>\n<p>Spark can access the Hive metastore catalog, and therefore you can access all tables in Hive with Spark. You can even execute DDL statements from Spark.</p>\n</div>"}]}},{"text":"%sql\nshow databases","dateUpdated":"2018-03-24T17:53:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{},"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"databaseName\ndefault\nweather\n"}]},"apps":[],"jobName":"paragraph_1521914031135_-1125934619","id":"20180321-123728_522441985","dateCreated":"2018-03-24T17:53:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:35648"},{"text":"val df = spark.read.table(\"weather.weather\")\nz.show(df.limit(10))\n","dateUpdated":"2018-03-24T17:53:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [year: string, usaf: string ... 11 more fields]\n"},{"type":"TABLE","data":"year\tusaf\twban\tdate\ttime\treport_type\twind_direction\twind_direction_qual\twind_observation\twind_speed\twind_speed_qual\tair_temperature\tair_temperature_qual\n2004\t010015\t99999\t20050101\t0020\tFM-16\t210\t1\tN\t4.1\t1\t2.0\t1\n2004\t010015\t99999\t20050101\t0050\tFM-15\t200\t1\tN\t4.1\t1\t2.0\t1\n2004\t010015\t99999\t20050101\t0120\tFM-16\t190\t1\tN\t4.1\t1\t2.0\t1\n2004\t010015\t99999\t20050101\t0220\tFM-16\t230\t1\tN\t4.1\t1\t2.0\t1\n2004\t010015\t99999\t20050101\t0250\tFM-15\t210\t1\tN\t4.1\t1\t2.0\t1\n2004\t010015\t99999\t20050102\t0950\tFM-15\t320\t1\tN\t3.1\t1\t0.0\t1\n2004\t010015\t99999\t20050102\t1050\tFM-15\t999\t9\tV\t1.5\t1\t0.0\t1\n2004\t010015\t99999\t20050102\t1120\tFM-16\t300\t1\tN\t3.6\t1\t0.0\t1\n2004\t010015\t99999\t20050102\t1150\tFM-15\t300\t1\tN\t2.6\t1\t0.0\t1\n2004\t010015\t99999\t20050102\t1220\tFM-16\t300\t1\tN\t2.6\t1\t0.0\t1\n"}]},"apps":[],"jobName":"paragraph_1521914031135_-1125934619","id":"20180321-123739_1070130850","dateCreated":"2018-03-24T17:53:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35649"},{"text":"%sql\nUSE weather","dateUpdated":"2018-03-24T17:53:51+0000","config":{"tableHide":true,"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":""}]},"apps":[],"jobName":"paragraph_1521914031137_-1140555077","id":"20180321-124446_449217463","dateCreated":"2018-03-24T17:53:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35652"},{"text":"%sql\nSHOW TABLES","dateUpdated":"2018-03-24T17:53:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{},"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"database\ttableName\tisTemporary\nweather\tcopy_of_stations\tfalse\nweather\traw_data\tfalse\nweather\tstations\tfalse\nweather\tweather\tfalse\nweather\tweather_single\tfalse\n\tweather_all\ttrue\n\tweather_single\ttrue\n"}]},"apps":[],"jobName":"paragraph_1521914031138_-1139400830","id":"20180321-124746_1648402801","dateCreated":"2018-03-24T17:53:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35653"},{"text":"%md\n## Storing Data\nOf course it is also possible to store data from Spark into Hive tables. You can use either `saveAsTable` which creates a new Hive table or `insertInto` which inserts the data into an existing Hive table.\n","user":"anonymous","dateUpdated":"2018-03-24T17:55:17+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Storing Data</h2>\n<p>Of course it is also possible to store data from Spark into Hive tables. You can use either <code>saveAsTable</code> which creates a new Hive table or <code>insertInto</code> which inserts the data into an existing Hive table.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521914067667_953679159","id":"20180324-175427_913859003","dateCreated":"2018-03-24T17:54:27+0000","dateStarted":"2018-03-24T17:55:17+0000","dateFinished":"2018-03-24T17:55:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:35654"},{"text":"val df = spark.read.table(\"weather_single\")\ndf.write.saveAsTable(\"weather.weather_single\")\n","dateUpdated":"2018-03-24T17:53:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1521914031139_-1139785579","id":"20180321-125237_766819112","dateCreated":"2018-03-24T17:53:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35655"},{"text":"val df = spark.read.table(\"weather_single\")\ndf.write.insertInto(\"weather.weather_single_csv\")\n","dateUpdated":"2018-03-24T17:53:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [usaf: string, wban: string ... 10 more fields]\n"}]},"apps":[],"jobName":"paragraph_1521914031139_-1139785579","id":"20180321-125401_1244155945","dateCreated":"2018-03-24T17:53:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35656"},{"text":"%md\n## Saving into Partitions\nIn many cases, you need to store data into a partitioned table. Unfortunately there is no easy way to do that with programmatic SQL. Instead a viable alternative is to register the corresponding DataFrame as a temporary view and then exectute some SQL to store the data into a Hive table.","user":"anonymous","dateUpdated":"2018-03-24T17:56:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Saving into Partitions</h2>\n<p>In many cases, you need to store data into a partitioned table. Unfortunately there is no easy way to do that with programmatic SQL. Instead a viable alternative is to register the corresponding DataFrame as a temporary view and then exectute some SQL to store the data into a Hive table.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521914142014_1402219755","id":"20180324-175542_2030606250","dateCreated":"2018-03-24T17:55:42+0000","dateStarted":"2018-03-24T17:56:49+0000","dateFinished":"2018-03-24T17:56:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:35657"},{"text":"val batchlauf_datum = \"2018-03-21\"\ndf.createOrReplaceTempView(\"temp_view\")\nspark.sql(s\"INSERT OVERWRITE TABLE meine_aprtitionierte_tabelle PARTITION(date='$batchlauf_datum') SELECT * FROM temp_view\")","dateUpdated":"2018-03-24T17:53:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"batchlauf_datum: String = 2018-03-21\norg.apache.spark.sql.AnalysisException: Table or view not found: temp_view; line 1 pos 95\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:649)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:601)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:631)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:624)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:624)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:570)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:637)\n  ... 51 elided\n"}]},"apps":[],"jobName":"paragraph_1521914031140_-1141709324","id":"20180321-125822_40142792","dateCreated":"2018-03-24T17:53:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35658"},{"dateUpdated":"2018-03-24T17:53:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521914031141_-1142094073","id":"20180321-131132_1062798584","dateCreated":"2018-03-24T17:53:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35659"}],"name":"Spark Hive","id":"2D8RD62XM","angularObjects":{"2D98FMAH1:shared_process":[],"2D7JMBHMW:shared_process":[],"2D97KQQMQ:shared_process":[],"2D75J9G8A:shared_process":[],"2D7EN88U2:shared_process":[],"2D9JW72F7:shared_process":[],"2D833H41P:shared_process":[],"2D9GNB6ZN:shared_process":[],"2D6C3QSGG:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}