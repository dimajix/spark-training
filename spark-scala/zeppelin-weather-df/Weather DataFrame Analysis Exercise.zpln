{"paragraphs":[{"text":"%md\n# Weather Data Analytics\nThis notebook performs some basic weather data analytics using the Spark DataFrame interface.","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Weather Data Analytics</h1>\n<p>This notebook performs some basic weather data analytics using the Spark DataFrame interface.</p>\n"}]},"apps":[],"jobName":"paragraph_1558525061008_969875391","id":"20160612-173621_1713564499","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:64355"},{"text":"// Common data location - adjust if not correct\nval storageLocation = \"s3://dimajix-training/data/weather\"","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558525061009_1524812082","id":"20160612-174608_1747052107","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64356"},{"text":"%md\n# Read in Weather Data\n\nFirst we need to read in all data. Data is stored in different directories for different years. Each directory contains one file per weather station. First lets peek into the single year `2003`, which is located at `storageLocation/2003`. This gives us a basic feeling for the data.\n\nSince the data is not CSV or something else, you need to use the `read.text` method of the Spark session (available in the notebook via `spark`), which will simply treat each line as a single column called `value`.\n\nStore the result in a variable `rawWeatherData2003` and display the first 10 lines using the Zepplin command `z.show`.","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{"storageLocation":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Read in Weather Data</h1>\n<p>First we need to read in all data. Data is stored in different directories for different years. Each directory contains one file per weather station. First lets peek into the single year <code>2003</code>, which is located at <code>storageLocation/2003</code>. This gives us a basic feeling for the data.</p>\n<p>Since the data is not CSV or something else, you need to use the <code>read.text</code> method of the Spark session (available in the notebook via <code>spark</code>), which will simply treat each line as a single column called <code>value</code>.</p>\n<p>Store the result in a variable <code>rawWeatherData2003</code> and display the first 10 lines using the Zepplin command <code>z.show</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1558525061009_-710573632","id":"20180310-111936_205472093","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64357"},{"text":"val rawWeatherData2003 = // YOUR CODE HERE\nz.show(/* YOUR CODE HERE* /)\n","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558525061010_2114571682","id":"20180310-112322_125525","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64358"},{"text":"%md\n### Read in all years\nNow we read in all years by creating a union. We also add the year as a logical partition column, this will be used later.\n\nIn order to read in all years, we use Scala functional programming, this makes things a lot more concise.","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Read in all years</h3>\n<p>Now we read in all years by creating a union. We also add the year as a logical partition column, this will be used later.</p>\n<p>In order to read in all years, we use Scala functional programming, this makes things a lot more concise.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1558525061010_-112380450","id":"20180310-113423_29861183","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64359"},{"text":"val rawWeatherData = (2003 to 2014) map {i => spark.read.text(s\"${storageLocation}/${i}\").withColumn(\"year\", lit(i)) } reduce((l,r) => l.union(r))\nz.show(rawWeatherData.limit(10))","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558525061010_1492481172","id":"20160612-174154_349496240","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64360"},{"text":"%md\n## Extract Information\n\nThe raw data is not exactly nice to work with, so we need to extract the relevant information by using appropriate `substr` operations.","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Extract Information</h2>\n<p>The raw data is not exactly nice to work with, so we need to extract the relevant information by using appropriate <code>substr</code> operations.</p>\n"}]},"apps":[],"jobName":"paragraph_1558525061010_-1313452783","id":"20180310-113446_1901710073","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64361"},{"text":"import org.apache.spark.sql.types._\n\nval weatherData = rawWeatherData.select(\n    col(\"year\"),\n    substring(col(\"value\"),5,6) as \"usaf\",\n    substring(col(\"value\"),11,5) as \"wban\",\n    substring(col(\"value\"),16,8) as \"date\",\n    substring(col(\"value\"),24,4) as \"time\",\n    substring(col(\"value\"),42,5) as \"report_type\",\n    substring(col(\"value\"),61,3) as \"wind_direction\",\n    substring(col(\"value\"),64,1) as \"wind_direction_qual\",\n    substring(col(\"value\"),65,1) as \"wind_observation\",\n    substring(col(\"value\"),66,4).cast(FloatType) / lit(10.0) as \"wind_speed\",\n    substring(col(\"value\"),70,1) as \"wind_speed_qual\",\n    substring(col(\"value\"),88,5).cast(FloatType) / lit(10.0) as \"air_temperature\",\n    substring(col(\"value\"),93,1) as \"air_temperature_qual\"\n)\n\nz.show(weatherData.limit(10))","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"result","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"result","index":0,"aggr":"sum"}}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558525061010_-1614777850","id":"20160612-175008_182048218","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64362"},{"text":"%md\n# Read in Station Metadata\n\nFortunately station metadata is stored as CSV, so we can directly read that using Sparks `read.csv` mechanisum. Read the data from the CSV file located at `$storageLocation/isd-history`. You should also specify the DataFrameReader option `header` to be `true`, this will use the first line of the CSV for creating column names.\n\nStore the result in a variable called `stationData` and again print the first 10 lines using `z.show`.\n\n**Note** the countries are NOT ISO codes, they are FIPS-codes: [https://en.wikipedia.org/wiki/List_of_FIPS_country_codes](https://en.wikipedia.org/wiki/List_of_FIPS_country_codes)","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{"storageLocation":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Read in Station Metadata</h1>\n<p>Fortunately station metadata is stored as CSV, so we can directly read that using Sparks <code>read.csv</code> mechanisum. Read the data from the CSV file located at <code>$storageLocation/isd-history</code>. You should also specify the DataFrameReader option <code>header</code> to be <code>true</code>, this will use the first line of the CSV for creating column names.</p>\n<p>Store the result in a variable called <code>stationData</code> and again print the first 10 lines using <code>z.show</code>.</p>\n<p><strong>Note</strong> the countries are NOT ISO codes, they are FIPS-codes: <a href=\"https://en.wikipedia.org/wiki/List_of_FIPS_country_codes\">https://en.wikipedia.org/wiki/List_of_FIPS_country_codes</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1558525061011_-893385359","id":"20160612-175533_1179463035","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64363"},{"text":"val stationData = // YOUR CODE HERE\n    \nz.show(/* YOUR CODE HERE */)","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"result","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"result","index":0,"aggr":"sum"}}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558525061011_-1513680203","id":"20160612-174633_119344646","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64364"},{"text":"%md\n# Analytics\n\nNow that we have everything in DataFrames, we can do the analysis easily.\n\n1. Load data as DataFrames (already done)\n2. Join both DataFrames on the station code (`wban` and `usaf`)\n3. Extract year from date (or use existing column)\n4. Project on relevant columns (`year`,`wind_speed`,`wind_speed_qual`,`air_temperature`,`air_temperature_qual`,`ctry`)\n5. Rename column `ctry` to `country`\n6. Group by country and year\n7. Aggregate minimum/maximum values for wind and temperature, pay attention to quality! You have to ignore values for which the quality is not \"1\"!\n8. Print the results, such that Zeppelin can make nice graphics :)\n \n**Again note** the countries are NOT ISO codes, they are FIPS-codes: [https://en.wikipedia.org/wiki/List_of_FIPS_country_codes](https://en.wikipedia.org/wiki/List_of_FIPS_country_codes), so don't be surprised if apparently the temperature of your country seems to be a little off.\n","user":"anonymous","dateUpdated":"2019-05-22T11:57:06+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Analytics</h1>\n<p>Now that we have everything in DataFrames, we can do the analysis easily.</p>\n<ol>\n  <li>Load data as DataFrames (already done)</li>\n  <li>Join both DataFrames on the station code (<code>wban</code> and <code>usaf</code>)</li>\n  <li>Extract year from date (or use existing column)</li>\n  <li>Project on relevant columns (<code>year</code>,<code>wind_speed</code>,<code>wind_speed_qual</code>,<code>air_temperature</code>,<code>air_temperature_qual</code>,<code>ctry</code>)</li>\n  <li>Rename column <code>ctry</code> to <code>country</code></li>\n  <li>Group by country and year</li>\n  <li>Aggregate minimum/maximum values for wind and temperature, pay attention to quality! You have to ignore values for which the quality is not &ldquo;1&rdquo;!</li>\n  <li>Print the results, such that Zeppelin can make nice graphics :)</li>\n</ol>\n<p><strong>Again note</strong> the countries are NOT ISO codes, they are FIPS-codes: <a href=\"https://en.wikipedia.org/wiki/List_of_FIPS_country_codes\">https://en.wikipedia.org/wiki/List_of_FIPS_country_codes</a>, so don&rsquo;t be surprised if apparently the temperature of your country seems to be a little off.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1558525061011_-548949","id":"20160612-175151_696167562","dateCreated":"2019-05-22T11:37:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:64365","dateFinished":"2019-05-22T11:57:06+0000","dateStarted":"2019-05-22T11:57:06+0000"},{"text":"// 2. Join data\nval joinedWeather = // YOUR CODE HERE\n\n// Display first five records\njoinedWeather.limit(5).show(truncate=false)","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"country","index":0,"aggr":"sum"}],"values":[{"name":"min_wind_speed","index":2,"aggr":"min"},{"name":"max_wind_speed","index":3,"aggr":"max"},{"name":"min_temperature","index":4,"aggr":"min"},{"name":"max_temperature","index":5,"aggr":"max"}],"groups":[{"name":"year","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"country","index":0,"aggr":"sum"},"yAxis":{"name":"year","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558525061011_-926235955","id":"20160612-180048_998679946","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64366"},{"text":"// 3., 4., 5. Project on relevant columns, extract year and rename ctry to countrs\nval projectedData = // YOUR CODE HERE\n\n// Display first five records\nprojectedData.limit(5).show(truncate=false)","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558525061011_-1941727835","id":"20160612-180513_1457527764","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64367"},{"text":"// 6. Group by country and year\nval groupedData = // YOUR CODE HERE\n\n// 7. Perform min/max aggregation, but ignore invalid values where quality is not 1\nval result = // YOUR CODE HERE","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558525061011_706211790","id":"20190404-113508_755247433","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64368"},{"text":"// 8. Print results\nz.show(result)","user":"anonymous","dateUpdated":"2019-05-22T11:37:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558525061011_-1073908418","id":"20190404-113529_900507199","dateCreated":"2019-05-22T11:37:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64369"}],"name":"Weather DataFrame Analysis Exercise","id":"2EC31QHQR","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}