{"paragraphs":[{"text":"%md\n# Load Data\nFirst we load the data from S3. We use the built-in \"csv\" method, which can use the first line has column names and which also supports infering the schema automatically. We use both and save some code for specifying the schema explictly.\n\nWe also peek inside the data by retrieving the first five records.","dateUpdated":"2018-03-20T18:07:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Load Data</h1>\n<p>First we load the data from S3. We use the built-in &ldquo;csv&rdquo; method, which can use the first line has column names and which also supports infering the schema automatically. We use both and save some code for specifying the schema explictly.</p>\n<p>We also peek inside the data by retrieving the first five records.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521569220417_-2079710518","id":"20180224-150345_1979378836","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3901"},{"text":"val rawData = spark.read\n    .option(\"header\",\"true\")\n    .option(\"inferSchema\",\"true\")\n    .csv(\"s3://dimajix-training/data/kc-house-data/\")\n\nz.show(rawData.limit(10))","dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rawData: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 19 more fields]\n"},{"type":"TABLE","data":"id\tdate\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\n7129300520\t20141013T000000\t221900\t3\t1.0\t1180\t5650\t1.0\t0\t0\t3\t7\t1180\t0\t1955\t0\t98178\t47.5112\t-122.257\t1340\t5650\n6414100192\t20141209T000000\t538000\t3\t2.25\t2570\t7242\t2.0\t0\t0\t3\t7\t2170\t400\t1951\t1991\t98125\t47.721\t-122.319\t1690\t7639\n5631500400\t20150225T000000\t180000\t2\t1.0\t770\t10000\t1.0\t0\t0\t3\t6\t770\t0\t1933\t0\t98028\t47.7379\t-122.233\t2720\t8062\n2487200875\t20141209T000000\t604000\t4\t3.0\t1960\t5000\t1.0\t0\t0\t5\t7\t1050\t910\t1965\t0\t98136\t47.5208\t-122.393\t1360\t5000\n1954400510\t20150218T000000\t510000\t3\t2.0\t1680\t8080\t1.0\t0\t0\t3\t8\t1680\t0\t1987\t0\t98074\t47.6168\t-122.045\t1800\t7503\n7237550310\t20140512T000000\t1225000\t4\t4.5\t5420\t101930\t1.0\t0\t0\t3\t11\t3890\t1530\t2001\t0\t98053\t47.6561\t-122.005\t4760\t101930\n1321400060\t20140627T000000\t257500\t3\t2.25\t1715\t6819\t2.0\t0\t0\t3\t7\t1715\t0\t1995\t0\t98003\t47.3097\t-122.327\t2238\t6819\n2008000270\t20150115T000000\t291850\t3\t1.5\t1060\t9711\t1.0\t0\t0\t3\t7\t1060\t0\t1963\t0\t98198\t47.4095\t-122.315\t1650\t9711\n2414600126\t20150415T000000\t229500\t3\t1.0\t1780\t7470\t1.0\t0\t0\t3\t7\t1050\t730\t1960\t0\t98146\t47.5123\t-122.337\t1780\t8113\n3793500160\t20150312T000000\t323000\t3\t2.5\t1890\t6560\t2.0\t0\t0\t3\t7\t1890\t0\t2003\t0\t98038\t47.3684\t-122.031\t2390\t7570\n"}]},"apps":[],"jobName":"paragraph_1521569220418_-2078556271","id":"20180224-145450_1588429371","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3902"},{"text":"// Split the data - 80% for training, 20% for validation\nval Array(trainData, validationData) = rawData.randomSplit(Array(0.8,0.2))\n\nprintln(s\"traingData = ${trainData.count}\")\nprintln(s\"validationData = ${validationData.count}\")","dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, date: string ... 19 more fields]\nvalidationData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, date: string ... 19 more fields]\ntraingData = 17261\nvalidationData = 4352\n"}]},"apps":[],"jobName":"paragraph_1521569220418_-2078556271","id":"20180304-103415_198808093","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3903"},{"text":"%md\n# Adding more Features\nThe RMSE tells us that on average our prediction actually performs pretty bad. How can we improve that? Obviously we used only the size of the house for the price prediction so far, but we have a whole lot of additional information. So let's make use of that. The mathematical idea is that we create a more complex (but still linear) model that also includes other features.\n\nLet's recall that a linear  model looks as follows:\n\n    y = SUM(coeff[i]*x[i]) + intercept\n    \nThis means that we are not limited to single feature `x`, but we can use many features `x[0]...x[n]`. Let's do that with the house data!","dateUpdated":"2018-03-20T18:07:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Adding more Features</h1>\n<p>The RMSE tells us that on average our prediction actually performs pretty bad. How can we improve that? Obviously we used only the size of the house for the price prediction so far, but we have a whole lot of additional information. So let&rsquo;s make use of that. The mathematical idea is that we create a more complex (but still linear) model that also includes other features.</p>\n<p>Let&rsquo;s recall that a linear model looks as follows:</p>\n<pre><code>y = SUM(coeff[i]*x[i]) + intercept\n</code></pre>\n<p>This means that we are not limited to single feature <code>x</code>, but we can use many features <code>x[0]...x[n]</code>. Let&rsquo;s do that with the house data!</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521569220418_-2078556271","id":"20180303-140614_1726950739","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3904"},{"text":"%md\n# Add more Features\n\nNow let's add even more features. Since we don't have any additional information, we model some of the features differently. So far we used all features as direct linear predictors, which implies that a grade of 4 is twice as good as 2. Maybe that is not the case and not all predictors have a linear influence. Specifically nominal and ordinal features should be modeled differntly as categories. More an that later.\n\nFirst let's have a look at the data agin using Spark `describe`","dateUpdated":"2018-03-20T18:07:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Add more Features</h1>\n<p>Now let's add even more features. Since we don't have any additional information, we model some of the features differently. So far we used all features as direct linear predictors, which implies that a grade of 4 is twice as good as 2. Maybe that is not the case and not all predictors have a linear influence. Specifically nominal and ordinal features should be modeled differntly as categories. More an that later.</p>\n<p>First let's have a look at the data agin using Spark <code>describe</code></p>\n"}]},"apps":[],"jobName":"paragraph_1521569220419_-2078941020","id":"20180224-153308_1063200189","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3905"},{"text":"z.show(rawData.describe())\n","dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"summary\tid\tdate\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\ncount\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\nmean\t4.580301520864988E9\tnull\t540088.1418\t3.37084162309721\t2.1147573219821405\t2079.8997362698374\t15106.967565816869\t1.4943089807060566\t0.007541757275713691\t0.23430342849211122\t3.4094295100171195\t7.656873178179799\t1788.3906907879516\t291.5090454818859\t1971.0051357978994\t84.40225790033776\t98077.93980474715\t47.56005251931704\t-122.21389640494083\t1986.552491556008\t12768.455651691113\nstddev\t2.8765655713120522E9\tnull\t367127.19648270035\t0.930061831147451\t0.770163157217741\t918.4408970468096\t41420.51151513551\t0.5399888951423489\t0.08651719772788748\t0.7663175692736114\t0.6507430463662044\t1.1754587569743344\t828.0909776519175\t442.57504267746685\t29.373410802386243\t401.67924001917504\t53.505026257472466\t0.13856371024192368\t0.14082834238139288\t685.3913042527788\t27304.179631338524\nmin\t1000102\t20140502T000000\t75000\t0\t0.0\t290\t520\t1.0\t0\t0\t1\t1\t290\t0\t1900\t0\t98001\t47.1559\t-122.519\t399\t651\nmax\t9900000190\t20150527T000000\t7700000\t33\t8.0\t13540\t1651359\t3.5\t1\t4\t5\t13\t9410\t4820\t2015\t2015\t98199\t47.7776\t-121.315\t6210\t871200\n"}]},"apps":[],"jobName":"paragraph_1521569220419_-2078941020","id":"20180303-143756_621375550","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3906"},{"text":"z.show(rawData.select(countDistinct(col(\"zipcode\"))))","dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"count(DISTINCT zipcode)\n70\n"}]},"apps":[],"jobName":"paragraph_1521569220419_-2078941020","id":"20180303-143935_1009221693","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3907"},{"text":"%md\n## New Features using One-Hot Encoding\n\nA simple but powerful method for creating new features from categories (i.e. nominal and ordinal features) is to use One-Hot-Encoding. For each nominal feature, the set of all possible values is indexed from 0 to some n. But since it cannot be assumed that larger values for n have a larger impact, a different approach is chosen. Instead each possible values is encoded by a 0/1 vector with only a single entry being one.\n\nLets try that with the tools Spark provides to us.","dateUpdated":"2018-03-20T18:07:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>New Features using One-Hot Encoding</h2>\n<p>A simple but powerful method for creating new features from categories (i.e. nominal and ordinal features) is to use One-Hot-Encoding. For each nominal feature, the set of all possible values is indexed from 0 to some n. But since it cannot be assumed that larger values for n have a larger impact, a different approach is chosen. Instead each possible values is encoded by a 0/1 vector with only a single entry being one.</p>\n<p>Lets try that with the tools Spark provides to us.</p>\n"}]},"apps":[],"jobName":"paragraph_1521569220419_-2078941020","id":"20180304-105127_866320180","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3908"},{"text":"%md\n### Indexing Nominal Data\nFirst we need to index the data. Since Spark cannot know, which or how many distinct values are present in a specific column, the `StringIndexer` works like a ML algorithm: First it needs to be fit to the data, thereby returning an `StringIndexerModel` which then can be used for transforming data.\n\nLet's perform both steps and let us look at the result","dateUpdated":"2018-03-20T18:07:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Indexing Nominal Data</h3>\n<p>First we need to index the data. Since Spark cannot know, which or how many distinct values are present in a specific column, the <code>StringIndexer</code> works like a ML algorithm: First it needs to be fit to the data, thereby returning an <code>StringIndexerModel</code> which then can be used for transforming data.</p>\n<p>Let's perform both steps and let us look at the result</p>\n"}]},"apps":[],"jobName":"paragraph_1521569220420_-2080864765","id":"20180310-150618_1032035630","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3909"},{"text":"import org.apache.spark.ml.feature._\n\nval indexer = new StringIndexer()\n    .setInputCol(\"zipcode\")\n    .setOutputCol(\"zipcode_idx\")\n    .setHandleInvalid(\"keep\")\n    \nval indexModel = indexer.fit(trainData)    \nval indexedZipData = indexModel.transform(trainData)\n\nz.show(indexedZipData.limit(10))\n","dateUpdated":"2018-03-20T18:07:00+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature._\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_b67fb1282e24\nindexModel: org.apache.spark.ml.feature.StringIndexerModel = strIdx_b67fb1282e24\nindexedZipData: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 20 more fields]\n"},{"type":"TABLE","data":"id\tdate\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\tzipcode_idx\n1000102\t20140916T000000\t280000\t6\t3.0\t2400\t9373\t2.0\t0\t0\t3\t7\t2400\t0\t1991\t0\t98002\t47.3262\t-122.214\t2060\t7316\t53.0\n1000102\t20150422T000000\t300000\t6\t3.0\t2400\t9373\t2.0\t0\t0\t3\t7\t2400\t0\t1991\t0\t98002\t47.3262\t-122.214\t2060\t7316\t53.0\n1200021\t20140811T000000\t400000\t3\t1.0\t1460\t43000\t1.0\t0\t0\t3\t7\t1460\t0\t1952\t0\t98166\t47.4434\t-122.347\t2250\t20023\t42.0\n2800031\t20150401T000000\t235000\t3\t1.0\t1430\t7599\t1.5\t0\t0\t4\t6\t1010\t420\t1930\t0\t98168\t47.4783\t-122.265\t1290\t10320\t38.0\n3600057\t20150319T000000\t402500\t4\t2.0\t1650\t3504\t1.0\t0\t0\t3\t7\t760\t890\t1951\t2013\t98144\t47.5803\t-122.294\t1480\t3504\t27.0\n3600072\t20150330T000000\t680000\t4\t2.75\t2220\t5310\t1.0\t0\t0\t5\t7\t1170\t1050\t1951\t0\t98144\t47.5801\t-122.294\t1540\t4200\t27.0\n3800008\t20150224T000000\t178000\t5\t1.5\t1990\t18200\t1.0\t0\t0\t3\t7\t1990\t0\t1960\t0\t98178\t47.4938\t-122.262\t1860\t8658\t40.0\n5200087\t20140709T000000\t487000\t4\t2.5\t2540\t5001\t2.0\t0\t0\t3\t9\t2540\t0\t2005\t0\t98108\t47.5423\t-122.302\t2360\t6834\t56.0\n6200017\t20141112T000000\t281000\t3\t1.0\t1340\t21336\t1.5\t0\t0\t4\t5\t1340\t0\t1945\t0\t98032\t47.4023\t-122.273\t1340\t37703\t62.0\n7200179\t20141016T000000\t150000\t2\t1.0\t840\t12750\t1.0\t0\t0\t3\t6\t840\t0\t1925\t0\t98055\t47.484\t-122.211\t1480\t6969\t43.0\n"}]},"apps":[],"jobName":"paragraph_1521569220420_-2080864765","id":"20180310-150203_394133894","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3910"},{"text":"%md\n### One-Hot-Encoder\nNow we have a single number (the index of the value) in a new column `zipcode_idx`. But in order to use the information in a linear model, we need to create sparse vectors from this index with only exactly one `1`. This can be done with the `OneHotEncoder` transformer. This time no fitting is required, the class can be used directly with its `transform` method.","dateUpdated":"2018-03-20T18:07:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>One-Hot-Encoder</h3>\n<p>Now we have a single number (the index of the value) in a new column <code>zipcode_idx</code>. But in order to use the information in a linear model, we need to create sparse vectors from this index with only exactly one <code>1</code>. This can be done with the <code>OneHotEncoder</code> transformer. This time no fitting is required, the class can be used directly with its <code>transform</code> method.</p>\n"}]},"apps":[],"jobName":"paragraph_1521569220420_-2080864765","id":"20180310-150817_1404988058","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3911"},{"text":"val encoder = new OneHotEncoder()\n    .setInputCol(\"zipcode_idx\")\n    .setOutputCol(\"zipcode_onehot\")\n\nval encodedZipData = encoder.transform(indexedZipData)\nz.show(encodedZipData.limit(10))","dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_ea48a8630d87\nencodedZipData: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 21 more fields]\n"},{"type":"TABLE","data":"id\tdate\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\tzipcode_idx\tzipcode_onehot\n1000102\t20140916T000000\t280000\t6\t3.0\t2400\t9373\t2.0\t0\t0\t3\t7\t2400\t0\t1991\t0\t98002\t47.3262\t-122.214\t2060\t7316\t53.0\t(70,[53],[1.0])\n1000102\t20150422T000000\t300000\t6\t3.0\t2400\t9373\t2.0\t0\t0\t3\t7\t2400\t0\t1991\t0\t98002\t47.3262\t-122.214\t2060\t7316\t53.0\t(70,[53],[1.0])\n1200021\t20140811T000000\t400000\t3\t1.0\t1460\t43000\t1.0\t0\t0\t3\t7\t1460\t0\t1952\t0\t98166\t47.4434\t-122.347\t2250\t20023\t42.0\t(70,[42],[1.0])\n2800031\t20150401T000000\t235000\t3\t1.0\t1430\t7599\t1.5\t0\t0\t4\t6\t1010\t420\t1930\t0\t98168\t47.4783\t-122.265\t1290\t10320\t38.0\t(70,[38],[1.0])\n3600057\t20150319T000000\t402500\t4\t2.0\t1650\t3504\t1.0\t0\t0\t3\t7\t760\t890\t1951\t2013\t98144\t47.5803\t-122.294\t1480\t3504\t27.0\t(70,[27],[1.0])\n3600072\t20150330T000000\t680000\t4\t2.75\t2220\t5310\t1.0\t0\t0\t5\t7\t1170\t1050\t1951\t0\t98144\t47.5801\t-122.294\t1540\t4200\t27.0\t(70,[27],[1.0])\n3800008\t20150224T000000\t178000\t5\t1.5\t1990\t18200\t1.0\t0\t0\t3\t7\t1990\t0\t1960\t0\t98178\t47.4938\t-122.262\t1860\t8658\t40.0\t(70,[40],[1.0])\n5200087\t20140709T000000\t487000\t4\t2.5\t2540\t5001\t2.0\t0\t0\t3\t9\t2540\t0\t2005\t0\t98108\t47.5423\t-122.302\t2360\t6834\t56.0\t(70,[56],[1.0])\n6200017\t20141112T000000\t281000\t3\t1.0\t1340\t21336\t1.5\t0\t0\t4\t5\t1340\t0\t1945\t0\t98032\t47.4023\t-122.273\t1340\t37703\t62.0\t(70,[62],[1.0])\n7200179\t20141016T000000\t150000\t2\t1.0\t840\t12750\t1.0\t0\t0\t3\t6\t840\t0\t1925\t0\t98055\t47.484\t-122.211\t1480\t6969\t43.0\t(70,[43],[1.0])\n"}]},"apps":[],"jobName":"paragraph_1521569220421_-2081249513","id":"20180310-150331_1565568506","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3912"},{"text":"%md\n# Creating Pipelines\n\nSince it would be tedious to add all features one after another and apply a full chain of transformations to the training set, the validation set and eventually to new data, Spark provides a `Pipeline` abstraction. A Pipeline simply contains a sequence of Transformations and (possibly multiple) machine learning algorithms. The whole pipeline then can be trained using the `fit` method which will return a `PipelineModel` instance. This instance contains all transformers and trained models and then can be used directly for prediction.","dateUpdated":"2018-03-20T19:05:07+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating Pipelines</h1>\n<p>Since it would be tedious to add all features one after another and apply a full chain of transformations to the training set, the validation set and eventually to new data, Spark provides a <code>Pipeline</code> abstraction. A Pipeline simply contains a sequence of Transformations and (possibly multiple) machine learning algorithms. The whole pipeline then can be trained using the <code>fit</code> method which will return a <code>PipelineModel</code> instance. This instance contains all transformers and trained models and then can be used directly for prediction.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521569220421_-2081249513","id":"20180310-145908_1673430871","dateCreated":"2018-03-20T18:07:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3913","user":"anonymous","dateFinished":"2018-03-20T19:05:07+0000","dateStarted":"2018-03-20T19:05:07+0000"},{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\n\nval pipeline = new Pipeline().setStages(Array(\n    // For every nominal feature, you have to create a pair of StringIndexer and OneHotEncoder. \n    // The StringIndexer should store its index result in some new column, which then is used \n    // by the OneHotEncoder to create a one-hot vector.\n    new StringIndexer()\n        .setInputCol(\"bathrooms\")\n        .setOutputCol(\"bathrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bathrooms_idx\")\n        .setOutputCol(\"bathrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"bedrooms\")\n        .setOutputCol(\"bedrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bedrooms_idx\")\n        .setOutputCol(\"bedrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"floors\")\n        .setOutputCol(\"floors_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"floors_idx\")\n        .setOutputCol(\"floors_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"view\")\n        .setOutputCol(\"view_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"condition\")\n        .setOutputCol(\"condition_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"grade\")\n        .setOutputCol(\"grade_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"grade_idx\")\n        .setOutputCol(\"grade_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"zipcode\")\n        .setOutputCol(\"zipcode_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"zipcode_idx\")\n        .setOutputCol(\"zipcode_onehot\"),\n    new VectorAssembler()\n        .setInputCols(Array(\"bedrooms_onehot\", \"bathrooms_onehot\", \"sqft_living\", \"sqft_lot\", \"floors_onehot\", \"waterfront\", \"view_onehot\", \"condition_onehot\", \"grade_onehot\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"zipcode_onehot\", \"sqft_living15\", \"sqft_lot15\"))\n        .setOutputCol(\"features\"),\n    new LinearRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"price\")\n    )\n)\n","dateUpdated":"2018-03-20T19:05:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\npipeline: org.apache.spark.ml.Pipeline = pipeline_9e637aee1b27\n"}]},"apps":[],"jobName":"paragraph_1521569220422_-2080095267","id":"20180225-145446_499092244","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3914"},{"text":"%md\n### Train model with training data\n\nOnce you created the `Pipeline`, you can fit it in a single step using the `fit` method. This will return an instance of the class `PipelineModel`. Assign this model instace to a value called `model`.\n\nAnd remember: Use the training data for fitting!","dateUpdated":"2018-03-20T19:12:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Train model with training data</h3>\n<p>Once you created the <code>Pipeline</code>, you can fit it in a single step using the <code>fit</code> method. This will return an instance of the class <code>PipelineModel</code>. Assign this model instace to a value called <code>model</code>.</p>\n<p>And remember: Use the training data for fitting!</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521569220422_-2080095267","id":"20180310-145712_108529033","dateCreated":"2018-03-20T18:07:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3915","user":"anonymous","dateFinished":"2018-03-20T19:12:31+0000","dateStarted":"2018-03-20T19:12:31+0000"},{"text":"val model = pipeline.fit(trainData)","dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"model: org.apache.spark.ml.PipelineModel = pipeline_9e637aee1b27\n"}]},"apps":[],"jobName":"paragraph_1521569220422_-2080095267","id":"20180310-145711_1974986569","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3916"},{"text":"%md\n### Evaluate model using validation data\n\nNow that we have a model, we need to measure its performance. This requires that predictions are created by applying the model to the validation data by using the `transform` method of the moodel. The quality metric of the prediction is implemented in the `RegressionEvaluator` class from the Spark ML evaluation package. Create an instance of the evaluator and configure it appropriately to use the column `price` as the target (label) variable and the column `prediction` (which has been created by the pipeline model) as the prediction column. Also remember to set the metric name to `rmse`. Finally feed in the predicted data into the evaluator, which in turn will calculate the desired quality metric (RMSE in our case).","dateUpdated":"2018-03-20T19:20:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Evaluate model using validation data</h3>\n<p>Now that we have a model, we need to measure its performance. This requires that predictions are created by applying the model to the validation data by using the <code>transform</code> method of the moodel. The quality metric of the prediction is implemented in the <code>RegressionEvaluator</code> class from the Spark ML evaluation package. Create an instance of the evaluator and configure it appropriately to use the column <code>price</code> as the target (label) variable and the column <code>prediction</code> (which has been created by the pipeline model) as the prediction column. Also remember to set the metric name to <code>rmse</code>. Finally feed in the predicted data into the evaluator, which in turn will calculate the desired quality metric (RMSE in our case).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521569220422_-2080095267","id":"20180310-145710_1425414098","dateCreated":"2018-03-20T18:07:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3917","user":"anonymous","dateFinished":"2018-03-20T19:20:39+0000","dateStarted":"2018-03-20T19:20:39+0000"},{"text":"import org.apache.spark.ml.evaluation._\n\n// Create and configure a RegressionEvaluator\nval evaluator = new RegressionEvaluator()\n    .setLabelCol(\"price\")\n    .setPredictionCol(\"prediction\")\n    .setMetricName(\"rmse\")\n    \n// Create predictions of the validationData by using the \"transform\" method of the model\nval pred = model.transform(validationData)\n\n// Now measure the quality of the prediction by using the \"evaluate\" method of the evaluator\nval rmse = evaluator.evaluate(pred)\n\nprintln(s\"RMSE = $rmse\")","dateUpdated":"2018-03-20T19:17:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.evaluation._\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_868c1a0437a8\npred: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 33 more fields]\nrmse: Double = 143170.76722302838\nRMSE = 143170.76722302838\n"}]},"apps":[],"jobName":"paragraph_1521569220423_-2080480016","id":"20180310-145710_924247341","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3918"},{"text":"%md\n# Adding more Models\nAnother way of improving the overall prediction is to add multiple models to a single Pipeline. Each downstream ML algorithm has access to the prediction of the previous stages. This way we can create two independant models and eventually fit a mixed model as the last step. In this example we want to use a simple linear model created by a `LinearRegression` and combine that model with a Poisson model created by a `GeneralizedLinearRegression`. The results of both models eventually are combined using a final `LinearRegression` model.","dateUpdated":"2018-03-20T19:27:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Adding more Models</h1>\n<p>Another way of improving the overall prediction is to add multiple models to a single Pipeline. Each downstream ML algorithm has access to the prediction of the previous stages. This way we can create two independant models and eventually fit a mixed model as the last step. In this example we want to use a simple linear model created by a <code>LinearRegression</code> and combine that model with a Poisson model created by a <code>GeneralizedLinearRegression</code>. The results of both models eventually are combined using a final <code>LinearRegression</code> model.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521569220423_-2080480016","id":"20180304-103951_1075865665","dateCreated":"2018-03-20T18:07:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3919","user":"anonymous","dateFinished":"2018-03-20T19:27:43+0000","dateStarted":"2018-03-20T19:27:43+0000"},{"text":"import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\n\n\nval pipeline = new Pipeline().setStages(Array(\n    new StringIndexer()\n        .setInputCol(\"bathrooms\")\n        .setOutputCol(\"bathrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bathrooms_idx\")\n        .setOutputCol(\"bathrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"bedrooms\")\n        .setOutputCol(\"bedrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bedrooms_idx\")\n        .setOutputCol(\"bedrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"floors\")\n        .setOutputCol(\"floors_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"floors_idx\")\n        .setOutputCol(\"floors_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"view\")\n        .setOutputCol(\"view_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"condition\")\n        .setOutputCol(\"condition_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"grade\")\n        .setOutputCol(\"grade_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"grade_idx\")\n        .setOutputCol(\"grade_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"zipcode\")\n        .setOutputCol(\"zipcode_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"zipcode_idx\")\n        .setOutputCol(\"zipcode_onehot\"),\n    new VectorAssembler()\n        .setInputCols(Array(\"bedrooms_onehot\", \"bathrooms_onehot\", \"sqft_living\", \"sqft_lot\", \"floors_onehot\", \"waterfront\", \"view_onehot\", \"condition_onehot\", \"grade_onehot\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"zipcode_onehot\", \"sqft_living15\", \"sqft_lot15\"))\n        .setOutputCol(\"features\"),\n    new LinearRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"price\")\n        .setPredictionCol(\"linear_prediction\"),\n    new GeneralizedLinearRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"price\")\n        .setFamily(\"poisson\")\n        .setLink(\"log\")\n        .setPredictionCol(\"poisson_prediction\"),\n    new VectorAssembler()\n        .setInputCols(Array(\"linear_prediction\",\"poisson_prediction\"))\n        .setOutputCol(\"pred_features\"),\n    new LinearRegression()\n        .setFeaturesCol(\"pred_features\")\n        .setLabelCol(\"price\")\n        .setPredictionCol(\"prediction\")\n    )\n)","dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\npipeline: org.apache.spark.ml.Pipeline = pipeline_f3d0f8fec1dc\n"}]},"apps":[],"jobName":"paragraph_1521569220423_-2080480016","id":"20180310-125337_48072730","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3920"},{"text":"%md\n### Train model with training data\n\nAgain as usual we train a model using the `fit` method of the pipeline.","dateUpdated":"2018-03-20T19:28:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Train model with training data</h3>\n<p>Again as usual we train a model using the <code>fit</code> method of the pipeline.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521569220423_-2080480016","id":"20180310-145611_1248206854","dateCreated":"2018-03-20T18:07:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3921","user":"anonymous","dateFinished":"2018-03-20T19:28:30+0000","dateStarted":"2018-03-20T19:28:30+0000"},{"text":"val model = pipeline.fit(trainData)","dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"model: org.apache.spark.ml.PipelineModel = pipeline_f3d0f8fec1dc\n"}]},"apps":[],"jobName":"paragraph_1521569220424_-2082403760","id":"20180310-145533_1332151548","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3922"},{"text":"%md\n### Evaluate model using validation data\n\nAnd eventually we measure the performance of the combined model by using the evaluator created some steps above.","dateUpdated":"2018-03-20T19:29:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Evaluate model using validation data</h3>\n<p>And eventually we measure the performance of the combined model by using the evaluator created some steps above.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521569220424_-2082403760","id":"20180310-145626_1980099531","dateCreated":"2018-03-20T18:07:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3923","user":"anonymous","dateFinished":"2018-03-20T19:29:24+0000","dateStarted":"2018-03-20T19:29:24+0000"},{"text":"// First create predictions by applying the learnt pipeline model to the validation data\nval pred = model.transform(validationData)\n\n// And now calculate the performance metric by using the evaluator on the predictions\nval rmse = evaluator.evaluate(pred)\n\nprintln(s\"RMSE = $rmse\")","dateUpdated":"2018-03-20T19:30:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pred: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 36 more fields]\nrmse: Double = 150299.1882805813\nRMSE = 150299.1882805813\n"}]},"apps":[],"jobName":"paragraph_1521569220424_-2082403760","id":"20180310-145542_1344236768","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3924"},{"text":"%md\n### Inspect Model\nLet us inspect the coefficients of the last step, which tells us which of both models (linear or poisson) has more weight.","dateUpdated":"2018-03-20T18:07:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Inspect Model</h3>\n<p>Let us inspect the coefficients of the last step, which tells us which of both models (linear or poisson) has more weight.</p>\n"}]},"apps":[],"jobName":"paragraph_1521569220425_-2082788509","id":"20180310-125931_108725758","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3925"},{"text":"model.stages.last.asInstanceOf[LinearRegressionModel].coefficients","dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res272: org.apache.spark.ml.linalg.Vector = [0.19420202720326232,0.8057979331534735]\n"}]},"apps":[],"jobName":"paragraph_1521569220425_-2082788509","id":"20180310-125654_760871982","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3926"},{"dateUpdated":"2018-03-20T18:07:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521569220425_-2082788509","id":"20180310-125739_1025770034","dateCreated":"2018-03-20T18:07:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3927"}],"name":"House Prices Pipeline Solution","id":"2DBYS4K15","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}