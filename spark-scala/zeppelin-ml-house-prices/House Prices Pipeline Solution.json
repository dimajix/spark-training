{"paragraphs":[{"text":"%md\n# Load Data\nFirst we load the data from S3. We use the built-in \"csv\" method, which can use the first line has column names and which also supports infering the schema automatically. We use both and save some code for specifying the schema explictly.\n\nWe also peek inside the data by retrieving the first five records.","dateUpdated":"2018-03-10T14:51:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Load Data</h1>\n<p>First we load the data from S3. We use the built-in &ldquo;csv&rdquo; method, which can use the first line has column names and which also supports infering the schema automatically. We use both and save some code for specifying the schema explictly.</p>\n<p>We also peek inside the data by retrieving the first five records.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520693473053_1770584998","id":"20180224-150345_1979378836","dateCreated":"2018-03-10T14:51:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:25878"},{"text":"val rawData = spark.read\n    .option(\"header\",\"true\")\n    .option(\"inferSchema\",\"true\")\n    .csv(\"s3://dimajix-training/data/kc-house-data/\")\n\nz.show(rawData.limit(10))","user":"anonymous","dateUpdated":"2018-03-10T14:58:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rawData: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 19 more fields]\n"},{"type":"TABLE","data":"id\tdate\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\n7129300520\t20141013T000000\t221900\t3\t1.0\t1180\t5650\t1.0\t0\t0\t3\t7\t1180\t0\t1955\t0\t98178\t47.5112\t-122.257\t1340\t5650\n6414100192\t20141209T000000\t538000\t3\t2.25\t2570\t7242\t2.0\t0\t0\t3\t7\t2170\t400\t1951\t1991\t98125\t47.721\t-122.319\t1690\t7639\n5631500400\t20150225T000000\t180000\t2\t1.0\t770\t10000\t1.0\t0\t0\t3\t6\t770\t0\t1933\t0\t98028\t47.7379\t-122.233\t2720\t8062\n2487200875\t20141209T000000\t604000\t4\t3.0\t1960\t5000\t1.0\t0\t0\t5\t7\t1050\t910\t1965\t0\t98136\t47.5208\t-122.393\t1360\t5000\n1954400510\t20150218T000000\t510000\t3\t2.0\t1680\t8080\t1.0\t0\t0\t3\t8\t1680\t0\t1987\t0\t98074\t47.6168\t-122.045\t1800\t7503\n7237550310\t20140512T000000\t1225000\t4\t4.5\t5420\t101930\t1.0\t0\t0\t3\t11\t3890\t1530\t2001\t0\t98053\t47.6561\t-122.005\t4760\t101930\n1321400060\t20140627T000000\t257500\t3\t2.25\t1715\t6819\t2.0\t0\t0\t3\t7\t1715\t0\t1995\t0\t98003\t47.3097\t-122.327\t2238\t6819\n2008000270\t20150115T000000\t291850\t3\t1.5\t1060\t9711\t1.0\t0\t0\t3\t7\t1060\t0\t1963\t0\t98198\t47.4095\t-122.315\t1650\t9711\n2414600126\t20150415T000000\t229500\t3\t1.0\t1780\t7470\t1.0\t0\t0\t3\t7\t1050\t730\t1960\t0\t98146\t47.5123\t-122.337\t1780\t8113\n3793500160\t20150312T000000\t323000\t3\t2.5\t1890\t6560\t2.0\t0\t0\t3\t7\t1890\t0\t2003\t0\t98038\t47.3684\t-122.031\t2390\t7570\n"}]},"apps":[],"jobName":"paragraph_1520693473053_1770584998","id":"20180224-145450_1588429371","dateCreated":"2018-03-10T14:51:13+0000","dateStarted":"2018-03-10T14:58:40+0000","dateFinished":"2018-03-10T14:58:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25879"},{"text":"// Split the data - 80% for training, 20% for validation\nval Array(trainData, validationData) = rawData.randomSplit(Array(0.8,0.2))\n\nprintln(s\"traingData = ${trainData.count}\")\nprintln(s\"validationData = ${validationData.count}\")","user":"anonymous","dateUpdated":"2018-03-10T15:04:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, date: string ... 19 more fields]\nvalidationData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, date: string ... 19 more fields]\ntraingData = 17261\nvalidationData = 4352\n"}]},"apps":[],"jobName":"paragraph_1520693473058_1757888284","id":"20180304-103415_198808093","dateCreated":"2018-03-10T14:51:13+0000","dateStarted":"2018-03-10T15:04:32+0000","dateFinished":"2018-03-10T15:04:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25880"},{"text":"%md\n# Adding more Features\nThe RMSE tells us that on average our prediction actually performs pretty bad. How can we improve that? Obviously we used only the size of the house for the price prediction so far, but we have a whole lot of additional information. So let's make use of that. The mathematical idea is that we create a more complex (but still linear) model that also includes other features.\n\nLet's recall that a linear  model looks as follows:\n\n    y = SUM(coeff[i]*x[i]) + intercept\n    \nThis means that we are not limited to single feature `x`, but we can use many features `x[0]...x[n]`. Let's do that with the house data!","dateUpdated":"2018-03-10T14:51:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Adding more Features</h1>\n<p>The RMSE tells us that on average our prediction actually performs pretty bad. How can we improve that? Obviously we used only the size of the house for the price prediction so far, but we have a whole lot of additional information. So let&rsquo;s make use of that. The mathematical idea is that we create a more complex (but still linear) model that also includes other features.</p>\n<p>Let&rsquo;s recall that a linear model looks as follows:</p>\n<pre><code>y = SUM(coeff[i]*x[i]) + intercept\n</code></pre>\n<p>This means that we are not limited to single feature <code>x</code>, but we can use many features <code>x[0]...x[n]</code>. Let&rsquo;s do that with the house data!</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520693473057_1756734038","id":"20180303-140614_1726950739","dateCreated":"2018-03-10T14:51:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25881"},{"text":"%md\n# Add more Features\n\nNow let's add even more features. Since we don't have any additional information, we model some of the features differently. So far we used all features as direct linear predictors, which implies that a grade of 4 is twice as good as 2. Maybe that is not the case and not all predictors have a linear influence. Specifically nominal and ordinal features should be modeled differntly as categories. More an that later.\n\nFirst let's have a look at the data agin using Spark `describe`","dateUpdated":"2018-03-10T14:51:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Add more Features</h1>\n<p>Now let's add even more features. Since we don't have any additional information, we model some of the features differently. So far we used all features as direct linear predictors, which implies that a grade of 4 is twice as good as 2. Maybe that is not the case and not all predictors have a linear influence. Specifically nominal and ordinal features should be modeled differntly as categories. More an that later.</p>\n<p>First let's have a look at the data agin using Spark <code>describe</code></p>\n"}]},"apps":[],"jobName":"paragraph_1520693473059_1757503536","id":"20180224-153308_1063200189","dateCreated":"2018-03-10T14:51:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25882"},{"text":"z.show(rawData.describe())\n","user":"anonymous","dateUpdated":"2018-03-10T14:58:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"summary\tid\tdate\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\ncount\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\t21613\nmean\t4.580301520864988E9\tnull\t540088.1418\t3.37084162309721\t2.1147573219821405\t2079.8997362698374\t15106.967565816869\t1.4943089807060566\t0.007541757275713691\t0.23430342849211122\t3.4094295100171195\t7.656873178179799\t1788.3906907879516\t291.5090454818859\t1971.0051357978994\t84.40225790033776\t98077.93980474715\t47.56005251931704\t-122.21389640494083\t1986.552491556008\t12768.455651691113\nstddev\t2.8765655713120522E9\tnull\t367127.19648270035\t0.930061831147451\t0.770163157217741\t918.4408970468096\t41420.51151513551\t0.5399888951423489\t0.08651719772788748\t0.7663175692736114\t0.6507430463662044\t1.1754587569743344\t828.0909776519175\t442.57504267746685\t29.373410802386243\t401.67924001917504\t53.505026257472466\t0.13856371024192368\t0.14082834238139288\t685.3913042527788\t27304.179631338524\nmin\t1000102\t20140502T000000\t75000\t0\t0.0\t290\t520\t1.0\t0\t0\t1\t1\t290\t0\t1900\t0\t98001\t47.1559\t-122.519\t399\t651\nmax\t9900000190\t20150527T000000\t7700000\t33\t8.0\t13540\t1651359\t3.5\t1\t4\t5\t13\t9410\t4820\t2015\t2015\t98199\t47.7776\t-121.315\t6210\t871200\n"}]},"apps":[],"jobName":"paragraph_1520693473059_1757503536","id":"20180303-143756_621375550","dateCreated":"2018-03-10T14:51:13+0000","dateStarted":"2018-03-10T14:58:55+0000","dateFinished":"2018-03-10T14:59:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25883"},{"text":"z.show(rawData.select(countDistinct(col(\"zipcode\"))))","user":"anonymous","dateUpdated":"2018-03-10T14:59:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"count(DISTINCT zipcode)\n70\n"}]},"apps":[],"jobName":"paragraph_1520693473059_1757503536","id":"20180303-143935_1009221693","dateCreated":"2018-03-10T14:51:13+0000","dateStarted":"2018-03-10T14:59:03+0000","dateFinished":"2018-03-10T14:59:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25884"},{"text":"%md\n## New Features using One-Hot Encoding\n\nA simple but powerful method for creating new features from categories (i.e. nominal and ordinal features) is to use One-Hot-Encoding. For each nominal feature, the set of all possible values is indexed from 0 to some n. But since it cannot be assumed that larger values for n have a larger impact, a different approach is chosen. Instead each possible values is encoded by a 0/1 vector with only a single entry being one.\n\nLets try that with the tools Spark provides to us.","user":"anonymous","dateUpdated":"2018-03-10T15:02:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>New Features using One-Hot Encoding</h2>\n<p>A simple but powerful method for creating new features from categories (i.e. nominal and ordinal features) is to use One-Hot-Encoding. For each nominal feature, the set of all possible values is indexed from 0 to some n. But since it cannot be assumed that larger values for n have a larger impact, a different approach is chosen. Instead each possible values is encoded by a 0/1 vector with only a single entry being one.</p>\n<p>Lets try that with the tools Spark provides to us.</p>\n"}]},"apps":[],"jobName":"paragraph_1520693473059_1757503536","id":"20180304-105127_866320180","dateCreated":"2018-03-10T14:51:13+0000","dateStarted":"2018-03-10T15:02:00+0000","dateFinished":"2018-03-10T15:02:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25885"},{"text":"%md\n### Indexing Nominal Data\nFirst we need to index the data. Since Spark cannot know, which or how many distinct values are present in a specific column, the `StringIndexer` works like a ML algorithm: First it needs to be fit to the data, thereby returning an `StringIndexerModel` which then can be used for transforming data.\n\nLet's perform both steps and let us look at the result","user":"anonymous","dateUpdated":"2018-03-10T15:08:14+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Indexing Nominal Data</h3>\n<p>First we need to index the data. Since Spark cannot know, which or how many distinct values are present in a specific column, the <code>StringIndexer</code> works like a ML algorithm: First it needs to be fit to the data, thereby returning an <code>StringIndexerModel</code> which then can be used for transforming data.</p>\n<p>Let's perform both steps and let us look at the result</p>\n"}]},"apps":[],"jobName":"paragraph_1520694378195_1668927566","id":"20180310-150618_1032035630","dateCreated":"2018-03-10T15:06:18+0000","dateStarted":"2018-03-10T15:08:14+0000","dateFinished":"2018-03-10T15:08:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25886"},{"text":"import org.apache.spark.ml.feature._\n\nval indexer = new StringIndexer()\n    .setInputCol(\"zipcode\")\n    .setOutputCol(\"zipcode_idx\")\n    .setHandleInvalid(\"keep\")\n    \nval indexModel = indexer.fit(trainData)    \nval indexedZipData = indexModel.transform(trainData)\n\nz.show(indexedZipData.limit(10))\n","user":"anonymous","dateUpdated":"2018-03-10T15:04:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature._\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_b67fb1282e24\nindexModel: org.apache.spark.ml.feature.StringIndexerModel = strIdx_b67fb1282e24\nindexedZipData: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 20 more fields]\n"},{"type":"TABLE","data":"id\tdate\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\tzipcode_idx\n1000102\t20140916T000000\t280000\t6\t3.0\t2400\t9373\t2.0\t0\t0\t3\t7\t2400\t0\t1991\t0\t98002\t47.3262\t-122.214\t2060\t7316\t53.0\n1000102\t20150422T000000\t300000\t6\t3.0\t2400\t9373\t2.0\t0\t0\t3\t7\t2400\t0\t1991\t0\t98002\t47.3262\t-122.214\t2060\t7316\t53.0\n1200021\t20140811T000000\t400000\t3\t1.0\t1460\t43000\t1.0\t0\t0\t3\t7\t1460\t0\t1952\t0\t98166\t47.4434\t-122.347\t2250\t20023\t42.0\n2800031\t20150401T000000\t235000\t3\t1.0\t1430\t7599\t1.5\t0\t0\t4\t6\t1010\t420\t1930\t0\t98168\t47.4783\t-122.265\t1290\t10320\t38.0\n3600057\t20150319T000000\t402500\t4\t2.0\t1650\t3504\t1.0\t0\t0\t3\t7\t760\t890\t1951\t2013\t98144\t47.5803\t-122.294\t1480\t3504\t27.0\n3600072\t20150330T000000\t680000\t4\t2.75\t2220\t5310\t1.0\t0\t0\t5\t7\t1170\t1050\t1951\t0\t98144\t47.5801\t-122.294\t1540\t4200\t27.0\n3800008\t20150224T000000\t178000\t5\t1.5\t1990\t18200\t1.0\t0\t0\t3\t7\t1990\t0\t1960\t0\t98178\t47.4938\t-122.262\t1860\t8658\t40.0\n5200087\t20140709T000000\t487000\t4\t2.5\t2540\t5001\t2.0\t0\t0\t3\t9\t2540\t0\t2005\t0\t98108\t47.5423\t-122.302\t2360\t6834\t56.0\n6200017\t20141112T000000\t281000\t3\t1.0\t1340\t21336\t1.5\t0\t0\t4\t5\t1340\t0\t1945\t0\t98032\t47.4023\t-122.273\t1340\t37703\t62.0\n7200179\t20141016T000000\t150000\t2\t1.0\t840\t12750\t1.0\t0\t0\t3\t6\t840\t0\t1925\t0\t98055\t47.484\t-122.211\t1480\t6969\t43.0\n"}]},"apps":[],"jobName":"paragraph_1520694123759_1343265243","id":"20180310-150203_394133894","dateCreated":"2018-03-10T15:02:03+0000","dateStarted":"2018-03-10T15:04:47+0000","dateFinished":"2018-03-10T15:04:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25887"},{"text":"%md\n### One-Hot-Encoder\nNow we have a single number (the index of the value) in a new column `zipcode_idx`. But in order to use the information in a linear model, we need to create sparse vectors from this index with only exactly one `1`. This can be done with the `OneHotEncoder` transformer. This time no fitting is required, the class can be used directly with its `transform` method.","user":"anonymous","dateUpdated":"2018-03-10T15:09:45+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>One-Hot-Encoder</h3>\n<p>Now we have a single number (the index of the value) in a new column <code>zipcode_idx</code>. But in order to use the information in a linear model, we need to create sparse vectors from this index with only exactly one <code>1</code>. This can be done with the <code>OneHotEncoder</code> transformer. This time no fitting is required, the class can be used directly with its <code>transform</code> method.</p>\n"}]},"apps":[],"jobName":"paragraph_1520694497005_-2039530809","id":"20180310-150817_1404988058","dateCreated":"2018-03-10T15:08:17+0000","dateStarted":"2018-03-10T15:09:45+0000","dateFinished":"2018-03-10T15:09:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25888"},{"text":"val encoder = new OneHotEncoder()\n    .setInputCol(\"zipcode_idx\")\n    .setOutputCol(\"zipcode_onehot\")\n\nval encodedZipData = encoder.transform(indexedZipData)\nz.show(encodedZipData.limit(10))","user":"anonymous","dateUpdated":"2018-03-10T15:05:41+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_ea48a8630d87\nencodedZipData: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 21 more fields]\n"},{"type":"TABLE","data":"id\tdate\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\tzipcode_idx\tzipcode_onehot\n1000102\t20140916T000000\t280000\t6\t3.0\t2400\t9373\t2.0\t0\t0\t3\t7\t2400\t0\t1991\t0\t98002\t47.3262\t-122.214\t2060\t7316\t53.0\t(70,[53],[1.0])\n1000102\t20150422T000000\t300000\t6\t3.0\t2400\t9373\t2.0\t0\t0\t3\t7\t2400\t0\t1991\t0\t98002\t47.3262\t-122.214\t2060\t7316\t53.0\t(70,[53],[1.0])\n1200021\t20140811T000000\t400000\t3\t1.0\t1460\t43000\t1.0\t0\t0\t3\t7\t1460\t0\t1952\t0\t98166\t47.4434\t-122.347\t2250\t20023\t42.0\t(70,[42],[1.0])\n2800031\t20150401T000000\t235000\t3\t1.0\t1430\t7599\t1.5\t0\t0\t4\t6\t1010\t420\t1930\t0\t98168\t47.4783\t-122.265\t1290\t10320\t38.0\t(70,[38],[1.0])\n3600057\t20150319T000000\t402500\t4\t2.0\t1650\t3504\t1.0\t0\t0\t3\t7\t760\t890\t1951\t2013\t98144\t47.5803\t-122.294\t1480\t3504\t27.0\t(70,[27],[1.0])\n3600072\t20150330T000000\t680000\t4\t2.75\t2220\t5310\t1.0\t0\t0\t5\t7\t1170\t1050\t1951\t0\t98144\t47.5801\t-122.294\t1540\t4200\t27.0\t(70,[27],[1.0])\n3800008\t20150224T000000\t178000\t5\t1.5\t1990\t18200\t1.0\t0\t0\t3\t7\t1990\t0\t1960\t0\t98178\t47.4938\t-122.262\t1860\t8658\t40.0\t(70,[40],[1.0])\n5200087\t20140709T000000\t487000\t4\t2.5\t2540\t5001\t2.0\t0\t0\t3\t9\t2540\t0\t2005\t0\t98108\t47.5423\t-122.302\t2360\t6834\t56.0\t(70,[56],[1.0])\n6200017\t20141112T000000\t281000\t3\t1.0\t1340\t21336\t1.5\t0\t0\t4\t5\t1340\t0\t1945\t0\t98032\t47.4023\t-122.273\t1340\t37703\t62.0\t(70,[62],[1.0])\n7200179\t20141016T000000\t150000\t2\t1.0\t840\t12750\t1.0\t0\t0\t3\t6\t840\t0\t1925\t0\t98055\t47.484\t-122.211\t1480\t6969\t43.0\t(70,[43],[1.0])\n"}]},"apps":[],"jobName":"paragraph_1520694211736_1931464233","id":"20180310-150331_1565568506","dateCreated":"2018-03-10T15:03:31+0000","dateStarted":"2018-03-10T15:05:41+0000","dateFinished":"2018-03-10T15:05:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25889"},{"text":"%md\n# Creating Pipelines\n\nSince it would be tedious to add all features one after another and apply a full chain of transformations to the training set, the validation set and eventually to new data, Spark provides a `Pipeline` abstraction","user":"anonymous","dateUpdated":"2018-03-10T15:00:04+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Creating Pipelines</h1>\n<p>Since it would be tedious to add all features one after another and apply a full chain of transformations to the training set, the validation set and eventually to new data, Spark provides a <code>Pipeline</code> abstraction</p>\n"}]},"apps":[],"jobName":"paragraph_1520693948357_-627296030","id":"20180310-145908_1673430871","dateCreated":"2018-03-10T14:59:08+0000","dateStarted":"2018-03-10T15:00:04+0000","dateFinished":"2018-03-10T15:00:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25890"},{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\n\nval pipeline = new Pipeline().setStages(Array(\n    new StringIndexer()\n        .setInputCol(\"bathrooms\")\n        .setOutputCol(\"bathrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bathrooms_idx\")\n        .setOutputCol(\"bathrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"bedrooms\")\n        .setOutputCol(\"bedrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bedrooms_idx\")\n        .setOutputCol(\"bedrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"floors\")\n        .setOutputCol(\"floors_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"floors_idx\")\n        .setOutputCol(\"floors_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"view\")\n        .setOutputCol(\"view_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"condition\")\n        .setOutputCol(\"condition_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"grade\")\n        .setOutputCol(\"grade_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"grade_idx\")\n        .setOutputCol(\"grade_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"zipcode\")\n        .setOutputCol(\"zipcode_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"zipcode_idx\")\n        .setOutputCol(\"zipcode_onehot\"),\n    new VectorAssembler()\n        .setInputCols(Array(\"bedrooms_onehot\", \"bathrooms_onehot\", \"sqft_living\", \"sqft_lot\", \"floors_onehot\", \"waterfront\", \"view_onehot\", \"condition_onehot\", \"grade_onehot\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"zipcode_onehot\", \"sqft_living15\", \"sqft_lot15\"))\n        .setOutputCol(\"features\"),\n    new LinearRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"price\")\n    )\n)\n","user":"anonymous","dateUpdated":"2018-03-10T15:15:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\npipeline: org.apache.spark.ml.Pipeline = pipeline_9e637aee1b27\n"}]},"apps":[],"jobName":"paragraph_1520693473059_1757503536","id":"20180225-145446_499092244","dateCreated":"2018-03-10T14:51:13+0000","dateStarted":"2018-03-10T15:15:10+0000","dateFinished":"2018-03-10T15:15:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25891"},{"text":"%md\n### Train model with training data","user":"anonymous","dateUpdated":"2018-03-10T14:57:26+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Train model with training data</h3>\n"}]},"apps":[],"jobName":"paragraph_1520693832065_-1053349335","id":"20180310-145712_108529033","dateCreated":"2018-03-10T14:57:12+0000","dateStarted":"2018-03-10T14:57:26+0000","dateFinished":"2018-03-10T14:57:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25892"},{"text":"val model = pipeline.fit(trainData)","user":"anonymous","dateUpdated":"2018-03-10T15:15:14+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"model: org.apache.spark.ml.PipelineModel = pipeline_9e637aee1b27\n"}]},"apps":[],"jobName":"paragraph_1520693831432_1810736801","id":"20180310-145711_1974986569","dateCreated":"2018-03-10T14:57:11+0000","dateStarted":"2018-03-10T15:15:14+0000","dateFinished":"2018-03-10T15:15:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25893"},{"text":"%md\n### Evaluate model using validation data","user":"anonymous","dateUpdated":"2018-03-10T14:57:44+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Evaluate model using validation data</h3>\n"}]},"apps":[],"jobName":"paragraph_1520693830748_1889225577","id":"20180310-145710_1425414098","dateCreated":"2018-03-10T14:57:10+0000","dateStarted":"2018-03-10T14:57:44+0000","dateFinished":"2018-03-10T14:57:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25894"},{"text":"import org.apache.spark.ml.evaluation._\n\nval evaluator = new RegressionEvaluator()\n    .setLabelCol(\"price\")\n    .setPredictionCol(\"prediction\")\n    .setMetricName(\"rmse\")\n    \nval pred = model.transform(validationData)\nval rmse = evaluator.evaluate(pred)\n\nprintln(s\"RMSE = $rmse\")","user":"anonymous","dateUpdated":"2018-03-10T15:15:22+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.evaluation._\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_868c1a0437a8\npred: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 33 more fields]\nrmse: Double = 143170.76722302838\nRMSE = 143170.76722302838\n"}]},"apps":[],"jobName":"paragraph_1520693830008_-1924036096","id":"20180310-145710_924247341","dateCreated":"2018-03-10T14:57:10+0000","dateStarted":"2018-03-10T15:15:22+0000","dateFinished":"2018-03-10T15:15:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25895"},{"text":"%md\n# Adding more Models\nAnother way of improving the overall prediction is to add multiple models to a single Pipeline. Each downstream ML algorithm has access to the prediction of the previous stages. This way we can create two independant models and eventually fit a mixed model as the last step.","dateUpdated":"2018-03-10T14:51:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Adding more Models</h1>\n<p>Another way of improving the overall prediction is to add multiple models to a single Pipeline. Each downstream ML algorithm has access to the prediction of the previous stages. This way we can create two independant models and eventually fit a mixed model as the last step.</p>\n"}]},"apps":[],"jobName":"paragraph_1520693473059_1757503536","id":"20180304-103951_1075865665","dateCreated":"2018-03-10T14:51:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25896"},{"text":"import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\n\n\nval pipeline = new Pipeline().setStages(Array(\n    new StringIndexer()\n        .setInputCol(\"bathrooms\")\n        .setOutputCol(\"bathrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bathrooms_idx\")\n        .setOutputCol(\"bathrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"bedrooms\")\n        .setOutputCol(\"bedrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bedrooms_idx\")\n        .setOutputCol(\"bedrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"floors\")\n        .setOutputCol(\"floors_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"floors_idx\")\n        .setOutputCol(\"floors_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"view\")\n        .setOutputCol(\"view_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"condition\")\n        .setOutputCol(\"condition_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"grade\")\n        .setOutputCol(\"grade_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"grade_idx\")\n        .setOutputCol(\"grade_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"zipcode\")\n        .setOutputCol(\"zipcode_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"zipcode_idx\")\n        .setOutputCol(\"zipcode_onehot\"),\n    new VectorAssembler()\n        .setInputCols(Array(\"bedrooms_onehot\", \"bathrooms_onehot\", \"sqft_living\", \"sqft_lot\", \"floors_onehot\", \"waterfront\", \"view_onehot\", \"condition_onehot\", \"grade_onehot\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"zipcode_onehot\", \"sqft_living15\", \"sqft_lot15\"))\n        .setOutputCol(\"features\"),\n    new LinearRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"price\")\n        .setPredictionCol(\"linear_prediction\"),\n    new GeneralizedLinearRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"price\")\n        .setFamily(\"poisson\")\n        .setLink(\"log\")\n        .setPredictionCol(\"poisson_prediction\"),\n    new VectorAssembler()\n        .setInputCols(Array(\"linear_prediction\",\"poisson_prediction\"))\n        .setOutputCol(\"pred_features\"),\n    new LinearRegression()\n        .setFeaturesCol(\"pred_features\")\n        .setLabelCol(\"price\")\n        .setPredictionCol(\"prediction\")\n    )\n)","user":"anonymous","dateUpdated":"2018-03-10T15:43:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\npipeline: org.apache.spark.ml.Pipeline = pipeline_f3d0f8fec1dc\n"}]},"apps":[],"jobName":"paragraph_1520693473060_1755579791","id":"20180310-125337_48072730","dateCreated":"2018-03-10T14:51:13+0000","dateStarted":"2018-03-10T15:43:27+0000","dateFinished":"2018-03-10T15:43:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25897"},{"text":"%md\n### Train model with training data","user":"anonymous","dateUpdated":"2018-03-10T14:57:26+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Train model with training data</h3>\n"}]},"apps":[],"jobName":"paragraph_1520693771164_882414326","id":"20180310-145611_1248206854","dateCreated":"2018-03-10T14:56:11+0000","dateStarted":"2018-03-10T14:57:26+0000","dateFinished":"2018-03-10T14:57:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25898"},{"text":"val model = pipeline.fit(trainData)","user":"anonymous","dateUpdated":"2018-03-10T15:43:34+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"model: org.apache.spark.ml.PipelineModel = pipeline_f3d0f8fec1dc\n"}]},"apps":[],"jobName":"paragraph_1520693733455_157685316","id":"20180310-145533_1332151548","dateCreated":"2018-03-10T14:55:33+0000","dateStarted":"2018-03-10T15:43:34+0000","dateFinished":"2018-03-10T15:43:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25899"},{"text":"%md\n### Evaluate model using validation data","user":"anonymous","dateUpdated":"2018-03-10T14:57:41+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Evaluate model using validation data</h3>\n"}]},"apps":[],"jobName":"paragraph_1520693786195_1819308546","id":"20180310-145626_1980099531","dateCreated":"2018-03-10T14:56:26+0000","dateStarted":"2018-03-10T14:57:41+0000","dateFinished":"2018-03-10T14:57:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25900"},{"text":"val pred = model.transform(validationData)\nval rmse = evaluator.evaluate(pred)\n\nprintln(s\"RMSE = $rmse\")","user":"anonymous","dateUpdated":"2018-03-10T15:43:50+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pred: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 36 more fields]\nrmse: Double = 150299.1882805813\nRMSE = 150299.1882805813\n"}]},"apps":[],"jobName":"paragraph_1520693742954_-114885471","id":"20180310-145542_1344236768","dateCreated":"2018-03-10T14:55:42+0000","dateStarted":"2018-03-10T15:43:50+0000","dateFinished":"2018-03-10T15:43:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25901"},{"text":"%md\n### Inspect Model\nLet us inspect the coefficients of the last step, which tells us which of both models (linear or poisson) has more weight.","dateUpdated":"2018-03-10T14:51:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Inspect Model</h3>\n<p>Let us inspect the coefficients of the last step, which tells us which of both models (linear or poisson) has more weight.</p>\n"}]},"apps":[],"jobName":"paragraph_1520693473060_1755579791","id":"20180310-125931_108725758","dateCreated":"2018-03-10T14:51:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25902"},{"text":"model.stages.last.asInstanceOf[LinearRegressionModel].coefficients","user":"anonymous","dateUpdated":"2018-03-10T15:16:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res272: org.apache.spark.ml.linalg.Vector = [0.19420202720326232,0.8057979331534735]\n"}]},"apps":[],"jobName":"paragraph_1520693473060_1755579791","id":"20180310-125654_760871982","dateCreated":"2018-03-10T14:51:13+0000","dateStarted":"2018-03-10T15:16:09+0000","dateFinished":"2018-03-10T15:16:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25903"},{"dateUpdated":"2018-03-10T14:51:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520693473060_1755579791","id":"20180310-125739_1025770034","dateCreated":"2018-03-10T14:51:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25904"}],"name":"House Prices Pipeline Solution","id":"2D8XP2HV5","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}