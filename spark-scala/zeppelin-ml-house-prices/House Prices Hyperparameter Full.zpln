{"paragraphs":[{"text":"%md\n# Load Data\nFirst we load the data from S3. We use the built-in \"csv\" method, which can use the first line has column names and which also supports infering the schema automatically. We use both and save some code for specifying the schema explictly.\n\nWe also peek inside the data by retrieving the first five records.","dateUpdated":"2018-03-20T19:31:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Load Data</h1>\n<p>First we load the data from S3. We use the built-in &ldquo;csv&rdquo; method, which can use the first line has column names and which also supports infering the schema automatically. We use both and save some code for specifying the schema explictly.</p>\n<p>We also peek inside the data by retrieving the first five records.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521574306544_-2041017874","id":"20180224-150345_1979378836","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6879"},{"text":"val rawData = spark.read\n    .option(\"header\",\"true\")\n    .option(\"inferSchema\",\"true\")\n    .csv(\"s3://dimajix-training/data/kc-house-data/\")\n\nz.show(rawData.limit(10))","dateUpdated":"2018-03-20T19:31:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rawData: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 19 more fields]\n"},{"type":"TABLE","data":"id\tdate\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\n7129300520\t20141013T000000\t221900\t3\t1.0\t1180\t5650\t1.0\t0\t0\t3\t7\t1180\t0\t1955\t0\t98178\t47.5112\t-122.257\t1340\t5650\n6414100192\t20141209T000000\t538000\t3\t2.25\t2570\t7242\t2.0\t0\t0\t3\t7\t2170\t400\t1951\t1991\t98125\t47.721\t-122.319\t1690\t7639\n5631500400\t20150225T000000\t180000\t2\t1.0\t770\t10000\t1.0\t0\t0\t3\t6\t770\t0\t1933\t0\t98028\t47.7379\t-122.233\t2720\t8062\n2487200875\t20141209T000000\t604000\t4\t3.0\t1960\t5000\t1.0\t0\t0\t5\t7\t1050\t910\t1965\t0\t98136\t47.5208\t-122.393\t1360\t5000\n1954400510\t20150218T000000\t510000\t3\t2.0\t1680\t8080\t1.0\t0\t0\t3\t8\t1680\t0\t1987\t0\t98074\t47.6168\t-122.045\t1800\t7503\n7237550310\t20140512T000000\t1225000\t4\t4.5\t5420\t101930\t1.0\t0\t0\t3\t11\t3890\t1530\t2001\t0\t98053\t47.6561\t-122.005\t4760\t101930\n1321400060\t20140627T000000\t257500\t3\t2.25\t1715\t6819\t2.0\t0\t0\t3\t7\t1715\t0\t1995\t0\t98003\t47.3097\t-122.327\t2238\t6819\n2008000270\t20150115T000000\t291850\t3\t1.5\t1060\t9711\t1.0\t0\t0\t3\t7\t1060\t0\t1963\t0\t98198\t47.4095\t-122.315\t1650\t9711\n2414600126\t20150415T000000\t229500\t3\t1.0\t1780\t7470\t1.0\t0\t0\t3\t7\t1050\t730\t1960\t0\t98146\t47.5123\t-122.337\t1780\t8113\n3793500160\t20150312T000000\t323000\t3\t2.5\t1890\t6560\t2.0\t0\t0\t3\t7\t1890\t0\t2003\t0\t98038\t47.3684\t-122.031\t2390\t7570\n"}]},"apps":[],"jobName":"paragraph_1521574306545_-2041402623","id":"20180224-145450_1588429371","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6880"},{"text":"// Split the data - 80% for training, 20% for validation\nval Array(trainData, validationData) = rawData.randomSplit(Array(0.8,0.2))\n\nprintln(s\"traingData = ${trainData.count}\")\nprintln(s\"validationData = ${validationData.count}\")","dateUpdated":"2018-03-20T19:31:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, date: string ... 19 more fields]\nvalidationData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, date: string ... 19 more fields]\ntraingData = 17466\nvalidationData = 4147\n"}]},"apps":[],"jobName":"paragraph_1521574306545_-2041402623","id":"20180304-103415_198808093","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6881"},{"text":"%md\n# Create Pipeline\n\nWe reuse the long pipeline from last time. This time we want to tweak additional parameters, so called *Hyper-Parameters*","dateUpdated":"2018-03-20T19:31:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Create Pipeline</h1>\n<p>We reuse the long pipeline from last time. This time we want to tweak additional parameters, so called <em>Hyper-Parameters</em></p>\n"}]},"apps":[],"jobName":"paragraph_1521574306546_-2040248376","id":"20180310-145908_1673430871","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6882"},{"text":"import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\n\n\nval stages = Array(\n    new StringIndexer()\n        .setInputCol(\"bathrooms\")\n        .setOutputCol(\"bathrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bathrooms_idx\")\n        .setOutputCol(\"bathrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"bedrooms\")\n        .setOutputCol(\"bedrooms_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"bedrooms_idx\")\n        .setOutputCol(\"bedrooms_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"floors\")\n        .setOutputCol(\"floors_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"floors_idx\")\n        .setOutputCol(\"floors_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"view\")\n        .setOutputCol(\"view_onehot\"),\n    new OneHotEncoder()\n        .setInputCol(\"condition\")\n        .setOutputCol(\"condition_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"grade\")\n        .setOutputCol(\"grade_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"grade_idx\")\n        .setOutputCol(\"grade_onehot\"),\n    new StringIndexer()\n        .setInputCol(\"zipcode\")\n        .setOutputCol(\"zipcode_idx\")\n        .setHandleInvalid(\"keep\"),\n    new OneHotEncoder()\n        .setInputCol(\"zipcode_idx\")\n        .setOutputCol(\"zipcode_onehot\"),\n    new VectorAssembler()\n        .setInputCols(Array(\"bedrooms_onehot\", \"bathrooms_onehot\", \"sqft_living\", \"sqft_lot\", \"floors_onehot\", \"waterfront\", \"view_onehot\", \"condition_onehot\", \"grade_onehot\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"zipcode_onehot\", \"sqft_living15\", \"sqft_lot15\"))\n        .setOutputCol(\"features\"),\n    new LinearRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"price\")\n        .setPredictionCol(\"linear_prediction\"),\n    new GeneralizedLinearRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"price\")\n        .setFamily(\"poisson\")\n        .setLink(\"log\")\n        .setPredictionCol(\"poisson_prediction\"),\n    new VectorAssembler()\n        .setInputCols(Array(\"linear_prediction\",\"poisson_prediction\"))\n        .setOutputCol(\"pred_features\"),\n    new LinearRegression()\n        .setFeaturesCol(\"pred_features\")\n        .setLabelCol(\"price\")\n        .setPredictionCol(\"prediction\")\n    \n)\n\nval pipe = new Pipeline().setStages(stages)","dateUpdated":"2018-03-20T19:31:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.regression._\nstages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(strIdx_dc59b23f7962, oneHot_d2ac33812a98, strIdx_66c5ad691771, oneHot_298eb75fe5f5, strIdx_92d80738fdbd, oneHot_98ffcaac9f79, oneHot_3a1f2f8924c9, oneHot_950497b60788, strIdx_868545e708dc, oneHot_77ec783465c5, strIdx_2628498cf900, oneHot_1f52156b48b6, vecAssembler_7bbeaf6cb195, linReg_d0482055b80c, glm_d3db17e5f9c7, vecAssembler_6dacb9fa3625, linReg_6a6463f4daac)\npipe: org.apache.spark.ml.Pipeline = pipeline_919d2e36b7e3\n"}]},"apps":[],"jobName":"paragraph_1521574306546_-2040248376","id":"20180310-125337_48072730","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6883"},{"text":"%md\n# Hyper Parameter Tuning\n\nThe whole pipeline has some parameters which have an influence on the result, i.e. the accuracy. For example the size of the n-grams will probably have a big impact and also the minDF parameter of the CountVecttorizer will probably have some impact. These settings are called \"hyper parameters\", because they are also model parameters, but not learnt directly during the training phase. But which parameters work best?\n\nWe will use a CrossValidation to select the best set of hyperparameters.","dateUpdated":"2018-03-20T19:31:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Hyper Parameter Tuning</h1>\n<p>The whole pipeline has some parameters which have an influence on the result, i.e. the accuracy. For example the size of the n-grams will probably have a big impact and also the minDF parameter of the CountVecttorizer will probably have some impact. These settings are called &ldquo;hyper parameters&rdquo;, because they are also model parameters, but not learnt directly during the training phase. But which parameters work best?</p>\n<p>We will use a CrossValidation to select the best set of hyperparameters.</p>\n"}]},"apps":[],"jobName":"paragraph_1521574306546_-2040248376","id":"20180310-152535_117636103","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6884"},{"text":"stages(13).explainParams()","dateUpdated":"2018-03-20T19:31:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res276: String =\naggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\nfeaturesCol: features column name (default: features, current: features)\nfitIntercept: whether to fit an intercept term (default: true)\nlabelCol: label column name (default: label, current: price)\nmaxIter: maximum number of iterations (>= 0) (default: 100)\npredictionCol: prediction column name (default: prediction, current: linear_prediction)\nregParam: regularization parameter (>= 0) (default: 0.0)\nsolver: the solver algorithm for optimization. If this is not set or empty, default value is 'auto' (default: auto)\nstandardization: whether to ..."}]},"apps":[],"jobName":"paragraph_1521574306546_-2040248376","id":"20180310-152324_1493015438","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6885"},{"text":"stages(14).explainParams()","dateUpdated":"2018-03-20T19:31:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res277: String =\nfamily: The name of family which is a description of the error distribution to be used in the model. Supported options: gaussian, poisson, binomial, gamma, tweedie. (default: gaussian, current: poisson)\nfeaturesCol: features column name (default: features, current: features)\nfitIntercept: whether to fit an intercept term (default: true)\nlabelCol: label column name (default: label, current: price)\nlink: The name of link function which provides the relationship between the linear predictor and the mean of the distribution function. Supported options: cloglog, probit, logit, inverse, sqrt, identity, log (current: log)\nlinkPower: The index in the power link function. Only applicable to the Tweedie family. (undefined)\nlinkPredictionCol: link prediction (linear predictor) col..."}]},"apps":[],"jobName":"paragraph_1521574306547_-2040633125","id":"20180310-152426_996034994","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6886"},{"text":"%md\n## Create ParamGrid\n\nNow we create a param grid that should be used for using different sets of parameters. We want to tweak three parameters:\n\n* `elasticNetParam` of first LinearRegression  should take values in [0.25,0.5,0.75]\n* `regParam` of first LinearRegression  should take values in [0.01, 0.1, 0.3]\n* `link` opf the GeneralizedLinearRegression should take values in [\"identity\", \"log\"])\n\nIn order to create this grid, we first need to retrieve the corresponding stages from the pipeline, so we can access its parameters.","dateUpdated":"2018-03-20T19:35:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Create ParamGrid</h2>\n<p>Now we create a param grid that should be used for using different sets of parameters. We want to tweak three parameters:</p>\n<ul>\n<li><code>elasticNetParam</code> of first LinearRegression  should take values in [0.25,0.5,0.75]</li>\n<li><code>regParam</code> of first LinearRegression  should take values in [0.01, 0.1, 0.3]</li>\n<li><code>link</code> opf the GeneralizedLinearRegression should take values in [&ldquo;identity&rdquo;, &ldquo;log&rdquo;])</li>\n</ul>\n<p>In order to create this grid, we first need to retrieve the corresponding stages from the pipeline, so we can access its parameters.</p>\n"}]},"apps":[],"jobName":"paragraph_1521574306547_-2040633125","id":"20180310-152503_2061332323","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6887"},{"text":"import org.apache.spark.ml.tuning.ParamGridBuilder\n\nval linearRegression = stages(13)\nval generalizedLinearRegression = stages(14)\n\nval param_grid = new ParamGridBuilder()\n    .addGrid(linearRegression.getParam(\"elasticNetParam\"), Seq(0.25, 0.5, 0.75))\n    .addGrid(linearRegression.getParam(\"regParam\"), Seq(0.01, 0.1, 0.3))\n    .addGrid(generalizedLinearRegression.getParam(\"link\"), Seq(\"identity\", \"log\"))\n    .build()\n","dateUpdated":"2018-03-20T19:31:46+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.tuning.ParamGridBuilder\nlinearRegression: org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}} = linReg_d0482055b80c\ngeneralizedLinearRegression: org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}} = glm_d3db17e5f9c7\nparam_grid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\tlinReg_d0482055b80c-elasticNetParam: 0.25,\n\tglm_d3db17e5f9c7-link: identity,\n\tlinReg_d0482055b80c-regParam: 0.01\n}, {\n\tlinReg_d0482055b80c-elasticNetParam: 0.25,\n\tglm_d3db17e5f9c7-link: identity,\n\tlinReg_d0482055b80c-regParam: 0.1\n}, {\n\tlinReg_d0482055b80c-elasticNetParam: 0.25,\n\tglm_d3db17e5f9c7-link: identity,\n\tlinReg_d0482055b80c-regParam: 0.3\n}, {\n\tlinReg_d0482055b80c-elasticNetParam: 0.5,\n\tglm_d3db17e5f9c7-link: identity,\n\tlinReg_d0482055b80c-regParam: 0.01\n}, {\n\tlinReg_d0482055b80c-elasticNetParam: 0.5,\n\tglm_d3db17e5f9c7-link: identity,\n\tlinReg_d0482055b80c-regParam: 0.1\n}, {\n\tlinReg_d0482055b80c-elasticNetParam: 0.5,\n\tglm_d3db17e5f9c7-link: identity,\n\tlinReg_d0482055b80c-regParam: 0.3\n}, {\n\tlinReg_d0482055b80c-elast..."}]},"apps":[],"jobName":"paragraph_1521574306547_-2040633125","id":"20180310-152600_874798935","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6888"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574524861_24789271","id":"20180320-193524_1421311011","dateCreated":"2018-03-20T19:35:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7918","text":"%md\n## Create a CrossValidator\nNow we have a set of hyper parameters which should all be tested and the best set of parameters should eventually be selected. Spark offers some support for that to offload the coding by so called validators. We will use the `CrossValidator` from the package `org.apache.spark.ml.tuning`. The validator requires an estimator to fit (in our case a `Pipeline`) and an evaluator to assess the performance of each value of the hyper parameters (we will use the `RegressionEvaluator` again).","dateUpdated":"2018-03-20T19:39:59+0000","dateFinished":"2018-03-20T19:39:59+0000","dateStarted":"2018-03-20T19:39:59+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Create a CrossValidator</h2>\n<p>Now we have a set of hyper parameters which should all be tested and the best set of parameters should eventually be selected. Spark offers some support for that to offload the coding by so called validators. We will use the <code>CrossValidator</code> from the package <code>org.apache.spark.ml.tuning</code>. The validator requires an estimator to fit (in our case a <code>Pipeline</code>) and an evaluator to assess the performance of each value of the hyper parameters (we will use the <code>RegressionEvaluator</code> again).</p>\n</div>"}]}},{"text":"import org.apache.spark.ml.evaluation._\nimport org.apache.spark.ml.tuning._\n\nval evaluator = new RegressionEvaluator()\n    .setLabelCol(\"price\")\n    .setPredictionCol(\"prediction\")\n    .setMetricName(\"rmse\")\n\n// Create a CrossValidator instance\nval validator = new CrossValidator()\n    .setEstimator(pipe)\n    .setEstimatorParamMaps(param_grid)\n    .setEvaluator(evaluator)\n    .setNumFolds(3)\n    ","dateUpdated":"2018-03-20T19:31:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.evaluation._\nimport org.apache.spark.ml.tuning._\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_b9cf842bb220\nvalidator: org.apache.spark.ml.tuning.CrossValidator = cv_2cf3769d676d\n"}]},"apps":[],"jobName":"paragraph_1521574306548_-2042556870","id":"20180310-145710_924247341","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6889"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574726897_-1206282971","id":"20180320-193846_150690609","dateCreated":"2018-03-20T19:38:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7997","text":"%md\n## Fitting a Model\nThe `CrossValidator` again works as an `Estimator` with a `fit` method which will create an appropriate model.","dateUpdated":"2018-03-20T19:39:54+0000","dateFinished":"2018-03-20T19:39:54+0000","dateStarted":"2018-03-20T19:39:54+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Fitting a Model</h2>\n<p>The <code>CrossValidator</code> again works as an <code>Estimator</code> with a <code>fit</code> method which will create an appropriate model.</p>\n</div>"}]}},{"text":"val model = validator.fit(trainData)","dateUpdated":"2018-03-20T19:31:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"model: org.apache.spark.ml.tuning.CrossValidatorModel = cv_2cf3769d676d\n"}]},"apps":[],"jobName":"paragraph_1521574306548_-2042556870","id":"20180310-145533_1332151548","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6890"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574816788_689095735","id":"20180320-194016_291828037","dateCreated":"2018-03-20T19:40:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8098","text":"%md\n## Assess Performance\nEventually we want to verify the performance using the same validator as used for tuning, but with the validation data instead of the training data.","dateUpdated":"2018-03-20T19:40:55+0000","dateFinished":"2018-03-20T19:40:55+0000","dateStarted":"2018-03-20T19:40:55+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Assess Performance</h2>\n<p>Eventually we want to verify the performance using the same validator as used for tuning, but with the validation data instead of the training data.</p>\n</div>"}]}},{"text":"\nval pred = model.transform(validationData)\nval rmse = evaluator.evaluate(pred)\n\nprintln(s\"RMSE = $rmse\")","dateUpdated":"2018-03-20T19:31:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pred: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 36 more fields]\nrmse: Double = 162734.85919943213\nRMSE = 162734.85919943213\n"}]},"apps":[],"jobName":"paragraph_1521574306548_-2042556870","id":"20180310-152639_126013484","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6891"},{"dateUpdated":"2018-03-20T19:31:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521574306549_-2042941618","id":"20180310-125739_1025770034","dateCreated":"2018-03-20T19:31:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6892"}],"name":"House Prices Hyperparameter Full","id":"2DC161G46","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}